{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900b8386-4be9-4dde-a7c7-1ae52ef434ef",
   "metadata": {},
   "source": [
    "<h1>CreateSaveModelAmorphousML.ipynb</h1> \n",
    "\n",
    "This code requires that AmorphousFileLectureCreate.ipynb has been run. If you are confident that the model structures are correct (go to TestAFolder to test the structures) then this code creates the model. It trains on ALL experiments that were processed with AmorphousFileLectureCreate.ipynb \n",
    "\n",
    "__________________________________________________________________________________________\n",
    "\n",
    "OUTPUTS OF THE CODE: \n",
    "\n",
    "1. **AmorphousLogFileModelCreation.txt**\n",
    "A log file with every step that the algorithm has followed\n",
    "\n",
    "\n",
    "2. **AmorphousExecution_times.txt**\n",
    "It times how long the code took to create each model (0.5-1 mins aprox. per model)\n",
    "\n",
    "\n",
    "3. **ModelsAmorphous**\n",
    "For each Complexity and each num_augmentations (a.k.a, for each model structure and data organization) a folder is created.\n",
    "\n",
    "    3.1 **Model\\_{Complexity}\\_{num\\_augmentations}**\n",
    "**THESE ARE THE FOLDERS YOU NEED TO COPY AND PASTE INSIDE THE PREDICTING CODE FOLDERS. THE WHOLE FOLDER NOT JUST THE CONTENTS**\n",
    "Inside these folders you have the entire model and the scalers all ready to use for prediction. \n",
    "Note: The corrections are dependent on the static parameters and not the model or the training routine. Therefore this models will not give you the corrected predictions. (Do not worry, in the prediction files these corrections are automatically done if you don't change the flag that activates them)\n",
    " \n",
    "       3.1.1 Model_{Complexity}_{num_augmentations}.keras The model file\n",
    "\n",
    "       3.1.2 scaler_static_{Complexity}_{num_augmentations}_PolarizationD3.pkl. The scaler for the static parameters (initial and final polarizations included) used in this isolated-experiment iteration\n",
    "\n",
    "       3.1.3 scaler_time_{Complexity}_{num_augmentations}_PolarizationD3.pkl. The scaler for the time evolution used in this isolated-experiment iteration\n",
    "\n",
    "       3.1.4 scaler_y_{Complexity}_{num_augmentations}.pkl. The scaler for the polarization values used in this isolated-experiment iteration\n",
    "\n",
    "\n",
    "__________________________________________________________________________________________\n",
    "\n",
    "Process:\n",
    "\n",
    "It takes two lists o\n",
    "By changing the lists Names and Augmentations you can pick what model you create. Just write on the two lists your model names and the number of augmentations and it will create models combining them \n",
    "As an example,\n",
    "\n",
    "Names = [\"ModelA\",\"ModelB\",\"ModelC\"]\n",
    "\n",
    "Augmentations = [5,9,2]\n",
    "\n",
    "will create the models \"ModelA_5\",\"ModelB_9\" and \"ModelC_2\"\n",
    "\n",
    "In order to do that, the code first prepares the folder structure and the folder where the model will be stored.\n",
    "It then loops (if told to) over all polarization columns to create models for both.\n",
    "It extracts all information from the data base, scales it and saves those scalers\n",
    "Finally it trains the model and saves it in a distinct folder. That folder contains the scalers (in order to decipher how to scale it back to physical units) and the trained model.\n",
    "\n",
    "Here are some explanations of what the code does in a more detiled manner:\n",
    "\n",
    "1. **load\\_experiments**. It uses the directory where the array (numerical values) and parameter files resides, picks one polarization column and prepares a list of experiments to feed the ML algorythm. The output is as follows:\n",
    "\n",
    "    *encoded\\_experiments=[(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 1}$,(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 2}$,...]* \n",
    "    \n",
    "    Note that the Cell Id is Hot Encoded. The type of cell did not affect greatly the predictions. However in the future, it may be useful to give more information to Cell_ID. As of 2025, these strings are Hot Encoded which means that the code finds all the different types, creates columns for each type and writes 0 or 1 (bool) depending of whether the cell was from one type or the other. This is the standard procedure to feed categorical variables to ML.\n",
    "    The size of the output list depends on the number of pairs of .txt files present in the folder. This means that it also works for isolated experiments.\n",
    "    \n",
    "2. **augment_experiments** is a function that takes the original data list (as load\\_experiments returns it) and augments them _num\\_augmentations_ times. The final legth of the list will be (num\\_augmentations +1) times the original length. Due to the reduced size of the data base (Not more than 30 experiments before 2026), techniques like augmentation are required. Due to the stochastic nature of neutron detection, most measurements, if repeated under the same conditions, will yield different results. Of course, all measurements converge (with uncertainty) to what we can say is \"the true value\". Therefore, we can duplicate the experiments adding noise to the measurements to obtain \"hypothetical\" measurements that expand the data base. It was decided that the uncertainty won't be modified. To decide what type of noise could be applied two possibilities were considered. The first one was to suppose that sensor detection follows a Poisson distribution (law of rare events). The second one was to suppose that it follows a Normal Distribution of mean the measured value and width the uncertainty. As a Poisson distribution converges stochastically (in probability and distribution but not almost sure nor in $L^p$) to a normal distribution, it is safe to assume that they converge to the same result so the gaussian approach was selected. Also, the measurements are not raw counts but a function of them.\n",
    "$P=\\frac{n^+-n^-}{n^++n^-}$\n",
    "We have no direct data of the number of counts (the parameter of a Posissonian distribution) but a rough estimate points to them being (worst case scenario) within the order of the millions (note that after 100 counts, poisson distributions vary very little from normal distributions). Threfore, count detection can be approximated to a normal distribution and a linear combination of random variables distributed as normal distributions is also a random variable with a normal distribution. Therefore, it is safe to assume that sensor measurements can be modelled after a gaussian distribution.\n",
    "\n",
    "\n",
    "3. **build_dataset** is a function that prepares the data base to be fed directly into a ML model for training and validation. There are a few import decisions taken here. The way this function is set up, it removes 2 rows of data per polariser cell studied. The reason why it was done is because we want the ML model to be able to predict polarization decay when given the specs of the cell (the static parameters) and the initial and final polarization values (with their associated time values). The reason why those two values are considered \"known values\" for each cell is beacuse they can be easily measured experimentally and they give as a good estimate about the overall behaviour. In some experiments, the environment of the studied sample is too fragile to move and place the Si crystal for polarization measurements. Therefore, a working ML algorithm whose inputs are the specs of the cell and the initial and final polarizations (measured before and after the sample is in place) would enable experiments that could not be done before without proper polarization efficiency corrections. \n",
    "Also, we remove those two values per polariser cell from the training arrays (Xt, y and err). The reason why it is done is to avoid data leaks in the model. If we give those values as training data and also give them as parameters, the ML algorithm can run into the risk of memorizing those pairs (over-fitting) and worsen any new predicitions. Uncertainties for the first and last polarization measurements are not added to the static features. This was a decision taken to avoid giving too much weight to two variables that don't have value on their own (they compliment the polarization values but if the ML architecture is as shallow as\n",
    "the one used here, they can be considered independent variables and reduce the weight and importance of the other variables). \n",
    "Another consideration taken here was that the static features get duplicated in Xs a lot of times. One could think that using a similar method that the one used for augmentations of the data sets could also diversify the data base. However, we wanted all measurements of a same session and cell to be coherent \n",
    "and it wouldn't make sense to have different static features. Therefore, the safest approach was to only duplicate these values. For the augmented experiments the only parameters that are changed are the initial and final times and polarizations. There is no incompatibility here to what we have just said as these work as \"hypothetical independent experiments\". This is why augmentation is done before this function gets used.\n",
    "         \n",
    "\n",
    "4. **nll_loss** ML algorythms require a way to tell the algorythm if it is learning or not. The most standard practice is with a Loss function. If the loss value goes down that means that the algorythm is learning and, if a step increases the loss, then it is punished and tries other approaches. When using uncertainties when teaching the model, the most common loss function is the NLL or Negative log-likelihood of a normal distribution \n",
    "NLL$=\\frac{1}{2}$log$(σ^2)+\\frac{(y−μ)^2}{2σ^2}$\n",
    "where $\\sigma$ is the uncertainty in the predictions, $y$ is the measured value and $\\mu$ the predicted value. Instead of predicting $σ^2$ directly, we obtain its logarithm to have a more stable process (and avoid accounting precision as $\\sigma^{-2}$ which is numerically unstable when uncertainties are low).\n",
    "\n",
    "However we want to avoid uncertainties that drift too far from the overall model predictions. To achieve that, we can get a rough estimate on what the uncertainty of a set predictions looks like.\n",
    "Let $\\vec{\\mu}=\\left(\\mu_1,\\ldots,\\mu_N\\right)^T$ be the vector of $N$ predicted values. It can be considered as a random vector of variance:\n",
    "$Var(\\vec{\\mu})=\\frac{1}{N}\\sum_{i=1}^N\\left(\\mu_i-E(\\vec{\\mu})\\right)$\n",
    "where $E(\\vec{\\mu})$ is the mean of the predicted values. We then have two different variances, one obtained as the sparseness of the predictions, (denoted as $Var\\left(\\vec{\\mu}\\right)$, and one obtained as a result of the internal ML calculations (denoted as $\\sigma^2$). A penalty can be added to the loss functions to force the model to try to reduce this differences. An easy way to model it is to obtain the difference of those variances and square the result (taking the absolute value was also a good estimate, but using squared values punished big discrepancies in a stronger way).\n",
    "\n",
    "$StrayPenalty = B \\cdot \\left[\\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}\\left(\\vec{\\mu}\\right)\\right)\\right]^2$\n",
    "where $B$ is a constant used to control the weight of this penalty. The reason why $Var\\left(\\vec{\\mu}\\right)$ was used and not $Var\\left(\\vec{y}\\right)$ (with $\\vec{y}$ the vector of measured values) was to avoid noise in the original data to tamper with the loss function. It would be physically clearer to use measured values sparseness as a way to guide the model but some experimental uncertainties are clearly underestimated and that would cause this penalty to dominate the loss and obscure the main loss protocol, the NLL.\n",
    "\n",
    "Also, we also want to punish the model if it tries overestimating $\\sigma$. If the model is unable to minimize $y-\\mu$, in order to lower NLL, it increases $\\sigma$. If no precautions are taken, this \"escape solution\" achieves bad predictions with inflated uncertainties that simulate a low loss value. A new penalty was added that punishes overestimation of the uncertainties more than underestimation (which never happened). The slope correction done further on the pipeline can \"fix\" this issue but what the model returns then is closer to a poorly calculated linear fit\n",
    "Therefore, an addition penalty was added.\n",
    "\n",
    "$OverestimatePenalty= C \\cdot \\max\\left(0, \\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}(\\vec{\\mu})\\right)\\right)$\n",
    "where $C$ is a constant used to control the weight of this penalty\n",
    "\n",
    "5. **Define_Complexity** It consists of a single function called _Define\\_Complexity_. Given the name of a model it defines the model function of the ML algorithm. To be precise, it defines a function called _build_model_ every time _Define\\_Complexity_ runs. If _Define\\_Complexity_ gets run another time, it will define (possibly) another _build_model_ if the _Complexity_ variable changes\n",
    "\n",
    "Here we have functions that train, validate and fit the models. Some models require the variables to be scaled or will scale them. Extra precautions need to be taken into account\n",
    "\n",
    "6. **model\\_fitting**. It is a function that logs and runs model.fit() on a two-input Keras model and returns the training history. It needs **static, time and polarization variables scaled**. Training is not done using the uncertainties of the data as it was decided that uncertainty information is encoded in the augmentations. Note: No validation is done anywhere in the code. Here are some of the reasons:\n",
    "    \n",
    "    6.1. The data base is very small. The amorphous data base contains only 199 points while the crystalline one contains 251. Removing a small percentage of those points for validation might leave the data base too small and underfitting might worsen the result more than fine tuning parametrs with validation.\n",
    "    \n",
    "    6.2. A randomized validation split may be physically wrong. Therefore it should be chronological, not shuffled. However, in crystalline experiments, there are decay experiments that have only four or five intermediate points. Even removing one point for validation is a massive hit on the experiment. Therefore, it is risky to add validation\n",
    "    \n",
    "    6.3. To find good models, a Leave-one-out approach was used. For a certain model structure, an experiment gets removed and the model and it trains on all the remaining experiments. Then, the model tries to predict this isolated experiment. Afterwards, the experiment is returned and a new one becomes isolated. This process loops for all experiments and an overall score of the model is computed. This process was done for 498 models for crystalline materials. This is a stronger (and more expensive) method than validation as it is not dependent on the validation splits and avoids possible information leaks.\n",
    "\n",
    "Also, eight randomly picked models were tested with and without validation and with and without an asymetric uncertainty-overestimated penalizing loss. The result showed that the Loss update was an improvement and validation did not increase performance (without validation, the results were slightly better).\n",
    "\n",
    "7. **model\\_prediction**. It is a funtion that predicts with a given model. It needs **static and time variables scaled**. This scaling must be coherent to the one done in the rest of the funtions.\n",
    "\n",
    "8. **train**. This function is the one responsible of scaling the inputs and training the model (it uses **model\\_fitting**)\n",
    "\n",
    "    8.1. It creates the independent arrays with all the encoded experiments (augmented or not) using build_dataset\n",
    "    \n",
    "    8.2. Then it scales the data. ML algorithms work better when the inputs and outputs are normalized. The reason why we don't normalize inside the function is to have those scaler defined globally and not locally\n",
    "    \n",
    "    8.3. It builds the model depending on the use\\_uncertainty bool. (It changes the loss function and the output).\n",
    "    \n",
    "    8.4. It trains the model and returns its history (the trained model)\n",
    "    \n",
    "9. **align_static_vectors**. It converts the columns not present on an isolated experiment to zeros.\n",
    "    \n",
    "10. **model_predict_sloped** It substracts a linear function to the predicted values. If done correctly this makes it so that the polarization predictions at the initial and final time points are the same as the measured polarization values at those times. This fixes a vertical shift and also an overall slope. As it is a correction done with experimental values, the algorithm is still \"universal\". However we can't fully say that it is a pure ML algorithm. The \"correctness\" of this method is subjective. It is a warning in the ML front that there is an issue with the data base but it is a valid fix for experimentalists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e37db-cbf3-4f69-a2a0-66712bf2f84b",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe856c1-6988-41be-bb98-fe85681e1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import shutil\n",
    "import gc\n",
    "import joblib\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras import Input, Model, regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85456788-1fb3-4606-90ab-0f72d3a6cb49",
   "metadata": {},
   "source": [
    "## 2. Auxiliary Functions and log file creation\n",
    "\n",
    "1. _PrintDebug_ is a flag that allows the code to output on screen all the steps. If it is set to false, it won´t show anything. However, all information will be properly logged whether this flag is set to true or false. The name of the log is determined by the variable *log_file_path*. The code runs faster if it is set to False.\n",
    "\n",
    "2. _ShowPlot_ is a similar flag that allows the code to show on screen all plots that are being produced. They are all stored independently of whether this flag is True or False. The code runs faster if it is set to False.\n",
    "\n",
    "3. _LogNoise_ is another flag that allows numerical values in the log. Most of these values are not worth keeping but if you want to see if there are no NaNs or zeros you can turn it on\n",
    "\n",
    "\n",
    "3. **log_message** is a function used for writting on the log file\n",
    "\n",
    "4. **win_long_path** is a function that \"fixes\" directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e89e6dd-51ca-421b-a0b1-823d63812bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found (skipped): C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousLogFileModelCreation.txt\n",
      "Not found (skipped): C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousExecution_times.txt\n",
      "Not found (skipped): C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousModels\n"
     ]
    }
   ],
   "source": [
    "PrintDebug = True #This Bool will determine if all logs should be printed on screen on the Python Notebook. The log writing is always on. If False the code will be faster.\n",
    "ShowPlot = False #This Bool works the same but with showing on screen the plots (they are always saved even with this variable being False). Reduces program cost if False\n",
    "LogNoise = False #This Bool allows numerical values in the log. Most of these values are not worth keeping but if you want to see if there are no NaNs or zeros you can turn it on\n",
    "\n",
    "log_file_path = \"AmorphousLogFileModelCreation.txt\"\n",
    "def log_message(message):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        message (string): The text that will be logged\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    Notes:\n",
    "        It will write the string \"message\" in the log file.\n",
    "        If PrintDebug==True then it will also print the string\n",
    "    \"\"\"\n",
    "    message = str(message)\n",
    "    if PrintDebug:\n",
    "        print(message)\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "        log_file.write(str(message) + \"\\n\")\n",
    "        \n",
    "################################################################\n",
    "\n",
    "def long_path(path):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        path (path): The path that needs to be converted\n",
    "    \n",
    "    Returns:\n",
    "        The updated path string or path depending on the platform used\n",
    "        \n",
    "    Notes:\n",
    "        To avoid Windows 260 character limit for Windows paths, a special \"prefix\" is added.\n",
    "        It also unifies how directories are managed.\n",
    "        Also works with Linux and Mac\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert to Path and resolve to absolute\n",
    "    path = Path(path).resolve()\n",
    "    \n",
    "    #Windows only:  \n",
    "    if os.name == \"nt\":\n",
    "        path_str = str(path)\n",
    "        if not path_str.startswith(\"\\\\\\\\?\\\\\"):\n",
    "            # UNC paths need special handling\n",
    "            if path_str.startswith(\"\\\\\\\\\"):\n",
    "                path_str = \"\\\\\\\\?\\\\UNC\\\\\" + path_str[2:]\n",
    "            else:\n",
    "                path_str = \"\\\\\\\\?\\\\\" + path_str\n",
    "            return path_str\n",
    "    \n",
    "    return path\n",
    "\n",
    "to_erase = [\n",
    "    \"AmorphousLogFileModelCreation.txt\",\n",
    "    \"AmorphousExecution_times.txt\",\n",
    "    \"AmorphousModels\"]\n",
    "\n",
    "for item in to_erase:\n",
    "    path = os.path.abspath(item)\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                log_message(f\"Deleted file: {path}\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                log_message(f\"Deleted folder: {path}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\" Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"Not found (skipped): {path}\")\n",
    "        \n",
    "with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(\"=== Log started ===\\n\")\n",
    "    log_file.write(\"All outputs from functions have a string at the beginning to show the origin:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a226b-f083-4ed4-804f-28141b509b93",
   "metadata": {},
   "source": [
    "## 3. Functions\n",
    "\n",
    "1. **load\\_experiments**. It uses the directory where the array (numerical values) and parameter files resides, picks one polarization column and prepares a list of experiments to feed the ML algorythm. The output is as follows:\n",
    "\n",
    "    *encoded\\_experiments=[(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 1}$,(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 2}$,...]* \n",
    "    \n",
    "    Note that the Cell Id is Hot Encoded. The type of cell did not affect greatly the predictions. However in the future, it may be useful to give more information to Cell_ID. As of 2025, these strings are Hot Encoded which means that the code finds all the different types, creates columns for each type and writes 0 or 1 (bool) depending of whether the cell was from one type or the other. This is the standard procedure to feed categorical variables to ML.\n",
    "    The size of the output list depends on the number of pairs of .txt files present in the folder. This means that it also works for isolated experiments.\n",
    "    \n",
    "2. **augment_experiments** is a function that takes the original data list (as load\\_experiments returns it) and augments them _num\\_augmentations_ times. The final legth of the list will be (num\\_augmentations +1) times the original length. Due to the reduced size of the data base (Not more than 30 experiments before 2026), techniques like augmentation are required. Due to the stochastic nature of neutron detection, most measurements, if repeated under the same conditions, will yield different results. Of course, all measurements converge (with uncertainty) to what we can say is \"the true value\". Therefore, we can duplicate the experiments adding noise to the measurements to obtain \"hypothetical\" measurements that expand the data base. It was decided that the uncertainty won't be modified. To decide what type of noise could be applied two possibilities were considered. The first one was to suppose that sensor detection follows a Poisson distribution (law of rare events). The second one was to suppose that it follows a Normal Distribution of mean the measured value and width the uncertainty. As a Poisson distribution converges stochastically (in probability and distribution but not almost sure nor in $L^p$) to a normal distribution, it is safe to assume that they converge to the same result so the gaussian approach was selected. Also, the measurements are not raw counts but a function of them.\n",
    "$P=\\frac{n^+-n^-}{n^++n^-}$\n",
    "We have no direct data of the number of counts (the parameter of a Posissonian distribution) but a rough estimate points to them being (worst case scenario) within the order of the millions (note that after 100 counts, poisson distributions vary very little from normal distributions). Threfore, count detection can be approximated to a normal distribution and a linear combination of random variables distributed as normal distributions is also a random variable with a normal distribution. Therefore, it is safe to assume that sensor measurements can be modelled after a gaussian distribution.\n",
    "\n",
    "\n",
    "3. **build_dataset** is a function that prepares the data base to be fed directly into a ML model for training and validation. There are a few import decisions taken here. The way this function is set up, it removes 2 rows of data per polariser cell studied. The reason why it was done is because we want the ML model to be able to predict polarization decay when given the specs of the cell (the static parameters) and the initial and final polarization values (with their associated time values). The reason why those two values are considered \"known values\" for each cell is beacuse they can be easily measured experimentally and they give as a good estimate about the overall behaviour. In some experiments, the environment of the studied sample is too fragile to move and place the Si crystal for polarization measurements. Therefore, a working ML algorithm whose inputs are the specs of the cell and the initial and final polarizations (measured before and after the sample is in place) would enable experiments that could not be done before without proper polarization efficiency corrections. \n",
    "Also, we remove those two values per polariser cell from the training arrays (Xt, y and err). The reason why it is done is to avoid data leaks in the model. If we give those values as training data and also give them as parameters, the ML algorithm can run into the risk of memorizing those pairs (over-fitting) and worsen any new predicitions. Uncertainties for the first and last polarization measurements are not added to the static features. This was a decision taken to avoid giving too much weight to two variables that don't have value on their own (they compliment the polarization values but if the ML architecture is as shallow as\n",
    "the one used here, they can be considered independent variables and reduce the weight and importance of the other variables). \n",
    "Another consideration taken here was that the static features get duplicated in Xs a lot of times. One could think that using a similar method that the one used for augmentations of the data sets could also diversify the data base. However, we wanted all measurements of a same session and cell to be coherent \n",
    "and it wouldn't make sense to have different static features. Therefore, the safest approach was to only duplicate these values. For the augmented experiments the only parameters that are changed are the initial and final times and polarizations. There is no incompatibility here to what we have just said as these work as \"hypothetical independent experiments\". This is why augmentation is done before this function gets used.\n",
    "         \n",
    "\n",
    "4. **nll_loss** ML algorythms require a way to tell the algorythm if it is learning or not. The most standard practice is with a Loss function. If the loss value goes down that means that the algorythm is learning and, if a step increases the loss, then it is punished and tries other approaches. When using uncertainties when teaching the model, the most common loss function is the NLL or Negative log-likelihood of a normal distribution \n",
    "NLL$=\\frac{1}{2}$log$(σ^2)+\\frac{(y−μ)^2}{2σ^2}$\n",
    "where $\\sigma$ is the uncertainty in the predictions, $y$ is the measured value and $\\mu$ the predicted value. Instead of predicting $σ^2$ directly, we obtain its logarithm to have a more stable process (and avoid accounting precision as $\\sigma^{-2}$ which is numerically unstable when uncertainties are low).\n",
    "\n",
    "However we want to avoid uncertainties that drift too far from the overall model predictions. To achieve that, we can get a rough estimate on what the uncertainty of a set predictions looks like.\n",
    "Let $\\vec{\\mu}=\\left(\\mu_1,\\ldots,\\mu_N\\right)^T$ be the vector of $N$ predicted values. It can be considered as a random vector of variance:\n",
    "$Var(\\vec{\\mu})=\\frac{1}{N}\\sum_{i=1}^N\\left(\\mu_i-E(\\vec{\\mu})\\right)$\n",
    "where $E(\\vec{\\mu})$ is the mean of the predicted values. We then have two different variances, one obtained as the sparseness of the predictions, (denoted as $Var\\left(\\vec{\\mu}\\right)$, and one obtained as a result of the internal ML calculations (denoted as $\\sigma^2$). A penalty can be added to the loss functions to force the model to try to reduce this differences. An easy way to model it is to obtain the difference of those variances and square the result (taking the absolute value was also a good estimate, but using squared values punished big discrepancies in a stronger way).\n",
    "\n",
    "$StrayPenalty = B \\cdot \\left[\\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}\\left(\\vec{\\mu}\\right)\\right)\\right]^2$\n",
    "where $B$ is a constant used to control the weight of this penalty. The reason why $Var\\left(\\vec{\\mu}\\right)$ was used and not $Var\\left(\\vec{y}\\right)$ (with $\\vec{y}$ the vector of measured values) was to avoid noise in the original data to tamper with the loss function. It would be physically clearer to use measured values sparseness as a way to guide the model but some experimental uncertainties are clearly underestimated and that would cause this penalty to dominate the loss and obscure the main loss protocol, the NLL.\n",
    "\n",
    "Also, we also want to punish the model if it tries overestimating $\\sigma$. If the model is unable to minimize $y-\\mu$, in order to lower NLL, it increases $\\sigma$. If no precautions are taken, this \"escape solution\" achieves bad predictions with inflated uncertainties that simulate a low loss value. A new penalty was added that punishes overestimation of the uncertainties more than underestimation (which never happened). The slope correction done further on the pipeline can \"fix\" this issue but what the model returns then is closer to a poorly calculated linear fit\n",
    "Therefore, an addition penalty was added.\n",
    "\n",
    "$OverestimatePenalty= C \\cdot \\max\\left(0, \\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}(\\vec{\\mu})\\right)\\right)$\n",
    "where $C$ is a constant used to control the weight of this penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24383b6-3dd2-42bb-b5a4-2d54950f930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_experiments(data_dir, polarization_column):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data_dir (str): The direction to the folder where all files to be loaded will be found\n",
    "        polarization_column: It is the name in the header (in the .txt files) of the column that will be read.\n",
    "                             It can be 'SoftPolarizationD3' or 'PolarizationD3'. All results were obtained with 'PolarizationD3'\n",
    "    Output: \n",
    "        A list, for all polarization decays like:\n",
    "        encoded_experiments = [\n",
    "            (static_values,   # list of static parameters (per experiment)\n",
    "            Deltatime,       # 1D numpy array of time values\n",
    "            polarization,    # 1D numpy array of polarization values\n",
    "            Uncertainty      # 1D numpy array of uncertainty values), ...]\n",
    "        A pd object with the Hot encoded Ids and the rest of the parameters per experiment (each experiment in a different row)\n",
    "    \n",
    "    Notes:\n",
    "    The steps the code does are the following:\n",
    "    1. Finds all array files (the ones with the numerical values of the decay) and loops over all of them\n",
    "    2. For each array files it reconstructs the name of the parameter file. It concatenates all parameter files into one pd structure.\n",
    "    3. Finally it loops over all array files appending the parameters, time, polarization and uncertainty of every experiment to a common list\n",
    "    \"\"\"\n",
    "    \n",
    "    log_message(f\"    load_experiments: Finding all Array Files...\")\n",
    "    # Step 1:\n",
    "\n",
    "    arrays_files = sorted(\n",
    "        glob.glob(os.path.join(data_dir, \"*.txt\"))) #Find all files that are .txt\n",
    "    arrays_files = [f for f in arrays_files if not f.endswith(\"_Parameters.txt\")] #Keep only the Arrays (not the parameters)\n",
    "\n",
    "    encoded_experiments = []\n",
    "    all_static_df = []\n",
    "    static_columns = ['CellID','AnalyserID', 'PolariserPressure','AnalyserPressure', 'LabPolarization', 'LabTime'] #Parameter header\n",
    "    for arrays_path in arrays_files:\n",
    "        base = os.path.basename(arrays_path)\n",
    "        # Build parameters filename by adding _Parameters before .txt\n",
    "        name_without_ext = os.path.splitext(base)[0]\n",
    "        parameters_filename = f\"{name_without_ext}_Parameters.txt\"\n",
    "        parameters_path = os.path.join(data_dir, parameters_filename)\n",
    "\n",
    "        # Read parameters file\n",
    "        try:\n",
    "            parameters_df = pd.read_csv(parameters_path) #Import the parameter file\n",
    "            if LogNoise:\n",
    "                log_message(f\"    load_experiments: Reading parameters file: {parameters_filename}\") #Clutter logging\n",
    "\n",
    "            #Get the first row as static data\n",
    "            static_row = parameters_df.iloc[0][static_columns] #Get the parameter numerical values\n",
    "            all_static_df.append(static_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            log_message(f\"    ****load_experiments: Failed to read parameters file: {parameters_filename}, error: {e}\")\n",
    "            continue\n",
    "\n",
    "    log_message(f\"    load_experiments: Create combined DataFrame for static parameters...\")\n",
    "    static_df = pd.DataFrame(all_static_df) #Combine all static rows into a Dataframe\n",
    "\n",
    "    log_message(f\"    load_experiments: Collected static data:\")\n",
    "    \"\"\"\n",
    "    The type of cell did not affect greatly the predictions. However in the future, it may be useful to give more information to the Cell_IDs.\n",
    "    As of 2025, these strings are Hot Encoded which means that the code finds all the different types, creates columns for each type and writes\n",
    "    0 or 1 (bool) depending of whether the cell was from one type or the other. This is the standard procedure to feed categorical varaibles to ML.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_message(f\"    load_experiments: Static dataframe columns:, {static_df.columns.tolist()}\")\n",
    "    log_message(f\"    load_experiments: Static dataframe shape:, {static_df.shape}\")\n",
    "\n",
    "    log_message(f\"    load_experiments: Hot encoding CellID.\")\n",
    "    categorical_cols = ['CellID', 'AnalyserID']\n",
    "    static_df = pd.get_dummies(static_df, columns=categorical_cols, prefix=['CellID', 'AnalyserID'])\n",
    "    # Now second pass: read arrays and create encoded_experiments with encoded static params\n",
    "    for i, arrays_path in enumerate(arrays_files):\n",
    "        base = os.path.basename(arrays_path)\n",
    "        name_without_ext = os.path.splitext(base)[0]\n",
    "        parameters_filename = f\"{name_without_ext}_Parameters.txt\"\n",
    "        parameters_path = os.path.join(data_dir, parameters_filename)\n",
    "        # log_message(f\"Reading arrays file: {base}\") #Clutter log\n",
    "        arrays_df = pd.read_csv(arrays_path) # Reads the time series data \n",
    "    \n",
    "        static_values = static_df.iloc[i].to_list() #Fetches the static parameters corresponding to this experiment\n",
    "    \n",
    "        Deltatime = arrays_df[\"DeltaTime\"].values\n",
    "        polarization = arrays_df[polarization_column].values #Extracts the time array and the selected polarization column.\n",
    "        #Save the uncertainty even if it is not used afterwards\n",
    "        Uncertainty = arrays_df[\"ErrPolarizationD3\"].values\n",
    "        #if len(Deltatime) > 2:\n",
    "        encoded_experiments.append((static_values, Deltatime, polarization, Uncertainty))\n",
    "        # log_message(f\"Creating Encoded Experiments (appending parameters, time array, polarization array and uncertainty array)\") #Clutter log\n",
    "    log_message(f\"    load_experiments: Loaded {len(encoded_experiments)} experiments.\")\n",
    "    return encoded_experiments, static_df.columns.tolist()\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "def augment_experiments(original_experiments, num_augmentations=5, base_seed=42):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        original_experiments (list): A list, for all polarization decays like encoded_experiments in last function:\n",
    "            original_experiments = [\n",
    "                (static_values,   # list of static parameters (per experiment)\n",
    "                Deltatime,       # 1D numpy array of time values\n",
    "                polarization,    # 1D numpy array of polarization values\n",
    "                Uncertainty      # 1D numpy array of uncertainty values), ...]\n",
    "        num_augmentations (int): The number of augmentations done per experiment.\n",
    "        base_seed (int): A seed that guarantees reproducibility\n",
    "    \n",
    "    Outputs:\n",
    "        A list that concatenates the original experiments with the augmented ones.\n",
    "        Instead of being a list of len(original_experiments) it becomes a list of\n",
    "        len(original_experiments) * (1+ num_augmentations)\n",
    "    \n",
    "    Notes: Due to the stochastic nature of neutron detection, most measurements, if\n",
    "    repeated under the same conditions, will yield different results. Of course, all\n",
    "    measurements converge (with uncertainty) to what we can say is \"the true value\".\n",
    "    Therefore, we can duplicate the experiments adding noise to the measurements to obtain\n",
    "    \"hypothetical\" measurements that expand the data base. It was decided that the\n",
    "    uncertainty won't be modified. To decide what type of noise could be applied two\n",
    "    possibilities were considered. The first one was to suppose that sensor detection\n",
    "    follows a Poisson distribution (law of rare events). The second one was to suppose\n",
    "    that it follows a Normal Distribution of mean the measured value and width the uncertainty.\n",
    "    As a poissonian distribution converges stochastically (in probability and distribution\n",
    "    but not almost sure nor in L^p) to a normal distribution, it is safe to assume that they\n",
    "    converge to the same result so the gaussian approach was selected. Also, the measurements\n",
    "    are not raw counts but a function of them. P=(n^+ - n^-)/(n^+ + n^-)\n",
    "    We have no direct data of the number of counts (the parameter of a Posissonian distribution)\n",
    "    but a rough estimate points to them being (worst case scenario) within the order of the millions\n",
    "    (After 100 counts, poisson distributions vary very little from normal distributions).\n",
    "    Threfore, count detection can be approximated to a normal distribution and a linear\n",
    "    combination of random variables distributed as normal distributions is also a random variable\n",
    "    with a normal distribution. Therefore, it is safe to assume that sensor measurements can be\n",
    "    modelled after a gaussian distribution.  \n",
    "    \"\"\"\n",
    "    \n",
    "    log_message(f\"    augment_experiments: Augmenting {len(original_experiments)} a number of {num_augmentations} times\")\n",
    "    augmented_experiments = []\n",
    "    for idx, (static, time, polar, uncertainty) in enumerate(original_experiments):\n",
    "        for n in range(num_augmentations):\n",
    "            seed = hash((idx, n, base_seed)) % 2**32\n",
    "            rng = np.random.default_rng(seed)\n",
    "            noise = rng.normal(loc=0.0, scale=uncertainty)\n",
    "            new_polar = polar + noise\n",
    "            if LogNoise:\n",
    "                log_message(f\"    augment_experiments:       Augmented experiment {idx} #{n} with seed {seed}\")\n",
    "            augmented_experiments.append((static, time, new_polar, uncertainty))\n",
    "    log_message(f\"    augment_experiments: Augmented to {len(original_experiments + augmented_experiments)} experiments\")\n",
    "    return original_experiments + augmented_experiments\n",
    "\n",
    "\n",
    "#######################################################################################3\n",
    "\n",
    "def build_dataset(experiments, mode=\"PolarizationD3\"):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        experiments (list): It is a list like the output of load_experiments or augment_experiments\n",
    "            experiments = [\n",
    "                (static_values,   # list of static parameters (per experiment)\n",
    "                Deltatime,       # 1D numpy array of time values\n",
    "                polarization,    # 1D numpy array of polarization values\n",
    "                Uncertainty      # 1D numpy array of uncertainty values), ...]\n",
    "        mode (str): The column of polarization that will be used. It can be either 'SoftPolarizationD3' or 'PolarizationD3'.\n",
    "    \n",
    "    Output:\n",
    "        1. Xs. An array of lists. Shape (number_of_samples, number_static_features) Each list contains all the original static parameters (hot encoded) plus the first and last polarization values with uncertainty.\n",
    "           To be precise, they get added in this order: static parameters + initial time + initial polarization + final time + final polarization.\n",
    "           For all samples, the static features get added to this list. This means that, if an experiment has M measurements, two get added to the parameter\n",
    "           list and then those parameter features get written M-2 times in this array\n",
    "        2. Xt. An array of time. Shape (number_of_samples, 1). All time values that are not used as parameters get added to this array. However they are not\n",
    "           saved directly. If they are for example 0,120,250 then they get saved as [0],[120],[250]. The reason is compatibility with Keras/TensorFlow.\n",
    "        3. y. An array of polarization. It is the same as Xt but with polarization values (the type of polarization is determined by _mode_)\n",
    "        4. err. An array of polarization uncertainties. It is the same as Xt and y but with polarization uncertainties.\n",
    "    Notes:\n",
    "        If there are only two rows then the file gets skipped. It shouldn't happen but there is logic for it. The reason why it gets skipped is because\n",
    "        there would not be any values left to use for training or validation    \n",
    "    \"\"\"\n",
    "\n",
    "    Xs, Xt, y, u = [], [], [], []\n",
    "    log_message(f\"    build_dataset: Starting build_dataset for column {mode} \")\n",
    "    log_message(f\"    build_dataset: Number of experiments to process: {len(experiments)} (should be (num_augmentations+1)*(number_of_experiments-1)\")\n",
    "    \n",
    "    for exp_idx, (static_params, delta_time, polarization, uncertainty) in enumerate(experiments):\n",
    "        if len(delta_time) < 2:\n",
    "            log_message(f\"    ****build_dataset:       Skipping experiment {exp_idx}: too few data points (len={len(delta_time)})\")\n",
    "            continue\n",
    "            \n",
    "\n",
    "        \n",
    "        if LogNoise:\n",
    "            log_message(f\"    build_dataset: Adding First and Last Polarization (with time) values as static parameters\") #Clutter log\n",
    "        init_idx = 0\n",
    "        final_idx = -1\n",
    "        initial_dt, initial_p = delta_time[init_idx], polarization[init_idx]\n",
    "        final_dt, final_p = delta_time[final_idx], polarization[final_idx]\n",
    "\n",
    "        static_vector = static_params + [\n",
    "            initial_dt, initial_p,\n",
    "            final_dt, final_p\n",
    "        ]\n",
    "        if LogNoise:\n",
    "            log_message(f\"    build_dataset: Experiment {exp_idx}: static_vector length={len(static_vector)} (should be 10 (three parameters, CellID hot encoded creates three posibilities, four for the initial and final polarization) \") #Clutter log\n",
    "        if LogNoise:\n",
    "            log_message(f\"    build_dataset: Building samples Static+time+polarization\") #Clutter log\n",
    "        \n",
    "        for t, p, err in zip(delta_time[1:-1], polarization[1:-1], uncertainty[1:-1]): \n",
    "            Xs.append(static_vector)\n",
    "            Xt.append([t])\n",
    "            y.append(p)\n",
    "            u.append(err) #We will ignore always uncertainty in parameters and even if they are not used, we will keep uncertainties in the data sets(same dimensions everywhere)\n",
    "\n",
    "    log_message(f\"    build_dataset: Number of experiments processed for mode '{mode}': {len(experiments)}\")\n",
    "    log_message(f\"    build_dataset: Final dataset shapes: Xs: {np.array(Xs).shape}, Xt: {np.array(Xt).shape}, y: {np.array(y).reshape(-1, 1).shape}\")\n",
    "    return np.array(Xs), np.array(Xt), np.array(y).reshape(-1, 1), np.array(u).reshape(-1, 1)\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def split_experiments(experiments, val_fraction=0.2, seed=42):\n",
    "    # Tests of validation. Currently unused\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_exp = len(experiments)\n",
    "    indices = rng.permutation(n_exp)\n",
    "\n",
    "    n_val = int(val_fraction * n_exp)\n",
    "    val_idx = indices[:n_val]\n",
    "    train_idx = indices[n_val:]\n",
    "\n",
    "    train_experiments = [experiments[i] for i in train_idx]\n",
    "    val_experiments   = [experiments[i] for i in val_idx]\n",
    "\n",
    "    log_message(f\"Split experiments: {len(train_experiments)} train / {len(val_experiments)} val\")\n",
    "    return train_experiments, val_experiments\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def nll_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        y_true (array): An array of the real values.\n",
    "        y_pred (array): An array of the predicted values\n",
    "    Output:\n",
    "     A scalar tensorflow tensor ( () ) with the value of the loss as the the mean Gaussian negative log-likelihood over the batch\n",
    "    Notes:\n",
    "        This loss function is not just a pure Negative Loss Likelyhood (NLL).\n",
    "        ML algorythms require a way to tell the algorythm if it is learning or not. The most standard practice is with a Loss function.\n",
    "        If the loss value goes down that means that the algorythm is learning and, if a step increases the loss, then it is punished and\n",
    "        tries other approaches. When using uncertainties when teaching the model, the most common loss function is the NLL or Negative\n",
    "        log-likelihood of a normal distribution \n",
    "        \n",
    "        NLL$=\\frac{1}{2}$log$(σ^2)+\\frac{(y−μ)^2}{2σ^2}$\n",
    "        \n",
    "        where $\\sigma$ is the uncertainty in the predictions, $y$ is the measured value and $\\mu$ the predicted value.\n",
    "        Instead of predicting $σ^2$ directly, we obtain its logarithm to have a more stable process (and avoid accounting precision as\n",
    "        $\\sigma^{-2}$ which is numerically unstable when uncertainties are low).\n",
    "\n",
    "        However we want to avoid uncertainties that drift too far from the overall model predictions. To achieve that, we can get a rough\n",
    "        estimate on what the uncertainty of a set predictions looks like. Let $\\vec{\\mu}=\\left(\\mu_1,\\ldots,\\mu_N\\right)^T$ be the vector\n",
    "        of $N$ predicted values. It can be considered as a random vector of variance:\n",
    "        \n",
    "        $Var(\\vec{\\mu})=\\frac{1}{N}\\sum_{i=1}^N\\left(\\mu_i-E(\\vec{\\mu})\\right)$\n",
    "        \n",
    "        where $E(\\vec{\\mu})$ is the mean of the predicted values. We then have two different variances, one obtained as the sparseness of\n",
    "        the predictions, (denoted as $Var\\left(\\vec{\\mu}\\right)$, and one obtained as a result of the internal ML calculations (denoted as\n",
    "        $\\sigma^2$). A penalty can be added to the loss functions to force the model to try to reduce this differences. An easy way to model\n",
    "        it is to obtain the difference of those variances and square the result (taking the absolute value was also a good estimate, but\n",
    "        using squared values punished big discrepancies in a stronger way).\n",
    "\n",
    "        $StrayPenalty = B \\cdot \\left[\\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}\\left(\\vec{\\mu}\\right)\\right)\\right]^2$\n",
    "        \n",
    "        where $B$ is a constant used to control the weight of this penalty. The reason why $Var\\left(\\vec{\\mu}\\right)$ was used and not\n",
    "        $Var\\left(\\vec{y}\\right)$ (with $\\vec{y}$ the vector of measured values) was to avoid noise in the original data to tamper with the\n",
    "        loss function. It would be physically clearer to use measured values sparseness as a way to guide the model but some experimental\n",
    "        uncertainties are clearly underestimated and that would cause this penalty to dominate the loss and obscure the main loss protocol, the NLL.\n",
    "\n",
    "        Also, we also want to punish the model if it tries overestimating $\\sigma$. If the model is unable to minimize $y-\\mu$, in order to\n",
    "        lower NLL, it increases $\\sigma$. If no precautions are taken, this \"escape solution\" achieves bad predictions with inflated\n",
    "        uncertainties that simulate a low loss value. A new penalty was added that punishes overestimation of the uncertainties more than\n",
    "        underestimation (which never happened). The slope correction done further on the pipeline can \"fix\" this issue but what the model\n",
    "        returns then is closer to a poorly calculated linear fit. Therefore, an addition penalty was added.\n",
    "\n",
    "        $OverestimatePenalty= C \\cdot \\max\\left(0, \\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}(\\vec{\\mu})\\right)\\right)$\n",
    "        \n",
    "        where $C$ is a constant used to control the weight of this penalty\n",
    "    \"\"\"\n",
    "\n",
    "    mu = y_pred[:, 0:1]\n",
    "    log_var = y_pred[:, 1:2]\n",
    "\n",
    "    # Base Gaussian NLL\n",
    "    precision = tf.exp(-log_var)\n",
    "    nll = tf.reduce_mean(0.5 * (log_var + tf.square(y_true - mu) * precision))\n",
    "\n",
    "\n",
    "    mu_centered = mu - tf.reduce_mean(mu)\n",
    "    sigma_ref = tf.sqrt(tf.reduce_mean(tf.square(mu_centered)) + 1e-6)\n",
    "    Stray_penalizer = 1e-2 #Parameter used to limit how far the uncertainty predictions stray from the experimental data\n",
    "    OverUncert_penalizer  = 5e-3 #Parameter used for punishing overestimated uncertainties\n",
    "    log_var_prior = tf.math.log(sigma_ref ** 2) #Order of magnitude of what uncertainties should look like\n",
    "\n",
    "    # Prior penalty. It penalizes if uncertainty strays too much from the experimental value\n",
    "    #It avoids underestimation of uncertainty and overestimation\n",
    "    penalty_stray = Stray_penalizer * tf.reduce_mean(tf.square(log_var - log_var_prior))\n",
    "\n",
    "    # Asymmetric penalty: punish overestimation more than underestimation\n",
    "    penalty_overestimate = OverUncert_penalizer * tf.reduce_mean(tf.nn.relu(log_var - log_var_prior))\n",
    "\n",
    "    return nll + penalty_stray + penalty_overestimate            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c37bbb-7d42-4679-815f-e41657c33be6",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "It consists of a single function called _Define\\_Complexity_. Given the name of a model it defines the model function of the ML algorithm. To be precise, it defines a function called _build_model_ every time _Define\\_Complexity_ runs. If _Define\\_Complexity_ gets run another time, it will define (possibly) another _build_model_ if the _Complexity_ variable changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b39473-d2d0-48df-85f4-d9369780d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Define_Complexity(Complexity):\n",
    "    from tensorflow.keras import Input, Model, regularizers\n",
    "    from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense, Dropout, Concatenate\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "    if Complexity == \"Average\":       \n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "\n",
    "    elif Complexity == \"Complex\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.3)(x_time)\n",
    "            x_time = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = Dropout(0.3)(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.3)(x)\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(16, activation='tanh')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "\n",
    "    elif Complexity == \"Simple\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.3)(x_time)\n",
    "            x_time = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-2))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "\n",
    "    elif Complexity == \"Naif\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif834\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif838\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif8316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif8332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif854\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif858\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif8516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif8532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif8104\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif8108\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif81016\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif81032\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    elif Complexity == \"Naif1634\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif1638\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif16316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif16332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif1654\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif1658\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif16516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif16532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif16104\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif16108\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif161016\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif161032\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif Complexity == \"Naif2434\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif2438\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif24316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif24332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif2454\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif2458\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif24516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif24532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif24104\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif24108\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif241016\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif241032\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"NaifTwice1D883316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D883332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D883516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D883532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D885316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D885332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D885516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D885532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D843316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D843332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D843516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D843532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D845316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D845332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D845516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D845532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"NaifTwice1D483316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D483332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D483516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D483532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D485316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D485332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D485516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D485532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D443316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D443332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D443516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D443532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D445316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D445332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D445516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D445532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"NaifTwiceDense414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                    \n",
    "    elif Complexity == \"NaifTwiceDense4116\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense4124\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense8116\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense8124\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense1614\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense1618\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense16116\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense16124\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense2414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense2418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense24116\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense24124\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"NaifTwiceDense424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                    \n",
    "    elif Complexity == \"NaifTwiceDense4216\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense4224\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense8216\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense8224\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense1624\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense1628\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense16216\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense16224\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense2424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense2428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense24216\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense24224\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"SimpleNoDropout444\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"SimpleNoDropout448\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "        \n",
    "    elif Complexity == \"SimpleNoDropout484\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout488\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout4164\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout4168\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "\n",
    "    elif Complexity == \"SimpleNoDropout844\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"SimpleNoDropout848\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"SimpleNoDropout884\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout888\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout8164\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout8168\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model         \n",
    "    elif Complexity == \"SimpleNoDropout1644\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"SimpleNoDropout1648\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"SimpleNoDropout1684\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout1688\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout16164\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout16168\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model         \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"Simple414414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple414418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple414424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple414428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple414814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple414818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple414824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple414828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                 \n",
    "    elif Complexity == \"Simple418414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple418418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple418424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple418428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple418814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple418818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple418824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple418828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model   \n",
    "    elif Complexity == \"Simple424414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple424418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple424424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple424428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple424814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple424818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple424824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple424828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                 \n",
    "    elif Complexity == \"Simple428414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple428418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple428424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple428428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple428814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple428818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple428824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple428828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model   \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"Simple814414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple814418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple814424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple814428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple814814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple814818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple814824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple814828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                 \n",
    "    elif Complexity == \"Simple818414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple818418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple818424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple818428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple818814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple818818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple818824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple818828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model   \n",
    "    elif Complexity == \"Simple824414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple824418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple824424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple824428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple824814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple824818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple824824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple824828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                 \n",
    "    elif Complexity == \"Simple828414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple828418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple828424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple828428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple828814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple828818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple828824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple828828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x_time)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model         \n",
    "        \n",
    "    else:\n",
    "        log_message(f\"****Define_Complexity: UNKNOWN MODEL TYPE. FAIL INCOMING! Please check that the model name actually exists.\")\n",
    "    log_message(f\"Define_Complexity: Model built\\n\")\n",
    "    return build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c177e0-bb41-412d-8e3d-78e29a298529",
   "metadata": {},
   "source": [
    "## 5. Model specific functions\n",
    "\n",
    "Here we have functions that train, validate and fit the models. Some models require the variables to be scaled or will scale them. Extra precautions need to be taken into account\n",
    "\n",
    "1. **model\\_fitting**. It is a function that logs and runs model.fit() on a two-input Keras model and returns the training history. It needs **static, time and polarization variables scaled**. Training is not done using the uncertainties of the data as it was decided that uncertainty information is encoded in the augmentations. Note: No validation is done anywhere in the code. Here are some of the reasons:\n",
    "    \n",
    "    1.1. The data base is very small. The amorphous data base contains only 199 points while the crystalline one contains 251. Removing a small percentage of those points for validation might leave the data base too small and underfitting might worsen the result more than fine tuning parametrs with validation.\n",
    "    \n",
    "    1.2. A randomized validation split may be physically wrong. Therefore it should be chronological, not shuffled. However, in crystalline experiments, there are decay experiments that have only four or five intermediate points. Even removing one point for validation is a massive hit on the experiment. Therefore, it is risky to add validation\n",
    "    \n",
    "    1.3. To find good models, a Leave-one-out approach was used. For a certain model structure, an experiment gets removed and the model and it trains on all the remaining experiments. Then, the model tries to predict this isolated experiment. Afterwards, the experiment is returned and a new one becomes isolated. This process loops for all experiments and an overall score of the model is computed. This process was done for 498 models for crystalline materials. This is a stronger (and more expensive) method than validation as it is not dependent on the validation splits and avoids possible information leaks.\n",
    "\n",
    "Also, eight randomly picked models were tested with and without validation and with and without an asymetric uncertainty-overestimated penalizing loss. The result showed that the Loss update was an improvement and validation did not increase performance (without validation, the results were slightly better).\n",
    "\n",
    "2. **model\\_prediction**. It is a funtion that predicts with a given model. It needs **static and time variables scaled**. This scaling must be coherent to the one done in the rest of the funtions.\n",
    "\n",
    "3. **train**. This function is the one responsible of scaling the inputs and training the model (it uses **model\\_fitting**)\n",
    "\n",
    "     3.1. It creates the independent arrays with all the encoded experiments (augmented or not) using build_dataset\n",
    "    \n",
    "     3.2. Then it scales the data. ML algorithms work better when the inputs and outputs are normalized. The reason why we don't normalize inside the function is to have those scaler defined globally and not locally\n",
    "    \n",
    "     3.3. It builds the model depending on the use\\_uncertainty bool. (It changes the loss function and the output).\n",
    "    \n",
    "     3.4. It trains the model and returns its history (the trained model)\n",
    "    \n",
    "4. **align_static_vectors**. It converts the columns not present on an isolated experiment to zeros.\n",
    "    \n",
    "5. **model_predict_sloped** It substracts a linear function to the predicted values. If done correctly this makes it so that the polarization predictions at the initial and final time points are the same as the measured polarization values at those times. This fixes a vertical shift and also an overall slope. As it is a correction done with experimental values, the algorithm is still \"universal\". However we can't fully say that it is a pure ML algorithm. The \"correctness\" of this method is subjective. It is a warning in the ML front that there is an issue with the data base but it is a valid fix for experimentalists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6efc83-4769-4a97-a3c1-e793af16c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fitting(model, X_static_scaled, X_time_scaled, y_scaled, epochs, batch_size, verbose, callbacks=None):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        model (keras model): This is the object that the build_model function returns.\n",
    "        X_static_scaled (array): It is an array os size (total number of intermediate measured points (let's call them 'samples'), number of static features). \n",
    "                                 Using the variables form build_dataset, the number of static features is 4+len(static_parameters(hot encoded)).\n",
    "                                 To be precise, it is just Xs scaled (an array of lists with the static features) but reshpaed into a 2d array\n",
    "        X_time_scaled (array): A 2d array of shape that reshapes Xt scaled from an array of lists (of size 1 like np.array([1],[13],[16],...)) to a 2D array\n",
    "        y_scaled (array): The same as X_time_scaled but with polarization values\n",
    "        epochs (int): Number of training epochs (times the model tries to validate the data and recalculates its parameters)\n",
    "        batch_size (int): The number of samples for every gradient update\n",
    "        verbose (int): It limits how much training output is printed\n",
    "        callbacks (str): It allows certain Keras callbacks (special code properties of keras). EarlyStopping, ReduceLROnPlateau or ModelCheckpoint are examples\n",
    "    Outputs:\n",
    "       A keras.callbacks.History object\n",
    "    Note:\n",
    "        It logs and runs model.fit() on a two-input Keras model and returns the training history.\n",
    "        IT REQUIRES THE INPUTS (static, time and polarization) TO BE NORMALIZED/SCALED\n",
    "        Training is not done using the uncertainties of the data.  \n",
    "    \"\"\"\n",
    "    log_message(f\"    Model_fitting: Training the model.\")\n",
    "    return model.fit(\n",
    "        [X_static_scaled, X_time_scaled],\n",
    "        y_scaled,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "\n",
    "def model_prediction(model, X_static_scaled, X_time_scaled):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X_static_scaled (array): A 2d array of Xs (samples, number of static features)\n",
    "        X_time_scaled (array): A 2d array of shape that reshapes Xt scaled from an array of lists (of size 1 like np.array([1],[13],[16],...)) to a 2D array\n",
    "        y_scaled (array): The same as X_time_scaled but with polarization values. It doesn't have to be the same as the one used in training\n",
    "    Output:\n",
    "        A 2d array with the predictions for those time values. It contains a column of the polarization predictions and another with the log of the variance\n",
    "    Notes:\n",
    "        IT REQUIRES THE INPUTS (static and time) TO BE NORMALIZED/SCALED\n",
    "    \"\"\"\n",
    "    log_message(f\"    model_prediction: Predicting {len(X_time_scaled)} time points\")\n",
    "    return model.predict([X_static_scaled, X_time_scaled], verbose=0)\n",
    "\n",
    "def train(encoded_experiments, scaler_static, scaler_time, scaler_y, use_uncertainty):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        encoded_experiments (list): A list like this:\n",
    "            encoded_experiments = [\n",
    "                (static_values,   # list of static parameters (per experiment)\n",
    "                Deltatime,       # 1D numpy array of time values\n",
    "                polarization,    # 1D numpy array of polarization values\n",
    "                Uncertainty      # 1D numpy array of uncertainty values), ...] \n",
    "        scaler_static: It is a scikit-learn scaler for the static features. Only MinMaxScaler (normalizes uniformly using the maximum and the minimum)\n",
    "        scaler_time: Analogously to scaler_static\n",
    "        scaler_y: Analogously to scaler_static\n",
    "        use_uncertainty (bool): It toggles whether it uses uncertainty for predictions or not. This changes the output of the model (mean and log_var or just the mean)\n",
    "    Output:\n",
    "        A fully trained model that can be used by model_predictions\n",
    "    Notes:\n",
    "        This function is the one responsible of scaling the inputs and training the model\n",
    "        1. It creates the independent arrays with all the encoded experiments (augmented or not) using build_dataset\n",
    "        2. Then it scales the data. ML algorithms work better when the inputs and outputs are normalized.\n",
    "           The reason why we don't normalize inside the function is to have those scaler defined globally and not locally\n",
    "        3. It builds the model depending on the use_uncertainty bool. (It changes the loss function and the output).\n",
    "        4. It trains the model and returns its history (the trained model)\n",
    "    \n",
    "    \"\"\"\n",
    "    log_message(f\"    train: Begin training and scaling with {len(encoded_experiments)} experiments.\")\n",
    "    X_static_all, X_time_all, y_all, u_all = build_dataset(encoded_experiments)\n",
    "    log_message(f\"Scaling all data\")\n",
    "    X_static_scaled = scaler_static.transform(X_static_all) #It only fits to the scaler. IT DOESN'T OVERWRITE THEM. WHAT A WASTE OF 20 DAYS OF MY LIFE >:(\n",
    "    X_time_scaled = scaler_time.transform(X_time_all)\n",
    "    y_scaled = scaler_y.transform(y_all)\n",
    "    \n",
    "    model = build_model(X_static_all.shape[1], use_uncertainty=use_uncertainty)\n",
    "    epochs = 300\n",
    "    batch_size = 32\n",
    "    log_message(f\"    train: Training final model with epochs={epochs}, batch_size={batch_size} and use_uncertainty = {use_uncertainty}\")\n",
    "    model_fitting(model, X_static_scaled, X_time_scaled, y_scaled, epochs, batch_size, verbose=0)\n",
    "    return model\n",
    "\n",
    "############################################################\n",
    "\n",
    "def align_static_vectors(experiments, static_columns_training, static_columns_isolated):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        experiments (list): A list like this:\n",
    "            experiments = [\n",
    "                (static_values,   # list of static parameters (per experiment)\n",
    "                Deltatime,       # 1D numpy array of time values\n",
    "                polarization,    # 1D numpy array of polarization values\n",
    "                Uncertainty      # 1D numpy array of uncertainty values), ...] \n",
    "        static_columns_training (list): A list of strings of all the static feature names used for training\n",
    "        static_columns_isolated (list): A list of strings of all the static feature names of the isolated experiment\n",
    "    Output:\n",
    "        aligned_experiments is a list of experiments with static vectors aligned to the training feature order.\n",
    "        To be precise (aligned_static, delta_time, polarization, uncertainty) where on the parameters where static_columns_isolated\n",
    "        had no values get turned into zeros. So, if static_columns_isolated doesn´t have a parameter 'Parameter 6', then, in the\n",
    "        original experiments lists, all numbers associated to 'Parameter 6' get turned to zero\n",
    "\n",
    "    Notes:\n",
    "        It helps the ML algorithm to focus on the important parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    aligned_experiments = []\n",
    "    for static, delta_time, polarization, uncertainty in experiments:\n",
    "        static_dict = dict(zip(static_columns_isolated, static))\n",
    "        aligned_static = [static_dict.get(col, 0.0) for col in static_columns_training]\n",
    "        aligned_experiments.append((aligned_static, delta_time, polarization, uncertainty))\n",
    "    log_message(f\"    align_static_vectors: Vectors aligned\")\n",
    "    return aligned_experiments\n",
    "\n",
    "def model_predict_sloped(model, X_static_scaled, X_time_scaled, m, n, scaler_y, scaler_time):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: A trained model\n",
    "        X_static_scaled (array). A 2d array of Xs (samples, number of static features)\n",
    "        X_time_scaled (array). A 2d array of shape that reshapes Xt scaled from an array of lists (of size 1 like np.array([1],[13],[16],...)) to a 2D array\n",
    "        m (float): The slope used for correction\n",
    "        n (float): The polarization shift for correction\n",
    "        scaler_time: It is a scikit-learn scaler for the time arrays. Only MinMaxScaler (normalizes uniformly using the maximum and the minimum)\n",
    "        scaler_y: Analogously to scaler_static\n",
    "    Outputs\n",
    "        y_pred_corrected_scaled is a 2D array where each column is the corrected predicted values (scaled) and the second one is the log of the variance (unchanged, a.k.a scaled)\n",
    "    Notes:\n",
    "        1. It predicts the polarization values\n",
    "        2. It inverse transforms the predicted values and the time points (not the uncertainty)\n",
    "        3. It calculates the correction curve in real units and corrects the predicted values.\n",
    "        4. Finally, it scales back the corrected polarization values and joins them with the unchanged logarithm of the variance\n",
    "        In summary, it substracts a linear function to the predicted values. If done correctly this makes it so that the polarization\n",
    "        predictions at the time points stored as static features are the same as the measured polarization values at those times.\n",
    "        This fixes a vertical shift and also an overall slope. As it is a correction done with experimental values, the algorithm is still\n",
    "        universal. However we can't fully say that it is a pure ML algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_message(f\"    model_predict_sloped: Correcting {len(X_time_scaled)} points by subtracting P(t) = {float(np.squeeze(m)):.3e} * t + {float(np.squeeze(n)):.3e}\")    # Step 1: Get scaled predictions from model\n",
    "    y_pred_scaled = model_prediction(model, X_static_scaled, X_time_scaled)\n",
    "    mean_scaled = y_pred_scaled[:, 0:1]  # shape (N,1)\n",
    "    mean_real = scaler_y.inverse_transform(mean_scaled)  # shape (N,1)\n",
    "    time_real = scaler_time.inverse_transform(X_time_scaled)  # shape (N,1)\n",
    "    \n",
    "    correction = m * time_real + n  # shape (N,1)\n",
    "    mean_corrected_real = mean_real - correction  # shape (N,1)\n",
    "    \n",
    "    mean_corrected_scaled = scaler_y.transform(mean_corrected_real)  # shape (N,1)\n",
    "    log_var_scaled = y_pred_scaled[:, 1:2]  # shape (N,1)\n",
    "    y_pred_corrected_scaled = np.hstack([mean_corrected_scaled, log_var_scaled])  # shape (N,2)\n",
    "    \n",
    "    return y_pred_corrected_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff597bcb-9ff7-4ae9-af36-c87c8e73af14",
   "metadata": {},
   "source": [
    "## 6. Model creation\n",
    "\n",
    "First it prepares the folder structure and the folder where the model will be stored.\n",
    "It then loops (if told to) over all polarization columns to create models for both.\n",
    "It extracts all information from the data base, scales it and saves those scalers\n",
    "Finally it trains the model and saves it in a distinct folder. That folder contains the scalers (in order to decipher how to scale it back to physical units) and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d2c87a-5bf3-49c9-9414-eb2742ef3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Full_model_training(data_dir, model_type, output_folder, use_uncertainty, num_augmentations, build_model):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data_dir (str): The directory folder that contains all the files for the data base\n",
    "        model_type (str): A string with the name of the model\n",
    "        use_uncertainty (bool): A bool variable that can toggle whether the model uses or not uncertainties in their output (not during training)\n",
    "        num_augmentations (int): The number of augmentations used in the model.\n",
    "        build_model (function): It uses the model defined from Define_Complexity. That function gets called outside this function but the build_model \n",
    "        function needs to be given as an argument for isolate_experiments\n",
    "    \n",
    "    Outputs:\n",
    "        None\n",
    "    Notes:\n",
    "        First it prepares the folder structure and the folder where the model will be stored.\n",
    "        It then loops (if told to) over all polarization columns to create models for both.\n",
    "        It extracts all information from the data base, scales it and saves those scalers\n",
    "        Finally it trains the model and saves it\n",
    "    \"\"\"\n",
    "    #1. Prepare folders and subfolders for the newly created model\n",
    "    data_dir = Path(data_dir)\n",
    "    output_folder = Path(output_folder)\n",
    "    output_subfolder = output_folder / f\"Model_{model_type}_{num_augmentations}\"\n",
    "    output_subfolder.mkdir(exist_ok=True)\n",
    "    log_message(f\"Full_model_training: Created folder: {output_subfolder}\")\n",
    "\n",
    "    # 2. Train and predict using both modes or whatever mode you choose\n",
    "    for mode in [\"PolarizationD3\"]: \n",
    "        # 3. Load all experiments and augment them\n",
    "        log_message(f\"Full_model_training: Load all experiments and augment them {num_augmentations} times.\")            \n",
    "\n",
    "        experiments, static_columns = load_all_experiments(data_dir, polarization_column=mode)\n",
    "        encoded_experiments = augment_experiments(experiments, num_augmentations, base_seed=42)\n",
    "\n",
    "        \n",
    "        #4. Scalers. All polarization and time values are scaled using all the experiments. The static parameters are scaled afterwards\n",
    "        log_message(f\"Full_model_training: Scaling all training experiments...\")\n",
    "        all_time = []\n",
    "        all_y = []\n",
    "        \n",
    "        for static, time, pol, _ in encoded_experiments:\n",
    "            all_time.append(time)\n",
    "            all_y.append(pol)\n",
    "        \n",
    "        all_time = np.concatenate(all_time).reshape(-1, 1) #unscaled\n",
    "        all_y = np.concatenate(all_y).reshape(-1, 1) #unscaled\n",
    "        \n",
    "        scaler_time = MinMaxScaler().fit(all_time) #fit to all\n",
    "        scaler_y = MinMaxScaler().fit(all_y) #fit to all\n",
    "        \n",
    "        # Prepare all the experiments so that they can be introduced in the training functions\n",
    "        X_static_raw, X_time_raw, y_raw, u = build_dataset(encoded_experiments, mode=mode)\n",
    "        scaler_static = MinMaxScaler().fit(X_static_raw) #fit to all\n",
    "\n",
    "        if LogNoise:\n",
    "            log_message(f\"      Scaler static min_: , {scaler_static.min_}\")\n",
    "            log_message(f\"      Scaler static scale_: , {scaler_static.scale_}\")\n",
    "            log_message(f\"      Scaler static data_min_: , {scaler_static.data_min_}\")\n",
    "            log_message(f\"      Scaler static data_max_: , {scaler_static.data_max_}\")\n",
    "            \n",
    "            log_message(f\"      Scaler time min_: , {scaler_time.min_}\")\n",
    "            log_message(f\"      Scaler time scale_: , {scaler_time.scale_}\")\n",
    "            log_message(f\"      Scaler time data_min_: , {scaler_time.data_min_}\")\n",
    "            log_message(f\"      Scaler time data_max_: , {scaler_time.data_max_}\")\n",
    "            \n",
    "            log_message(f\"      Scaler polarization min_: , {scaler_y.min_}\")\n",
    "            log_message(f\"      Scaler polarization scale_: , {scaler_y.scale_}\")\n",
    "            log_message(f\"      Scaler polarization data_min_: , {scaler_y.data_min_}\")\n",
    "            log_message(f\"      Scaler polarization data_max_: , {scaler_y.data_max_}\")\n",
    "        \n",
    "        # Save the scalers. This is useful because otherwise we have no way to use the model outside of the loop (because we don't know how to unscale the values)\n",
    "        joblib.dump(scaler_static, long_path(output_subfolder / f\"scaler_static_{mode}.pkl\"))\n",
    "        joblib.dump(scaler_time, long_path(output_subfolder / f\"scaler_time_{mode}.pkl\"))\n",
    "        joblib.dump(scaler_y, long_path(output_subfolder / f\"scaler_y_{mode}.pkl\"))\n",
    "        log_message(f\"Full_model_training: Saved scalers for mode {mode} to: {output_subfolder}\")\n",
    "\n",
    "        #5. Train and save the model on the augmented experiments. The experiments enter unscaled and get scaled inside)\n",
    "        log_message(f\"Full_model_training: Train the model\")\n",
    "        model = train(\n",
    "            encoded_experiments=encoded_experiments,  \n",
    "            scaler_static=scaler_static,\n",
    "            scaler_time=scaler_time,\n",
    "            scaler_y=scaler_y,\n",
    "            use_uncertainty=use_uncertainty)\n",
    "\n",
    "\n",
    "        model_path = output_subfolder / f\"Model_{model_type}_{num_augmentations}.keras\"\n",
    "        model.save(long_path(model_path))\n",
    "        log_message(f\"Full_model_training: Saved model to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9518455-29ea-4b07-9f8b-114c2670350b",
   "metadata": {},
   "source": [
    "## 7 Main Code\n",
    "\n",
    "By changing the lists Complexity and num_augmentations you can pick what model you create. Just write on the two lists your model names and the number of augmentations and it will create models combining them \n",
    "As an example,\n",
    "\n",
    "Names = [\"ModelA\",\"ModelB\",\"ModelC\"]\n",
    "\n",
    "Augmentations = [5,9,2]\n",
    "\n",
    "will create the models \"ModelA_5\",\"ModelB_9\" and \"ModelC_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b1e62a-0d42-45bc-a057-5c3f0f5fc57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model Naif8332_5\n",
      "Creating model Naif8332_5\n",
      "Define_Complexity: Model built\n",
      "\n",
      "Full_model_training: Created folder: \\\\?\\C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousModels\\Model_Naif8332_5\n",
      "Full_model_training: Load all experiments and augment them 5 times.\n",
      "    load_experiments: Finding all Array Files...\n",
      "    load_experiments: Create combined DataFrame for static parameters...\n",
      "    load_experiments: Collected static data:\n",
      "    load_experiments: Static dataframe columns:, ['CellID', 'AnalyserID', 'PolariserPressure', 'AnalyserPressure', 'LabPolarization', 'LabTime']\n",
      "    load_experiments: Static dataframe shape:, (25, 6)\n",
      "    load_experiments: Hot encoding CellID.\n",
      "    load_experiments: Loaded 25 experiments.\n",
      "    augment_experiments: Augmenting 25 a number of 5 times\n",
      "    augment_experiments: Augmented to 150 experiments\n",
      "Full_model_training: Scaling all training experiments...\n",
      "    build_dataset: Starting build_dataset for column PolarizationD3 \n",
      "    build_dataset: Number of experiments to process: 150 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "    build_dataset: Number of experiments processed for mode 'PolarizationD3': 150\n",
      "    build_dataset: Final dataset shapes: Xs: (1194, 13), Xt: (1194, 1), y: (1194, 1)\n",
      "Full_model_training: Saved scalers for mode PolarizationD3 to: \\\\?\\C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousModels\\Model_Naif8332_5\n",
      "Full_model_training: Train the model\n",
      "    train: Begin training and scaling with 150 experiments.\n",
      "    build_dataset: Starting build_dataset for column PolarizationD3 \n",
      "    build_dataset: Number of experiments to process: 150 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "    build_dataset: Number of experiments processed for mode 'PolarizationD3': 150\n",
      "    build_dataset: Final dataset shapes: Xs: (1194, 13), Xt: (1194, 1), y: (1194, 1)\n",
      "Scaling all data\n",
      "    train: Training final model with epochs=300, batch_size=32 and use_uncertainty = True\n",
      "    Model_fitting: Training the model.\n",
      "Full_model_training: Saved model to \\\\?\\C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousModels\\Model_Naif8332_5\\Model_Naif8332_5.keras\n",
      "Execution finished in 50.601619 seconds. Logged to C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousExecution_times.txt\n",
      "\n",
      " _______________________________________________________ \n",
      " \n",
      "Creating model Naif854_6\n",
      "Creating model Naif854_6\n",
      "Define_Complexity: Model built\n",
      "\n",
      "Full_model_training: Created folder: \\\\?\\C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousModels\\Model_Naif854_6\n",
      "Full_model_training: Load all experiments and augment them 6 times.\n",
      "    load_experiments: Finding all Array Files...\n",
      "    load_experiments: Create combined DataFrame for static parameters...\n",
      "    load_experiments: Collected static data:\n",
      "    load_experiments: Static dataframe columns:, ['CellID', 'AnalyserID', 'PolariserPressure', 'AnalyserPressure', 'LabPolarization', 'LabTime']\n",
      "    load_experiments: Static dataframe shape:, (25, 6)\n",
      "    load_experiments: Hot encoding CellID.\n",
      "    load_experiments: Loaded 25 experiments.\n",
      "    augment_experiments: Augmenting 25 a number of 6 times\n",
      "    augment_experiments: Augmented to 175 experiments\n",
      "Full_model_training: Scaling all training experiments...\n",
      "    build_dataset: Starting build_dataset for column PolarizationD3 \n",
      "    build_dataset: Number of experiments to process: 175 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "    build_dataset: Number of experiments processed for mode 'PolarizationD3': 175\n",
      "    build_dataset: Final dataset shapes: Xs: (1393, 13), Xt: (1393, 1), y: (1393, 1)\n",
      "Full_model_training: Saved scalers for mode PolarizationD3 to: \\\\?\\C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousModels\\Model_Naif854_6\n",
      "Full_model_training: Train the model\n",
      "    train: Begin training and scaling with 175 experiments.\n",
      "    build_dataset: Starting build_dataset for column PolarizationD3 \n",
      "    build_dataset: Number of experiments to process: 175 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "    build_dataset: Number of experiments processed for mode 'PolarizationD3': 175\n",
      "    build_dataset: Final dataset shapes: Xs: (1393, 13), Xt: (1393, 1), y: (1393, 1)\n",
      "Scaling all data\n",
      "    train: Training final model with epochs=300, batch_size=32 and use_uncertainty = True\n",
      "    Model_fitting: Training the model.\n",
      "Full_model_training: Saved model to \\\\?\\C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousModels\\Model_Naif854_6\\Model_Naif854_6.keras\n",
      "Execution finished in 59.624574 seconds. Logged to C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousExecution_times.txt\n",
      "\n",
      " _______________________________________________________ \n",
      " \n",
      "Creating model SimpleNoDropout1648_7\n",
      "Creating model SimpleNoDropout1648_7\n",
      "Define_Complexity: Model built\n",
      "\n",
      "Full_model_training: Created folder: \\\\?\\C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousModels\\Model_SimpleNoDropout1648_7\n",
      "Full_model_training: Load all experiments and augment them 7 times.\n",
      "    load_experiments: Finding all Array Files...\n",
      "    load_experiments: Create combined DataFrame for static parameters...\n",
      "    load_experiments: Collected static data:\n",
      "    load_experiments: Static dataframe columns:, ['CellID', 'AnalyserID', 'PolariserPressure', 'AnalyserPressure', 'LabPolarization', 'LabTime']\n",
      "    load_experiments: Static dataframe shape:, (25, 6)\n",
      "    load_experiments: Hot encoding CellID.\n",
      "    load_experiments: Loaded 25 experiments.\n",
      "    augment_experiments: Augmenting 25 a number of 7 times\n",
      "    augment_experiments: Augmented to 200 experiments\n",
      "Full_model_training: Scaling all training experiments...\n",
      "    build_dataset: Starting build_dataset for column PolarizationD3 \n",
      "    build_dataset: Number of experiments to process: 200 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "    build_dataset: Number of experiments processed for mode 'PolarizationD3': 200\n",
      "    build_dataset: Final dataset shapes: Xs: (1592, 13), Xt: (1592, 1), y: (1592, 1)\n",
      "Full_model_training: Saved scalers for mode PolarizationD3 to: \\\\?\\C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousModels\\Model_SimpleNoDropout1648_7\n",
      "Full_model_training: Train the model\n",
      "    train: Begin training and scaling with 200 experiments.\n",
      "    build_dataset: Starting build_dataset for column PolarizationD3 \n",
      "    build_dataset: Number of experiments to process: 200 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "    build_dataset: Number of experiments processed for mode 'PolarizationD3': 200\n",
      "    build_dataset: Final dataset shapes: Xs: (1592, 13), Xt: (1592, 1), y: (1592, 1)\n",
      "Scaling all data\n",
      "    train: Training final model with epochs=300, batch_size=32 and use_uncertainty = True\n",
      "    Model_fitting: Training the model.\n",
      "Full_model_training: Saved model to \\\\?\\C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousModels\\Model_SimpleNoDropout1648_7\\Model_SimpleNoDropout1648_7.keras\n",
      "Execution finished in 70.671409 seconds. Logged to C:\\Users\\gopeb\\Desktop\\D3MLPolarization-final\\D3MLPolarization-main\\CreateModels\\ML\\AmorphousExecution_times.txt\n",
      "\n",
      " _______________________________________________________ \n",
      " \n"
     ]
    }
   ],
   "source": [
    "Names = [\"Naif8332\",\"Naif854\",\"SimpleNoDropout1648\"]\n",
    "Augmentations = [5,6,7]\n",
    "use_uncertainty = True\n",
    "\n",
    "\n",
    "# Define your paths\n",
    "data_dir = Path(\"../FileReadingStoring/AmorphousMLDataBase/\").resolve()\n",
    "log_file = Path.cwd().resolve() / \"AmorphousExecution_times.txt\"\n",
    "log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "os.makedirs(long_path(data_dir), exist_ok=True)\n",
    "\n",
    "for i in range (0,len(Names)):\n",
    "    start_time = time.time()\n",
    "    print(f\"Creating model {Names[i]}_{Augmentations[i]}\")\n",
    "    log_message(f\"Creating model {Names[i]}_{Augmentations[i]}\")\n",
    "    build_model = Define_Complexity(Names[i])\n",
    "    output_folder = Path(\"..\") / \"ML\" / f\"AmorphousModels\"\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    Full_model_training(\n",
    "        data_dir=long_path(data_dir),\n",
    "        model_type=Names[i],\n",
    "        output_folder=long_path(output_folder),\n",
    "        use_uncertainty=use_uncertainty,\n",
    "        num_augmentations=Augmentations[i],\n",
    "        build_model=build_model\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    with open(long_path(log_file), \"a\") as f:\n",
    "        f.write(f\"Execution time={elapsed_time:.6f} seconds for Crystal {Names[i]} {Augmentations[i]}\\n\")\n",
    "    log_message(f\"Execution finished in {elapsed_time:.6f} seconds. Logged to {log_file}\")\n",
    "    log_message(f\"\\n _______________________________________________________ \\n \")\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4629e2-838b-4bbf-b2af-0d2a809b4400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1114931-552e-4ee7-807a-b42f00d33ce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1efc6f-d101-4aaa-a8d9-354ef11f53eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25403347-ef0b-4ca4-b108-35f84146be12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built model Naif8108\n",
      "Naif8108\n",
      "Begin Naif8108 model with 5 augmentations\n",
      "Created folder: \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_Naif8108_5\n",
      "Finding all Array Files...\n",
      "Create combined DataFrame for static parameters...\n",
      "Collected static data:\n",
      "Static dataframe columns: ['CellID', 'AnalyserID', 'PolariserPressure', 'AnalyserPressure', 'LabPolarization', 'LabTime']\n",
      "Static dataframe shape: (25, 6)\n",
      "Hot encoding CellID PLACEHOLDER, VISIT TO THE LAB MAY BE NEEDED\n",
      "Loaded 25 experiments.\n",
      "Augmenting 25 a number of 5 times\n",
      "Augmented to 150 experiments\n",
      "Scale all experiments\n",
      "Starting build_dataset for column PolarizationD3 \n",
      "Number of experiments to process: 150 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "Number of experiments processed for mode 'PolarizationD3': 150\n",
      "Final dataset shapes: Xs: (1194, 13), Xt: (1194, 1), y: (1194, 1)\n",
      "Saved scalers to: \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_Naif8108_5\n",
      "Begin training and scaling with 150 experiments.\n",
      "Starting build_dataset for column PolarizationD3 \n",
      "Number of experiments to process: 150 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "Number of experiments processed for mode 'PolarizationD3': 150\n",
      "Final dataset shapes: Xs: (1194, 13), Xt: (1194, 1), y: (1194, 1)\n",
      "Scaling all data\n",
      "Training final model with epochs=300, batch_size=32 and use_uncertainty = True\n",
      "Training the model...\n",
      "Saved model to \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_Naif8108_5\\Model_Naif8108_5.keras\n",
      "Execution finished in 83.564177 seconds (Complexity=Naif8108, Augmentations=5). Logged to AmorphousExecution_times.txt\n",
      "\n",
      " _______________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Complexity in [\"Naif8108\"]: \n",
    "    build_model = Define_Complexity(Complexity)\n",
    "    print(Complexity)\n",
    "    for num_augmentations in [5]:  # idem with augmentations\n",
    "        log_message(f\"Begin {Complexity} model with {num_augmentations} augmentations\")\n",
    "        # Delete folder if exists\n",
    "        # Move all files back\n",
    "        # Path to the log file\n",
    "        log_file = Path(\"AmorphousExecution_times.txt\")\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "       \n",
    "        # Recreate output folder\n",
    "        output_folder = Path(f\"AmorphousModels\").resolve()\n",
    "        output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "        # Run main function\n",
    "        use_uncertainty = True\n",
    "\n",
    "\n",
    "        isolate_experiments(\n",
    "            data_dir=win_long_path(data_dir),\n",
    "            model_type=model_type,\n",
    "            output_folder=win_long_path(output_folder),\n",
    "            use_uncertainty=use_uncertainty,\n",
    "            num_augmentations=num_augmentations,\n",
    "            build_model=build_model\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        # Ensure the parent directory exists\n",
    "        log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Open with Windows long path safety\n",
    "        with open(win_long_path(str(log_file)), \"a\") as f:\n",
    "            f.write(f\"Execution time={elapsed_time:.6f} seconds for Amorphous {Complexity} {num_augmentations}\\n\")\n",
    "        \n",
    "        log_message(\n",
    "            f\"Execution finished in {elapsed_time:.6f} seconds \"\n",
    "            f\"(Complexity={Complexity}, Augmentations={num_augmentations}). \"\n",
    "            f\"Logged to {log_file}\"\n",
    "        )\n",
    "        log_message(f\"\\n _______________________________________________________ \\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e24987f-8ae4-40d2-b854-8c75139af3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built model Naif858\n",
      "Naif858\n",
      "Begin Naif858 model with 8 augmentations\n",
      "Created folder: \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_Naif858_8\n",
      "Finding all Array Files...\n",
      "Create combined DataFrame for static parameters...\n",
      "Collected static data:\n",
      "Static dataframe columns: ['CellID', 'AnalyserID', 'PolariserPressure', 'AnalyserPressure', 'LabPolarization', 'LabTime']\n",
      "Static dataframe shape: (25, 6)\n",
      "Hot encoding CellID PLACEHOLDER, VISIT TO THE LAB MAY BE NEEDED\n",
      "Loaded 25 experiments.\n",
      "Augmenting 25 a number of 8 times\n",
      "Augmented to 225 experiments\n",
      "Scale all experiments\n",
      "Starting build_dataset for column PolarizationD3 \n",
      "Number of experiments to process: 225 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "Number of experiments processed for mode 'PolarizationD3': 225\n",
      "Final dataset shapes: Xs: (1791, 13), Xt: (1791, 1), y: (1791, 1)\n",
      "Saved scalers to: \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_Naif858_8\n",
      "Begin training and scaling with 225 experiments.\n",
      "Starting build_dataset for column PolarizationD3 \n",
      "Number of experiments to process: 225 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "Number of experiments processed for mode 'PolarizationD3': 225\n",
      "Final dataset shapes: Xs: (1791, 13), Xt: (1791, 1), y: (1791, 1)\n",
      "Scaling all data\n",
      "Training final model with epochs=300, batch_size=32 and use_uncertainty = True\n",
      "Training the model...\n",
      "Saved model to \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_Naif858_8\\Model_Naif858_8.keras\n",
      "Execution finished in 104.460081 seconds (Complexity=Naif858, Augmentations=8). Logged to AmorphousExecution_times.txt\n",
      "\n",
      " _______________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Complexity in [\"Naif858\"]: \n",
    "    build_model = Define_Complexity(Complexity)\n",
    "    print(Complexity)\n",
    "    for num_augmentations in [8]:  # idem with augmentations\n",
    "        log_message(f\"Begin {Complexity} model with {num_augmentations} augmentations\")\n",
    "        # Delete folder if exists\n",
    "        # Move all files back\n",
    "        # Path to the log file\n",
    "        log_file = Path(\"AmorphousExecution_times.txt\")\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "       \n",
    "        # Recreate output folder\n",
    "        output_folder = Path(f\"AmorphousModels\").resolve()\n",
    "        output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "        # Run main function\n",
    "        use_uncertainty = True\n",
    "\n",
    "\n",
    "        isolate_experiments(\n",
    "            data_dir=win_long_path(data_dir),\n",
    "            model_type=model_type,\n",
    "            output_folder=win_long_path(output_folder),\n",
    "            use_uncertainty=use_uncertainty,\n",
    "            num_augmentations=num_augmentations,\n",
    "            build_model=build_model\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        # Ensure the parent directory exists\n",
    "        log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Open with Windows long path safety\n",
    "        with open(win_long_path(str(log_file)), \"a\") as f:\n",
    "            f.write(f\"Execution time={elapsed_time:.6f} seconds for Amorphous {Complexity} {num_augmentations}\\n\")\n",
    "        \n",
    "        log_message(\n",
    "            f\"Execution finished in {elapsed_time:.6f} seconds \"\n",
    "            f\"(Complexity={Complexity}, Augmentations={num_augmentations}). \"\n",
    "            f\"Logged to {log_file}\"\n",
    "        )\n",
    "        log_message(f\"\\n _______________________________________________________ \\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83201de8-cbfa-47f9-bb13-989ae933464f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built model Naif16108\n",
      "Naif16108\n",
      "Begin Naif16108 model with 14 augmentations\n",
      "Created folder: \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_Naif16108_14\n",
      "Finding all Array Files...\n",
      "Create combined DataFrame for static parameters...\n",
      "Collected static data:\n",
      "Static dataframe columns: ['CellID', 'AnalyserID', 'PolariserPressure', 'AnalyserPressure', 'LabPolarization', 'LabTime']\n",
      "Static dataframe shape: (25, 6)\n",
      "Hot encoding CellID PLACEHOLDER, VISIT TO THE LAB MAY BE NEEDED\n",
      "Loaded 25 experiments.\n",
      "Augmenting 25 a number of 14 times\n",
      "Augmented to 375 experiments\n",
      "Scale all experiments\n",
      "Starting build_dataset for column PolarizationD3 \n",
      "Number of experiments to process: 375 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "Number of experiments processed for mode 'PolarizationD3': 375\n",
      "Final dataset shapes: Xs: (2985, 13), Xt: (2985, 1), y: (2985, 1)\n",
      "Saved scalers to: \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_Naif16108_14\n",
      "Begin training and scaling with 375 experiments.\n",
      "Starting build_dataset for column PolarizationD3 \n",
      "Number of experiments to process: 375 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "Number of experiments processed for mode 'PolarizationD3': 375\n",
      "Final dataset shapes: Xs: (2985, 13), Xt: (2985, 1), y: (2985, 1)\n",
      "Scaling all data\n",
      "Training final model with epochs=300, batch_size=32 and use_uncertainty = True\n",
      "Training the model...\n",
      "Saved model to \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_Naif16108_14\\Model_Naif16108_14.keras\n",
      "Execution finished in 2482.339265 seconds (Complexity=Naif16108, Augmentations=14). Logged to AmorphousExecution_times.txt\n",
      "\n",
      " _______________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Complexity in [\"Naif16108\"]: \n",
    "    build_model = Define_Complexity(Complexity)\n",
    "    print(Complexity)\n",
    "    for num_augmentations in [14]:  # idem with augmentations\n",
    "        log_message(f\"Begin {Complexity} model with {num_augmentations} augmentations\")\n",
    "        # Delete folder if exists\n",
    "        # Move all files back\n",
    "        # Path to the log file\n",
    "        log_file = Path(\"AmorphousExecution_times.txt\")\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "       \n",
    "        # Recreate output folder\n",
    "        output_folder = Path(f\"AmorphousModels\").resolve()\n",
    "        output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "        # Run main function\n",
    "        use_uncertainty = True\n",
    "\n",
    "\n",
    "        isolate_experiments(\n",
    "            data_dir=win_long_path(data_dir),\n",
    "            model_type=model_type,\n",
    "            output_folder=win_long_path(output_folder),\n",
    "            use_uncertainty=use_uncertainty,\n",
    "            num_augmentations=num_augmentations,\n",
    "            build_model=build_model\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        # Ensure the parent directory exists\n",
    "        log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Open with Windows long path safety\n",
    "        with open(win_long_path(str(log_file)), \"a\") as f:\n",
    "            f.write(f\"Execution time={elapsed_time:.6f} seconds for Amorphous {Complexity} {num_augmentations}\\n\")\n",
    "        \n",
    "        log_message(\n",
    "            f\"Execution finished in {elapsed_time:.6f} seconds \"\n",
    "            f\"(Complexity={Complexity}, Augmentations={num_augmentations}). \"\n",
    "            f\"Logged to {log_file}\"\n",
    "        )\n",
    "        log_message(f\"\\n _______________________________________________________ \\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8fb68f3-00d4-414a-9a40-44bf3d43dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built model NaifTwiceDense16116\n",
      "NaifTwiceDense16116\n",
      "Begin NaifTwiceDense16116 model with 12 augmentations\n",
      "Created folder: \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_NaifTwiceDense16116_12\n",
      "Finding all Array Files...\n",
      "Create combined DataFrame for static parameters...\n",
      "Collected static data:\n",
      "Static dataframe columns: ['CellID', 'AnalyserID', 'PolariserPressure', 'AnalyserPressure', 'LabPolarization', 'LabTime']\n",
      "Static dataframe shape: (25, 6)\n",
      "Hot encoding CellID PLACEHOLDER, VISIT TO THE LAB MAY BE NEEDED\n",
      "Loaded 25 experiments.\n",
      "Augmenting 25 a number of 12 times\n",
      "Augmented to 325 experiments\n",
      "Scale all experiments\n",
      "Starting build_dataset for column PolarizationD3 \n",
      "Number of experiments to process: 325 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "Number of experiments processed for mode 'PolarizationD3': 325\n",
      "Final dataset shapes: Xs: (2587, 13), Xt: (2587, 1), y: (2587, 1)\n",
      "Saved scalers to: \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_NaifTwiceDense16116_12\n",
      "Begin training and scaling with 325 experiments.\n",
      "Starting build_dataset for column PolarizationD3 \n",
      "Number of experiments to process: 325 (should be (num_augmentations+1)*(number_of_experiments-1)\n",
      "Number of experiments processed for mode 'PolarizationD3': 325\n",
      "Final dataset shapes: Xs: (2587, 13), Xt: (2587, 1), y: (2587, 1)\n",
      "Scaling all data\n",
      "Training final model with epochs=300, batch_size=32 and use_uncertainty = True\n",
      "Training the model...\n",
      "Saved model to \\\\?\\C:\\Users\\gopeb\\Desktop\\PolarizationProject\\CreateModels\\ML\\AmorphousModels\\Model_NaifTwiceDense16116_12\\Model_NaifTwiceDense16116_12.keras\n",
      "Execution finished in 148.036300 seconds (Complexity=NaifTwiceDense16116, Augmentations=12). Logged to AmorphousExecution_times.txt\n",
      "\n",
      " _______________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Complexity in [\"NaifTwiceDense16116\"]: \n",
    "    build_model = Define_Complexity(Complexity)\n",
    "    print(Complexity)\n",
    "    for num_augmentations in [12]:  # idem with augmentations\n",
    "        log_message(f\"Begin {Complexity} model with {num_augmentations} augmentations\")\n",
    "        # Delete folder if exists\n",
    "        # Move all files back\n",
    "        # Path to the log file\n",
    "        log_file = Path(\"AmorphousExecution_times.txt\")\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "       \n",
    "        # Recreate output folder\n",
    "        output_folder = Path(f\"AmorphousModels\").resolve()\n",
    "        output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "        # Run main function\n",
    "        use_uncertainty = True\n",
    "\n",
    "\n",
    "        isolate_experiments(\n",
    "            data_dir=win_long_path(data_dir),\n",
    "            model_type=model_type,\n",
    "            output_folder=win_long_path(output_folder),\n",
    "            use_uncertainty=use_uncertainty,\n",
    "            num_augmentations=num_augmentations,\n",
    "            build_model=build_model\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        # Ensure the parent directory exists\n",
    "        log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Open with Windows long path safety\n",
    "        with open(win_long_path(str(log_file)), \"a\") as f:\n",
    "            f.write(f\"Execution time={elapsed_time:.6f} seconds for Amorphous {Complexity} {num_augmentations}\\n\")\n",
    "        \n",
    "        log_message(\n",
    "            f\"Execution finished in {elapsed_time:.6f} seconds \"\n",
    "            f\"(Complexity={Complexity}, Augmentations={num_augmentations}). \"\n",
    "            f\"Logged to {log_file}\"\n",
    "        )\n",
    "        log_message(f\"\\n _______________________________________________________ \\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf42ad-bb5d-4483-8832-d1ad0d2cbbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504bff4c-af3a-4769-91b9-ea2568a85b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
