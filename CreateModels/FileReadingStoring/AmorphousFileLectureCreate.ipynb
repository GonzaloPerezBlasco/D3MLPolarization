{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ca1d58-6504-4005-b1a4-808475c12f16",
   "metadata": {},
   "source": [
    "<h1>AmorphousFileLectureCreate.ipynb</h1> \n",
    "\n",
    "Reads from D3Files all the files it is going to process\n",
    "\n",
    "Outputs the following folders and files:\n",
    "\n",
    "1. AmorphousLog_Reading_Creation.txt\n",
    "Logs all the prints and every step the code does. If you trust the code, it is irrelevant. If you don't trust it or want to change it then this txt file will tell you how each experiment file has been processed and where there might have been issues.\n",
    "\n",
    "\n",
    "2. Amorphous_CellID\n",
    "Contains the Cell IDs found on all the files. Needed for the ML code.\n",
    "\n",
    "\n",
    "3. AmorphousPlotResults\n",
    "This folder will store the graphs of all the experiments that were accepted. Not needed for anything but it is nice to see the files that will be fed to the model. For each experiment you can find the following files:\n",
    "\n",
    "3.1 {base_name}_ExtendedArea.png \n",
    "It shows the same pot and also the range y_min=mx+n-1.3*N<y<mx+n+1.3*N<y_max. The points outside the green area will be discarded as they are considered to be too off to be considered correct.\n",
    "\n",
    "\n",
    "4. AmorphousMLDataBase\n",
    "Contains the .txt files NECESSARY for the ML algorithm. There are two per experiment\n",
    "\n",
    "4.1 {base_name}.txt \n",
    "Contains DeltaTime (the time of the measurement measured from the first VALID polarization measurement), PolarizationD3, SoftPolarizationD3 (the polarization after using a Savitzky-Golay filter) and ErrPolarizationD3 (the uncertainty)\n",
    "\n",
    "4.2 {base_name}_Parameters.txt\n",
    "Contains the CellID, Pressure, LabPolarization (the polarization measured at the lab) and LabTimeCellID (the time when it was measured)\n",
    "\n",
    "\n",
    "5. AmorphousFailuresTest\n",
    "Contains the plots 3.2 and 3.4 but for the experiments that failed the overall decreasing test. Check them if you can to see if any of your experiments has been placed there by mistake\n",
    "\n",
    "\n",
    "\n",
    "6. AmorphousDataBase\n",
    "Contains all the .fli files that were attempted to be read\n",
    "\n",
    "\n",
    "7. AmorphousBadFiles\n",
    "Contains all the .fli separated in experiment sets folders that were rejected (not enough points, negative polarizations, etc.)\n",
    "\n",
    "\n",
    "_________________________________________________________________________________________\n",
    "\n",
    "Process it follows:\n",
    "\n",
    "1. REMOVAL OF PREVIOUS ITERATIONS\n",
    "To avoid leaks and duplications, all files are erased before running the code file\n",
    "\n",
    "2. ZIP FOLDER TREATMENT\n",
    "The code will take all the zip files, extract them, remove duplicates using the name AND hash sha256.\n",
    "\n",
    "3. SEPARATION OF FLI FILES ACCORDING TO EXPERIMENTS\n",
    "\n",
    "Some fli files have the wrong structure (they are not polarization measurementes) and if they are polarization files they can have more than one experiment per file.\n",
    "For evey fli file we will read the contents and try to find the header (two strings in two consecutive lines). This symbolizes the beginning of an experiment\n",
    "If there are numerical values before the first header, that means that the process of saving the file occured before changing something of the experiment. These data rows will be skipped\n",
    "A correct fli file will have the following structure:\n",
    "polariser cell info ge18004 pressure/init. polar 2.30 0.79 initial date/time 21 11 23 @ 12:45\n",
    "analyser cell info sic1402 pressure/init. polar 2.00 0.79 initial date/time 21 11 23 @ 12:45\n",
    "   40661   3.000   3.000   3.000 21/11/23 12:50:35       0.00        0.7890    0.0020    8.4795    0.0897     120.00\n",
    "   40662   3.000   3.000   3.000 21/11/23 12:54:43       0.00        0.7851    0.0020    8.3048    0.0867     120.00\n",
    "    ...\n",
    "\n",
    "Which corresponds to the following information:\n",
    "    String:'polariser cell info', PolariserID, String:'pressure/init. polar', PolariserPressure(unknown units), InitialLabPolarization, String:'date/time', Day, Month, Year, String:'@', Hour:Minute\n",
    "    String:'analyser cell info', AnalyserID, String:'pressure/init. polar', AnalyserPressure(unknown units), InitialLabPolarization(always the same as the polariser cell), String:'date/time', Day, Month, Year, String:'@', Hour:Minute\n",
    "    Measurement Number, First Miller Index, Second Miller Index, Third Miller Index, Date Of Measurement, Time Of Measurement, Unkown Number, Polarization, Polarization uncertainty, Flipping Ratio, FlippingRatio Uncertainty, Duration of the measurement\n",
    "\n",
    "D3 uses two polariser cells, one between the reactor and the sample and a second between the sample and the sensor. The first one guarantees that only neutrons with the correct spin direction interacts with the sample. The second one guarantees that only the neutrons that have unchanged spin direction after interacting with the sample are detected by the sensor. The same happens with the analyser cells. The reason why this values are smaller than the ones measured with crystals or crystalline powders is that this polarizations are the multiplication of two polarizations. We have considered that temperature is not a relevant factor and the flipping ratio has no new information that polarization alrady posseses.\n",
    "First, the code will first locate the first header (ignoring eveything before) and save all the data afterwards (until the next header or end of the document) in a file with the suffix Array_{i} (i is the number of headers already processed in that fli file).\n",
    "Second it will try to find a combination of the strings with polariser and anlyser (there have been files where one comes before the other, others the other way around and some where the lab polarization.\n",
    "changes in the same cell in mere minutes. As there is not necessarily an absolute order, the code has become flexible to fix these issues)\n",
    "Third, it will save the headers as a file with the suffix Parameters.\n",
    "Fourth, the header row and the columns of Measurement Number, Unkown Number, Flipping Ratio, FlippingRatio Uncertainty and Time Between Measurements will be erased\n",
    "Fifth, not all data from all Miller Index combinations are polarization measurements. Even some of the ones that are polarization measurements are tampered (playing with magnetic fields for example).\n",
    "This means that there needs to be a way to select the correct combination. For starters, irrational Miller indices are not used for measurements with the samples (they need to be discarded)\n",
    "The integer Miller indices combination will be put to the test by all the functions defined before.\n",
    "For evey succesful experiment we will output:\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_ExtendedArea.png\" in AmorphousPlotResults. Shows the plot with the extended area with the raw data\n",
    "    Txt:    \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}.txt\" in AmorphousMLDataBase. It contains the four data columns (DeltaTime, PolarizationD3, SoftPolarizationD3, ErrPolarizationD3)\n",
    "    Txt:    \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\" in AmorphousMLDataBase. It contains the parameters (CellID, Pressure, LabPolarization, LabTime)\n",
    "These plots are not necessary but are saved for the user to know what all the files look like.\n",
    "The files that are wrong or useless when all is done are the folowing:\n",
    "    Txt:    \"{folder_name}_Arrays_{i}.txt\" in SeparatedFolder/{folder_name}. It still has the header and useless columns. It is the fli file of evey chunk, of every recorded experiment (correct or incorrect)\n",
    "    Folder: \"BadTest\" contains all the graphs of the data sets that were considered not worthy but had more points that the ones saved. Check them if your experiment was not properly added.\n",
    "\n",
    "No scores are obtained to say if a file is correct or not\n",
    "Seventh, it will try each Miller index combination for a set i value, apply a filter, and return filtered df + PrettyCombination. it will only add the filtered column if enough points & data changes significantly. If it doesn't change too much, the column PolarizationD3 will be duplicated with the new name\n",
    "Eight, it repeats the process of obtaining the area in m*x+n-N < y < m*x+n+N where 75% of the points are inside the area. It also multiplies the value of N by a factor AcceptableMultiplier and erases all points outside this bigger area. The enlarged area will be plotted and saved in PlotResults only for the normal data (not the softened one as the files were already adequately saved and filtered without the Savitzky–Golay filter). As uncertainties are clearly underestimated I tried to make them reasonable (looking at the dispersion of the points it is clear there is systematic uncertainties. Under the hypothesis that the polarization curve should be a soft curve (at least C^1) we will try to use χ^2 to add a provisional uncertainty margin fitting to a linear expression. This is a very inaccurate uncertainty increase but it is an improvement of the underestimated uncertainties (and the lack of ways to quantify the systematic uncertainty sources).\n",
    "    \n",
    "4. CellID SAVING\n",
    "It will safely store in a txt file all the polariser and analyser cell ids so that the code in ML can use them\n",
    "\n",
    "5. DUPLICATION REMOVAL\n",
    "It will check if the files created for the ML algorithm are duplicates and erases them in that case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd9b46-735b-4506-9f31-e0ed1acdedec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "\"\"\"\n",
    "1- LIBRARIES\n",
    "\"\"\" \n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import curve_fit\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "2- PRINTING AND LOG DETAILS. LONG PATH CORRECTIONS\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Here we have a custom function for printing and logging everything on a .txt file to be able to know what has happened on the code\n",
    "\n",
    "\"\"\"\n",
    "PrintDebug = True #This Bool will determine if all logs should be printed on screen on the Python Notebook. The log writing is always on. If False the code will be faster.\n",
    "ShowPlot = False #This Bool works the same but with showing on screen the plots (they are always saved even with this variable being False). Reduces program cost if False\n",
    "log_file_path = os.path.join(\".\", \"AmorphousLog_Reading_Creation.txt\")\n",
    "# Initialize log file at the start of the script\n",
    "with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(\"=== Log started ===\\n\")\n",
    "\n",
    "def log_message(message):\n",
    "    if PrintDebug:\n",
    "        print(message)\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "        log_file.write(message + \"\\n\")\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def win_long_path(path):\n",
    "    # Convert to Path and resolve to absolute\n",
    "    path = Path(path).resolve()\n",
    "\n",
    "    # Convert to string\n",
    "    path_str = str(path)\n",
    "\n",
    "    # Prepend \\\\?\\ if not already present\n",
    "    if not path_str.startswith(\"\\\\\\\\?\\\\\"):\n",
    "        path_str = \"\\\\\\\\?\\\\\" + path_str\n",
    "\n",
    "    return path_str\n",
    "\n",
    "to_erase = [\"AmorphousPolariserAndAnalyser_IDs.txt\",\n",
    "            \"AmorphousLog_Reading_Creation.txt\",\n",
    "            \"AmorphousBadFiles\",\n",
    "            \"AmorphousFailuresTest\",\n",
    "            \"AmorphousMLDataBase\",\n",
    "            \"AmorphousPlotResults\"          \n",
    "           ]\n",
    "for item in to_erase:\n",
    "    path = os.path.abspath(item)  # full path\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                log_message(f\"Deleted file: {path}\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                log_message(f\"Deleted folder: {path}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\" Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"Not found (skipped): {path}\")\n",
    "\n",
    "\"\"\"\n",
    "3- FUNCTIONS\n",
    "\"\"\"\n",
    "#3.1-Time function for the conversion of time into seconds\n",
    "def Time(Day_Ref, Hour_Ref):\n",
    "    \"\"\"\n",
    "    Expected variables (strings) should be in the form\n",
    "     Day_Ref = 'DD/MM/YY' a.k.a Day/Month/Year\n",
    "     Hour_Ref = 'HH:MM' a.k.a Hour:Month\n",
    "    Seconds will be ignored if fed to the function (there is a check before the call of the function in the main code that erases the seconds).\n",
    "    The function takes this strings and converts them to seconds\n",
    "    Could be an improvement to consider seconds but the headers don't use seconds, there is no information to prove that the time variables are precise to the second and ~30 seconds is negligible when working with time periods of up to 70000 seconds\n",
    "    \"\"\"\n",
    "    match = re.match(r\"(\\d+)/(\\d+)/(\\d+)\", Day_Ref)\n",
    "    if match:\n",
    "        DD = int(match.group(1))\n",
    "        MM = int(match.group(2))\n",
    "        YY = int(match.group(3))\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid date format: {Day_Ref}\")\n",
    "\n",
    "    # Parse time\n",
    "    match = re.match(r\"(\\d+):(\\d+):(\\d+)\", Hour_Ref)\n",
    "    if match:\n",
    "        Hour = int(match.group(1))\n",
    "        Minute = int(match.group(2))\n",
    "        Second = int(match.group(3))\n",
    "    else:\n",
    "        # Try HH:MM\n",
    "        match = re.match(r\"(\\d+):(\\d+)\", Hour_Ref)\n",
    "        if match:\n",
    "            Hour = int(match.group(1))\n",
    "            Minute = int(match.group(2))\n",
    "            Second = 0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time format: {Hour_Ref}\")\n",
    "\n",
    "    return datetime(YY + 2000 if YY < 100 else YY, MM, DD, Hour, Minute, Second)\n",
    "\n",
    "#3.2- Time function to compute the difference in time between two sets fo time\n",
    "def deltatime(AIni,BIni, AFin,BFin):\n",
    "    \"\"\"\n",
    "    There is nothing as \"Absolute time\". We want to compare durations or intervals of time.\n",
    "    The A variables are of the type 'DD/MM/YY' and the B ones are 'HH:MM'\n",
    "     A is for the time moment considered as reference\n",
    "     B is for the time moment that has been used for measuring something\n",
    "    \"\"\"\n",
    "    time1 = Time(AIni, BIni)\n",
    "    time2 = Time(AFin, BFin)\n",
    "    return( int((time2 - time1).total_seconds()))\n",
    "\n",
    "#3.3 Conversion to integers\n",
    "def format_combination(comb):\n",
    "    \"\"\"\n",
    "    The Miller Indeces in the are writen as strings with float like numbers in them, for example (4.0,1.0,2.0)\n",
    "    As only integer Miller Indices are required we can convert them to integers with this function.\n",
    "    \"\"\"\n",
    "    if comb is None:\n",
    "        return \"(None)\"\n",
    "    ints = tuple(int(float(x)) for x in comb)\n",
    "    return f\"({','.join(map(str, ints))})\"\n",
    "\n",
    "#3.4 Folder name changes\n",
    "def sanitize(name):\n",
    "    \"\"\"\n",
    "    Remove invalid characters for folder names. [<>:\"/\\\\|?*] all turn to _\n",
    "    \"\"\"\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', name)\n",
    "\n",
    "#3.5 Savitzky–Golay filter window parameters\n",
    "def savgol_params_func(n_points):\n",
    "    \"\"\"\n",
    "    Receives tha number of points of the file that the filter will be tried to be used on\n",
    "    Window length can't be greater than the number of points. To avoid Python Errors this function gives the appropiate window length and order of the polygone\n",
    "    \"\"\"\n",
    "    window_length = min(default_window_length, n_points)\n",
    "    if window_length % 2 == 0:\n",
    "        window_length -= 1\n",
    "    if window_length < polyorder + 2:\n",
    "        window_length = polyorder + 2\n",
    "        if window_length % 2 == 0:\n",
    "            window_length += 1\n",
    "    return {'window_length': window_length, 'polyorder': polyorder}\n",
    "\n",
    "#3.6 Filtering Methods (Unused for AMorphous substances)\n",
    "from pathlib import Path\n",
    "\n",
    "FilteringMethodInt = 12 \n",
    "\"\"\"\n",
    "This int chooses the method for filtering the wrong data. The options are:\n",
    "    FilteringMethodInt = 0   \n",
    "    FilteringMethodInt = 1\n",
    "    FilteringMethodInt = 2\n",
    "    FilteringMethodInt = 12\n",
    "    FilteringMethodInt = 3\n",
    "\"\"\"\n",
    "\n",
    "force_accept_files = [\n",
    "    \"PolarizationD3_EuAgAs_29_8_23_0_MillerIndex_(0,0,4)_Filtered.txt\",\n",
    "    \"PolarizationD3_EuAgAs_30_8_23_5_MillerIndex_(3,0,0)_Filtered.txt\",\n",
    "    \"PolarizationD3_SmI3_1_26_9_23_1_MillerIndex_(0,3,0)_Filtered.txt\",\n",
    "    \"PolarizationD3_SmI3_1_28_9_23_3_MillerIndex_(0,3,0)_Filtered.txt\",\n",
    "    \"PolarizationD3_MnSn-c_hor_1_22_3_24_1_MillerIndex_(3,0,0)_Filtered.txt\",\n",
    "    \"PolarizationD3_MnSn-c_hor_15_3_24_0_MillerIndex_(0,0,2)_Filtered.txt\",\n",
    "]\n",
    "force_reject_files = [\n",
    "    \"PolarizationD3_EuAgAs_1_29_8_23_0_MillerIndex_(1,1,1)_Filtered.txt\",\n",
    "    \"PolarizationD3_MnSn-c_ver_14_3_24_1_MillerIndex_(1,0,0)_Filtered.txt\"\n",
    "]  # The values were in the 0.3 to 0.2 range. Not a polarization\n",
    "\n",
    "\"\"\"\n",
    "Some files failed the FilteringMethodInt = 12 method despite being considered good files. Others passed the test but were clearly tampered.\n",
    "These lists override the testing and accepts/rejects them immediately\n",
    "\"\"\"\n",
    "\n",
    "def win_long_path(path):\n",
    "    \"\"\"Ensure Windows long path handling.\"\"\"\n",
    "    path = Path(path).resolve()\n",
    "    path_str = str(path)\n",
    "    if not path_str.startswith(\"\\\\\\\\?\\\\\"):\n",
    "        path_str = \"\\\\\\\\?\\\\\" + path_str\n",
    "    return path_str\n",
    "\n",
    "\n",
    "def Overall_Decrease(df_filtered, filename):\n",
    "    \"\"\"\n",
    "    The function that accepts files if they are decreasing sets of data (polarization evolution is a decay ALWAYS)\n",
    "    Will save plots of good files in AmorphousPlotResults so that the user can see what has been accepted.\n",
    "    Will save plots of bad files in AmorphousFailuresTest so that the user can see the plots being discarded.\n",
    "    \"\"\"\n",
    "    output_folder = Path.cwd() / \"AmorphousPlotResults\"\n",
    "    failures_folder = Path.cwd() / \"AmorphousFailuresTest\"\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    failures_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    x = df_filtered[\"DeltaTime\"].values\n",
    "    y = df_filtered[\"SoftPolarizationD3\"].values\n",
    "\n",
    "    def linear_func(x, m, n):\n",
    "        return m * x + n\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(linear_func, x, y)\n",
    "        m, n = popt\n",
    "        log_message(f\"      Linear fit slope m={m:.4e}, intercept n={n:.4f}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"      Error fitting data: {e}\")\n",
    "        return False\n",
    "\n",
    "    if m > 0:\n",
    "        log_message(\n",
    "            f\"      Overall slope is positive. Can't be polarization information. Skipping Combination\"\n",
    "        )\n",
    "        return False\n",
    "\n",
    "    return True  # Accept all Amorphous ones\n",
    "\n",
    "\n",
    "# 3.7 Function that decides the correct set of Miller Indices\n",
    "def filter_best_combination(\n",
    "    i,\n",
    "    df,\n",
    "    filter_func,\n",
    "    filter_column_idx,\n",
    "    new_column_name,\n",
    "    filter_params_func,\n",
    "    min_points_required=3,\n",
    "    tolerance=1e-8,\n",
    "    time_column_idx=None,\n",
    "    error_column_idx=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Try each Miller index combination for a set i value, apply a filter, and return filtered df + PrettyCombination.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "\n",
    "    folder_name = FileName.replace(\".fli\", \"\")\n",
    "    # TODO: wherever this function saves/loads, wrap with win_long_path()\n",
    "\n",
    "    # Group by first three columns (Miller indices)\n",
    "    combination_counts = (\n",
    "        df.groupby([df.columns[0], df.columns[1], df.columns[2]])\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    log_message(f\"Analyzing combinations in file: {folder_name}_Array_{i}.fli\")\n",
    "    #Read the three numbers from the .fli file\n",
    "    for comb, count in combination_counts.items(): \n",
    "        log_message(f\"Combination {comb} occurs {count} times in file {folder_name}.fli. Trying this combination\")\n",
    "        mask = (\n",
    "            (df.iloc[:,0] == comb[0]) &\n",
    "            (df.iloc[:,1] == comb[1]) &\n",
    "            (df.iloc[:,2] == comb[2])\n",
    "        )\n",
    "        PrettyCombination = format_combination(comb)\n",
    "        \n",
    "\n",
    "        filtered_df = df.loc[mask].copy()\n",
    "        # Requisites for the Combination to be valid:\n",
    "        # Requisite 1: Have data in the data\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      {PrettyCombination} has no data\")\n",
    "            continue\n",
    "        print(filtered_df)\n",
    "        filtered_df = filtered_df.drop(filtered_df.columns[5], axis=1)\n",
    "        filtered_df = filtered_df.drop(filtered_df.columns[5], axis=1)\n",
    "        filtered_df = filtered_df.drop(filtered_df.columns[5], axis=1)\n",
    "        # Requisite 2: Check if data column exists\n",
    "        if filtered_df.shape[1] <= filter_column_idx:\n",
    "            log_message(f\"      Expected column index {filter_column_idx} not found. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert to numeric all columns (all columns are considered as object type)\n",
    "        filtered_df = filtered_df.apply(pd.to_numeric, errors='coerce')\n",
    "        filtered_df = filtered_df.dropna()  # drops any rows with NaNs introduced by coercion\n",
    "        print(filtered_df)\n",
    "        # Check dtypes\n",
    "        all_numeric = all(dtype.kind in ('f', 'i') for dtype in filtered_df.dtypes)\n",
    "        \n",
    "        if all_numeric:\n",
    "            log_message(f\"      All columns have been successfully converted to numbers.\")\n",
    "        else:\n",
    "            log_message(f\"      Not all columns are numbers. Current dtypes:\")\n",
    "            log_message(f\"      {filtered_df.dtypes}\")\n",
    "            log_message(f\"      Expect Error Message from Python. Perhaps removing this file might be wise unless all files have the same issue\")\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      All rows dropped after conversion to numeric. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Requisite 3: Polarization is ALWAYS positive. If any is negative, that is not a polarization. Immediately sent to the Bad Files Folder\n",
    "        if (filtered_df.iloc[:, filter_column_idx] < 0).any():\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "            badfile_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "            filtered_df.to_csv(badfiles_txt_path, index=False, sep='\\t')\n",
    "            log_message(f\"      {PrettyCombination} has negative polarization values. Sent to BadFiles with name {filename}. Skipping to next Combination\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Requisite 4: Have at least three rows (otherwise we can't teach the ML algorithm nothing).\n",
    "        if len(filtered_df) < min_points_required:\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "            badfile_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "\n",
    "            #  Ensure long path compatibility\n",
    "            filtered_df.to_csv(win_long_path(badfiles_txt_path), index=False, sep=\"\\t\")\n",
    "\n",
    "            log_message(\n",
    "                f\"      {PrettyCombination} has only {len(filtered_df)} rows (< {min_points_required}). \"\n",
    "                f\"Sent to BadFiles with name {filename}. Skipping to next Combination\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Requisite 5: Be worthy of having the filter used.\n",
    "        y = filtered_df.iloc[:, filter_column_idx].values\n",
    "        filter_params = filter_params_func(len(y))\n",
    "        try:\n",
    "            y_filtered = filter_func(y, **filter_params)\n",
    "            diff = np.abs(y - y_filtered)\n",
    "            changed_count = np.sum(diff > tolerance)\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "            if changed_count > 0:\n",
    "                log_message(f\"      Filter changed {changed_count}/{len(y)} points. Adding column '{new_column_name}'.\")\n",
    "            else:\n",
    "                log_message(f\"      Filter applied but data unchanged. Adding '{new_column_name}' as duplicated values.\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error applying filter to combination {comb}: {e}\")\n",
    "            log_message(f\"      Adding '{new_column_name}' as duplicated values to proceed anyway.\")\n",
    "            # Just duplicate the original column\n",
    "            y_filtered = y.copy()\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "        #print(filtered_df)\n",
    "        # Requisite 5: Add the Derivative and BandWidth filtering logic\n",
    "        if Overall_Decrease(filtered_df, f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"):\n",
    "            log_message(f\"      {PrettyCombination} has surpassed all tests. Proceding with it.\")\n",
    "            return filtered_df, PrettyCombination\n",
    "        else:\n",
    "            log_message(f\"      {PrettyCombination} failed the Overall_Decrease test. Trying next combination.\")\n",
    "            continue\n",
    "    return None, None\n",
    "\n",
    "#3.8 Function that removes the points considered bad and fixes the underestimated Uncertainty  \n",
    "def RemoveOutcast_FixUncertainty(df_filtered, PrettyCombination, filename, AcceptableMultiplier=2.0, ShowPlot=False):\n",
    "    \"\"\"\n",
    "    Repeats the process of obtaining the area in m*x+n-N < y < m*x+n+N where 75% of the points are inside the area.\n",
    "    Multiplies the value of N by a factor AcceptableMultiplier and erases all points outisde this bigger area.\n",
    "    As uncertainties are clearly underestimated I tried to make them reasonable (looking at the dispersion of the points it is clear there is systematic uncertainties\n",
    "    Under the hypothesis that the polarization curve should be a soft curve (at least C^1) we will try to use χ^2 to add a provisional uncertainty margin fitting to a linear expression.\n",
    "    This is a very inaccurate uncertainty increase but it is an improvement of the underestimated uncertainties (and the lack of ways to quantify the systematic uncertainty sources)\n",
    "    The enlarged area will be plotted and saved in PlotResults for both the normal data and the softened data (Savitzky–Golay filter)\n",
    "    \"\"\"    \n",
    "    output_folder = Path.cwd() / \"AmorphousPlotResults\"\n",
    "    output_folder.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "\n",
    "    x = df_filtered['DeltaTime'].values\n",
    "    y_hard = df_filtered['PolarizationD3'].values\n",
    "    y_soft = df_filtered['SoftPolarizationD3'].values\n",
    "\n",
    "    #Linear fit\n",
    "    def linear_func(x, m, n):\n",
    "        return m * x + n\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(linear_func, x, y_hard)\n",
    "        m, n = popt\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error fitting data: {e}\")\n",
    "        return df_filtered  # Return original if error\n",
    "\n",
    "    #Find smallest N for 75% within band\n",
    "    num_points = len(x)\n",
    "    sorted_idx = np.argsort(x)\n",
    "    x_sorted = x[sorted_idx]\n",
    "    y_sorted = y_hard[sorted_idx]\n",
    "\n",
    "    N_start = 0.0001\n",
    "    N_step = 0.0001\n",
    "    N_max = 0.4\n",
    "    N = N_start\n",
    "    needed_N = None\n",
    "\n",
    "    while N <= N_max:\n",
    "        y_fit = linear_func(x_sorted, m, n)\n",
    "        upper = y_fit + N\n",
    "        lower = y_fit - N\n",
    "        inside = np.logical_and(y_sorted <= upper, y_sorted >= lower)\n",
    "        percent_inside = np.sum(inside) / num_points * 100\n",
    "\n",
    "        if percent_inside >= 75:\n",
    "            needed_N = N\n",
    "            break\n",
    "        N += N_step\n",
    "\n",
    "    if needed_N is None:\n",
    "        log_message(f\"[{filename}] No N found to contain 75% within ±{N_max}\")\n",
    "        return df_filtered  # Return original if no good N\n",
    "\n",
    "    # Compute extended band and filter out the points outside the extended band\n",
    "    y_fit_full = linear_func(x, m, n)\n",
    "    upper_band = y_fit_full + needed_N * AcceptableMultiplier\n",
    "    lower_band = y_fit_full - needed_N * AcceptableMultiplier\n",
    "\n",
    "    mask = np.logical_and(y_hard <= upper_band, y_hard >= lower_band)\n",
    "    df_cleaned = df_filtered[mask].copy()\n",
    "\n",
    "    log_message(f\"[{filename}] Filtering kept {np.sum(mask)} of {len(mask)} rows (±{needed_N * AcceptableMultiplier:.2e})\")\n",
    "\n",
    "    \n",
    "    # Rescale uncertainties using reduced chi-squared \n",
    "\n",
    "    sigma = df_filtered['ErrPolarizationD3'].values\n",
    "    \n",
    "    # Fit using curve_fit with uncertainties\n",
    "    popt, pcov = curve_fit(linear_func, x, y_hard, sigma=sigma, absolute_sigma=True)\n",
    "    \n",
    "    # Extract best-fit parameters and their uncertainties\n",
    "    m_fit, n_fit = popt\n",
    "    m_err, n_err = np.sqrt(np.diag(pcov))\n",
    "    \n",
    "    # Recalculate the reduced chi-squared https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Weighted_sample_variance. Need to find the real source\n",
    "    residuals = (y_hard - linear_func(x, *popt)) / sigma\n",
    "    dof = len(x) - len(popt)\n",
    "    chi_squared_red = np.sum(residuals**2) / dof\n",
    "    correction_factor = np.sqrt(chi_squared_red)\n",
    "    \n",
    "    # Automatically apply correction if needed\n",
    "    if correction_factor > 1:\n",
    "        df_cleaned['ErrPolarizationD3'] *= correction_factor\n",
    "        log_message(f\"[{filename}] Applied uncertainty correction factor: √(χ²) = {correction_factor:.2f}\")\n",
    "    else:\n",
    "        log_message(f\"[{filename}] No correction applied: √(χ²) = {correction_factor:.2f}\")\n",
    "    \n",
    "    # Optional: log the fit results\n",
    "    log_message(f\"[{filename}] Fit results: m = {m_fit:.4f} ± {m_err:.4f}, n = {n_fit:.4f} ± {n_err:.4f}\")\n",
    "\n",
    "\n",
    "    def make_clean_name(filename: str) -> str:\n",
    "        \"\"\"\n",
    "        Turn e.g.\n",
    "          PolarizationD3_CaFeAl_13_7_6_24_2_MillerIndex_(0,0,2)_Filtered.txt\n",
    "        into:\n",
    "          CaFeAl_13_7_6_24_2_(0,0,2)\n",
    "        and handle cases where filename contains '/' or '\\\\' (dates like DD/MM/YY).\n",
    "        \"\"\"\n",
    "        s = str(filename).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")  # prevent path splitting\n",
    "        base = Path(s).stem  # remove extension if present\n",
    "        if base.startswith(\"PolarizationD3_\"):\n",
    "            base = base[len(\"PolarizationD3_\"):]\n",
    "        if base.endswith(\"_Filtered\"):\n",
    "            base = base[:-len(\"_Filtered\")]\n",
    "        base = base.replace(\"MillerIndex_\", \"\")\n",
    "        return base\n",
    "    \n",
    "    def extended_area_plot_filename(filename: str) -> str:\n",
    "        \"\"\"EuAgAs_5_31_10_23_0_(3,0,0)_ExtendedArea.png\"\"\"\n",
    "        return f\"{make_clean_name(filename)}_ExtendedArea.png\"\n",
    "    # Extract values\n",
    "    # Data\n",
    "    T = df_filtered['DeltaTime'].values\n",
    "    P_soft = df_filtered['SoftPolarizationD3'].values\n",
    "    P_hard = df_filtered['PolarizationD3'].values\n",
    "    Err = df_filtered['ErrPolarizationD3'].values if 'ErrPolarizationD3' in df_filtered.columns else np.zeros_like(P_soft)\n",
    "    \n",
    "    # Clean title + filename\n",
    "    clean = make_clean_name(filename)\n",
    "    save_name = extended_area_plot_filename(filename)  # ends with _ExtendedArea.png\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Black points with error bars\n",
    "    plt.scatter(T, P_hard, s=30, color=\"black\", label=\"PolarizationD3\", marker='o')\n",
    "    if Err is not None:\n",
    "        plt.errorbar(T, P_hard, yerr=Err, fmt='none', ecolor='black', alpha=0.6, capsize=2)\n",
    "    \n",
    "    # Blue linear fit\n",
    "    fit = linear_func(T, m, n)\n",
    "    plt.plot(T, fit, '-', color='blue', label=\"Linear Fit\")\n",
    "    \n",
    "    # Bands: light blue (±N) and translucent green (±N*AcceptableMultiplier)\n",
    "    if needed_N is not None:\n",
    "        upper_narrow = fit + needed_N\n",
    "        lower_narrow = fit - needed_N\n",
    "        upper_wide   = fit + needed_N * AcceptableMultiplier\n",
    "        lower_wide   = fit - needed_N * AcceptableMultiplier\n",
    "    \n",
    "        plt.fill_between(T, lower_narrow, upper_narrow, color='lightblue', alpha=0.35, label=f'Band ±{needed_N:.2e}')\n",
    "        plt.fill_between(T, lower_wide,   upper_wide,   color='green',     alpha=0.18, label=f'Filter Band ±{(needed_N*AcceptableMultiplier):.2e}')\n",
    "    \n",
    "    # Labels\n",
    "    plt.xlabel(\"DeltaTime\")\n",
    "    plt.ylabel(\"PolarizationD3\")\n",
    "    plt.title(f\"{clean}_ExtendedArea\")  # title matches saved name (sans .png)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # --- Y limits that include error bars and bands ---\n",
    "    vals_min = [np.nanmin(P_hard - Err), np.nanmin(P_hard + Err)]\n",
    "    vals_max = [np.nanmax(P_hard - Err), np.nanmax(P_hard + Err)]\n",
    "    if needed_N is not None:\n",
    "        vals_min += [np.nanmin(lower_narrow), np.nanmin(lower_wide)]\n",
    "        vals_max += [np.nanmax(upper_narrow), np.nanmax(upper_wide)]\n",
    "    # Small margins\n",
    "    ymin = np.nanmin(vals_min)\n",
    "    ymax = np.nanmax(vals_max)\n",
    "    pad = 0.02 * (ymax - ymin if np.isfinite(ymax - ymin) and (ymax - ymin) > 0 else 1.0)\n",
    "    plt.ylim(ymin - pad, ymax + pad)\n",
    "    \n",
    "    # Save\n",
    "    plot_path_hard = output_folder / save_name\n",
    "    plot_path_hard.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(win_long_path(plot_path_hard), dpi=300, bbox_inches='tight')\n",
    "    #if ShowPlot:\n",
    "    #    plt.show()\n",
    "    plt.close()\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. ZIP FOLDER TREATMENT\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "To make it easier for the user to use, you can just download the \"processed\" files in the ILL data base using the zip file. To avoid duplications\n",
    "and other issues, the files won't be deleted. This reduces speed as it needs to reprocess all the files but we make sure that it works\n",
    "This code and the next will take all the zip files, extract them, remove duplicates using the name AND hash sha256.\n",
    "\"\"\"\n",
    "to_erase = [\n",
    "    \"AmorphousLog_Reading_Creation.txt\",\n",
    "    \"AmorphousPolariserAndAnalyser_IDs.txt\",\n",
    "    \"AmorphousSeparatedFolder\",\n",
    "    \"AmorphousPlotResults\",\n",
    "    \"AmorphousMLDataBase\",\n",
    "    \"AmorphousFailuresTest\",\n",
    "    \"AmorphousDataBase\",\n",
    "    \"AmorphousBadFiles\"\n",
    "]\n",
    "for item in to_erase:\n",
    "    path = os.path.abspath(item)  # full path\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                print(f\"Deleted file: {path}\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                print(f\"Deleted folder: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Not found (skipped): {path}\")\n",
    "        \n",
    "def file_hash(filepath, algo=\"sha256\", block_size=65536):\n",
    "    \"\"\"Compute hash of a file (default SHA256).\"\"\"\n",
    "    h = hashlib.new(algo)\n",
    "    with open(win_long_path(filepath), \"rb\") as f:  #  long path safe\n",
    "        for block in iter(lambda: f.read(block_size), b\"\"):\n",
    "            h.update(block)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "folder = win_long_path(\"D3Files\")  # Folder where all the raw data folders reside\n",
    "zip_files = [f for f in os.listdir(folder) if f.lower().endswith(\".zip\")]\n",
    "\n",
    "log_message(f\"Reading ZIP files. Checking for true duplicates by content...\")\n",
    "base_names = set()\n",
    "seen_hashes = {}\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    zip_path = os.path.join(folder, zip_file)\n",
    "    name, ext = os.path.splitext(zip_file)\n",
    "\n",
    "    # Compute hash of this file\n",
    "    filehash = file_hash(zip_path)\n",
    "\n",
    "    if filehash in seen_hashes:\n",
    "        log_message(f\"Duplicate confirmed by hash! Removing: {zip_file} (same as {seen_hashes[filehash]})\")\n",
    "        os.remove(win_long_path(zip_path))  \n",
    "    else:\n",
    "        seen_hashes[filehash] = zip_file\n",
    "        base_names.add(name)\n",
    "\n",
    "log_message(f\"\\n All duplicates (by content) removed. Begin unzipping...\\n\")\n",
    "\n",
    "# Refresh zip_files list after removals\n",
    "zip_files = [f for f in os.listdir(folder) if f.lower().endswith(\".zip\")]\n",
    "\n",
    "# Unzip and remove original zip files\n",
    "for zip_file in zip_files:\n",
    "    zip_path = os.path.join(folder, zip_file)\n",
    "    if zipfile.is_zipfile(win_long_path(zip_path)):  \n",
    "        folder_name = sanitize(os.path.splitext(zip_file)[0])\n",
    "        extract_dir = os.path.join(folder, folder_name)\n",
    "        log_message(f\"Unzipping: {zip_file} -> {extract_dir}\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(win_long_path(zip_path), 'r') as zip_ref:  \n",
    "                zip_ref.extractall(win_long_path(extract_dir))  \n",
    "        except Exception as e:\n",
    "            log_message(f\"WARNING: Error extracting {zip_file}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"WARNING: Skipping invalid zip file: {zip_file}\")\n",
    "\n",
    "log_message(f\"\\n Finished Unzipping. Experiments stored in individual folders substituting the zip files\\n\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "5.EXTRACTION OF THE FLI FILES THAT (MIGHT) CONTAIN INOFRMATION ABOUT POLARIZATION\n",
    "\"\"\"\n",
    "source_folder = win_long_path(\"D3Files\")\n",
    "database_folder = Path(\"AmorphousDataBase\").resolve()\n",
    "database_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "log_message(f\"\\n\\n\\n Scanning all folders for .fli files...\\n \")\n",
    "for item in os.listdir(source_folder):\n",
    "    item_path = os.path.join(source_folder, item)\n",
    "\n",
    "    if os.path.isdir(item_path):\n",
    "        log_message(f\"Processing folder: {item}\")\n",
    "        \n",
    "        for root, dirs, files in os.walk(item_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(\".fli\"):\n",
    "                    src_file = os.path.join(root, file)\n",
    "                    dest_file = os.path.join(database_folder, file)\n",
    "\n",
    "                    # Handle duplicate names\n",
    "                    counter = 1\n",
    "                    base_name, ext = os.path.splitext(file)\n",
    "                    while os.path.exists(win_long_path(dest_file)):\n",
    "                        dest_file = os.path.join(database_folder, f\"{base_name}_{counter}{ext}\")\n",
    "                        counter += 1\n",
    "                    \n",
    "                    log_message(f\"Copying: {src_file} -> {dest_file}\")\n",
    "                    shutil.copy2(win_long_path(src_file), win_long_path(dest_file))  \n",
    "        \n",
    "        # Delete original folder\n",
    "        log_message(f\"Deleting folder: {item_path}\")\n",
    "        shutil.rmtree(win_long_path(item_path))  # long path safe\n",
    "\n",
    "log_message(f\"\\n All .fli files collected, sent from folder {source_folder} to folder {database_folder} and unzipped folders removed. \\n\")\n",
    "\n",
    "\"\"\"\n",
    "6 SEPARATION OF FLI FILES ACCORDING TO EXPERIMENTS\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Some fli files have the wrong structure (they are not polarization measurementes) and if they are polarization files they can have more than one experiment per file.\n",
    "For evey fli file we will read the contents and try to find the header (two strings in two consecutive lines). This symbolizes the beginning of an experiment\n",
    "If there are numerical values before the first header, that means that the process of saving the file occured before changing something of the experiment. These data rows will be skipped\n",
    "A correct fli file will have the following structure:\n",
    "polariser cell info ge18004 pressure/init. polar 2.30 0.79 initial date/time 21 11 23 @ 12:45\n",
    "analyser cell info sic1402 pressure/init. polar 2.00 0.79 initial date/time 21 11 23 @ 12:45\n",
    "   40661   3.000   3.000   3.000 21/11/23 12:50:35       0.00        0.7890    0.0020    8.4795    0.0897     120.00\n",
    "   40662   3.000   3.000   3.000 21/11/23 12:54:43       0.00        0.7851    0.0020    8.3048    0.0867     120.00\n",
    "    ...\n",
    "\n",
    "Which corresponds to the following information:\n",
    "    String:'polariser cell info', PolariserID, String:'pressure/init. polar', PolariserPressure(unknown units), InitialLabPolarization, String:'date/time', Day, Month, Year, String:'@', Hour:Minute\n",
    "    String:'analyser cell info', AnalyserID, String:'pressure/init. polar', AnalyserPressure(unknown units), InitialLabPolarization(always the same as the polariser cell), String:'date/time', Day, Month, Year, String:'@', Hour:Minute\n",
    "    Measurement Number, First Miller Index, Second Miller Index, Third Miller Index, Date Of Measurement, Time Of Measurement, Unkown Number,\n",
    "                        Polarization, Polarization uncertainty, Flipping Ratio, FlippingRatio Uncertainty, Duration of the measurement\n",
    "\n",
    "D3 uses two polariser cells, one between the reactor and the sample and a second between the sample and the sensor. The first one guarantees that only neutrons with the correct spin direction\n",
    "interacts with the sample. The second one guarantees that only the neutrons that have unchanged spin direction after interacting with the sample are detected by the sensor. The same happens\n",
    "with the analyser cells. The reason why this values are smaller than the ones measured with crystals or crystalline powders is that this polarizations are the multiplication of two polarizations \n",
    "We have considered that temperature is not a relevant factor and the flipping ratio has no new information that polarization alrady posseses.\n",
    "First, the code will first locate the first header (ignoring eveything before) and save all the data afterwards (until the next header or end of the document) in a file with the suffix Array_{i} (i is the number of headers already processed in that fli file)\n",
    "Second it will try to find a combination of the strings with polariser and anlyser (there have been files where one comes before the other, others the other way around and some where the lab polarization\n",
    "changes in the same cell in mere minutes. As there is not necessarily an absolute order, the code has become flexible to fix these issues)\n",
    "Third, it will save the headers as a file with the suffix Parameters.\n",
    "Fourth, the header row and the columns of Measurement Number, Unkown Number, Flipping Ratio, FlippingRatio Uncertainty and Time Between Measurements will be erased\n",
    "Fifth, not all data from all Miller Index combinations are polarization measurements. Even some of the ones that are polarization measurements are tampered (playing with magnetic fields for example).\n",
    "This means that there needs to be a way to select the correct combination. For starters, irrational Miller indices are not used for measurements with the samples (they need to be discarded)\n",
    "The integer Miller indices combination will be put to the test by all the functions defined before.\n",
    "For evey succesful experiment we will output:\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_ExtendedArea.png\" in AmorphousPlotResults. Shows the plot with the extended area with the raw data\n",
    "    Txt:    \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}.txt\" in AmorphousMLDataBase. It contains the four data columns (DeltaTime, PolarizationD3, SoftPolarizationD3, ErrPolarizationD3)\n",
    "    Txt:    \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\" in AmorphousMLDataBase. It contains the parameters (CellID, Pressure, LabPolarization, LabTime)\n",
    "These plots are not necessary but are saved for the user to know what all the files look like.\n",
    "The files that are wrong or useless when all is done are the folowing:\n",
    "    Txt:    \"{folder_name}_Arrays_{i}.txt\" in SeparatedFolder/{folder_name}. It still has the header and useless columns. It is the fli file of evey chunk, of every recorded experiment (correct or incorrect)\n",
    "    Folder: \"BadTest\" contains all the graphs of the data sets that were considered not worthy but had more points that the ones saved. Check them if your experiment was not properly added\n",
    "\"\"\"\n",
    "# Path to the original folder and the final folder\n",
    "DataBase = Path('AmorphousDataBase')\n",
    "output_base = Path('AmorphousSeparatedFolder')\n",
    "\n",
    "# List all .fli files in that folder, prepare folders\n",
    "FileNameList = [f.name for f in DataBase.glob('*.fli')]\n",
    "polyorder = 2\n",
    "default_window_length = 5\n",
    "SeparatedFolder = Path(\"AmorphousSeparatedFolder\")\n",
    "BadFilesFolder = Path(\"AmorphousBadFiles\")\n",
    "MLDataBaseFolder = Path(\"AmorphousMLDataBase\")\n",
    "BadFilesFolder.mkdir(exist_ok=True)\n",
    "MLDataBaseFolder.mkdir(exist_ok=True)\n",
    "log_message(f\"\\n\\n Files in the data base that will be (tried) to be used\\n {FileNameList}\\n\")\n",
    "\n",
    "for FileName in FileNameList:\n",
    "    \"\"\"READ THE FILE AND SEPARATE IT INTO EACH EXPERIMENT USING THE POLARIZATION CELL\"\"\"\n",
    "    # 1- Open file\n",
    "    folder_name = FileName.replace(\".fli\", \"\")\n",
    "    output_folder = output_base / folder_name\n",
    "    file_path = DataBase / FileName\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    with open(win_long_path(file_path), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 2- Locate the header with CellID, Pressure, etc. Chunks are the data rows between 'polariser cell info'\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    started = False\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"polariser cell info\"):\n",
    "            if started and current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = [line]\n",
    "            started = True\n",
    "        else:\n",
    "            if started:\n",
    "                current_chunk.append(line)\n",
    "    if not started:\n",
    "        log_message(f\" File '{FileName}' does NOT contain any 'polariser cell info' header. Skipping.\\n\")\n",
    "        continue\n",
    "    else:\n",
    "        log_message(f\" File '{FileName}' contains at least one 'polariser cell info' header.\")\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    # 3- Save .fli files for each chunk\n",
    "    base_name = FileName.replace(\".fli\", \"\")\n",
    "    log_message(f\"Creating all the Array files \\n\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        fli_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "        fli_path = output_folder / fli_filename\n",
    "        with open(win_long_path(fli_path), \"w\") as f_out:  # \n",
    "            f_out.writelines(chunk)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    started = False  # flag to know when we found first header   \n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()  \n",
    "        # Detect header section\n",
    "        if line.startswith(\"polariser cell info\") or line.startswith(\"analyser cell info\"):\n",
    "            header_block = []\n",
    "            header_found = {\"polariser\": None, \"analyser\": None}   \n",
    "            # Collect consecutive headers, but keep only the last polariser+analyser\n",
    "            while i < len(lines) and (\n",
    "                lines[i].strip().startswith(\"polariser cell info\")\n",
    "                or lines[i].strip().startswith(\"analyser cell info\")\n",
    "            ):\n",
    "                current = lines[i].strip()\n",
    "                if current.startswith(\"polariser\"):\n",
    "                    header_found[\"polariser\"] = current\n",
    "                elif current.startswith(\"analyser\"):\n",
    "                    header_found[\"analyser\"] = current\n",
    "                header_block.append(current)\n",
    "                i += 1   \n",
    "            # Build chunk if both headers are present\n",
    "            if header_found[\"polariser\"] and header_found[\"analyser\"]:\n",
    "                new_chunk = []           \n",
    "                # Always keep order: polariser first, analyser second\n",
    "                for header in (header_found[\"polariser\"], header_found[\"analyser\"]):\n",
    "                    if not header.endswith(\"\\n\"):\n",
    "                        header += \"\\n\"\n",
    "                    new_chunk.append(header)           \n",
    "                # Collect data lines until next header\n",
    "                while i < len(lines) and not (\n",
    "                    lines[i].startswith(\"polariser cell info\")\n",
    "                    or lines[i].startswith(\"analyser cell info\")\n",
    "                ):\n",
    "                    line = lines[i]\n",
    "                    if not line.endswith(\"\\n\"):\n",
    "                        line += \"\\n\"  # guarantee each row has newline\n",
    "                    new_chunk.append(line)\n",
    "                    i += 1           \n",
    "                chunks.append(new_chunk)\n",
    "            else:\n",
    "                log_message(f\" File '{FileName}' skipped a block: missing polariser or analyser (headers={header_block}).\")  \n",
    "        else:\n",
    "            i += 1\n",
    "    # Logging\n",
    "    if not chunks:\n",
    "        log_message(f\" File '{FileName}' does NOT contain any valid polariser+analyser pair. Skipping.\\n\")\n",
    "        continue\n",
    "    else:\n",
    "        log_message(f\" File '{FileName}' contains {len(chunks)} valid experiment blocks.\")\n",
    "\n",
    "\n",
    "    # 3- Save .fli files for every correct chunk\n",
    "    base_name = FileName.replace(\".fli\", \"\")  # remove .fli for clean filenames\n",
    "    log_message(f\"Creating all the Array files \\n\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        fli_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "        fli_path = output_folder / fli_filename\n",
    "        with open(win_long_path(fli_path), \"w\") as f_out:\n",
    "            f_out.writelines(chunk)  # each row is written exactly as it was\n",
    "    # CellID file at the same level as the notebook\n",
    "    cellid_file = Path(\"AmorphousCell_ID.txt\").resolve()\n",
    "\n",
    "    # Make sure the parent folder exists (should be current working directory)\n",
    "    cellid_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create the file if it doesn't exist\n",
    "    if not cellid_file.exists():\n",
    "        cellid_file.touch()  # creates an empty file\n",
    "    # 4- As CellID can be exchanged with real parameters, it is written in an independent file\n",
    "    try:\n",
    "        with open(win_long_path(cellid_file), 'r') as file:\n",
    "            seen_strings = set(line.strip() for line in file)\n",
    "    except FileNotFoundError:\n",
    "        seen_strings = set()\n",
    "    # Path to the file\n",
    "\n",
    "    # 5- Open each Array file and work with it (The Array file still has the header)\n",
    "    with open(win_long_path(cellid_file), 'a') as file:\n",
    "        for i in range(len(chunks)):\n",
    "            FLI_filename = f\"{base_name}_Arrays_{i}.fli\"  # Name of the Array file\n",
    "            FLI_path = output_folder / FLI_filename  # Path\n",
    "            df = pd.read_csv(win_long_path(FLI_path), sep=r'\\s+', header=None, on_bad_lines='skip')\n",
    "            log_message(f\"Reading {FLI_path}, removing ***WARNING No centering scan found\")\n",
    "            warning_str = \"***WARNING No centering scan found\"\n",
    "            print(df)\n",
    "\n",
    "            \n",
    "            #5.1 Combine first 4 columns as strings, join them with space, and filter rows containing this phrase (it is not important for us)\n",
    "            df = df[~df.iloc[:, :5].astype(str).agg(' '.join, axis=1).str.contains('No centering scan found', regex=False)] \n",
    "            \n",
    "            #5.2 Extract useful information from the header. Hopefully, CellID, Pressure, LabPolarization, Year, Month, Day, time of lab measurement before first experiment measurement (negative time) will be stored locally\n",
    "            log_message(f\"Header Information Extraction...\")\n",
    "            PolariserID =          df.iloc[0].tolist()[3]\n",
    "            AnalyserID =           df.iloc[1].tolist()[3]\n",
    "            PolariserPressure =    df.iloc[0].tolist()[6]\n",
    "            AnalyserPressure =     df.iloc[1].tolist()[6]\n",
    "            LabPolarization = df.iloc[0].tolist()[7]\n",
    "\n",
    "            try:\n",
    "                HM, DD, MM, YY = df.iloc[0].tolist()[14], int(df.iloc[0].tolist()[10]), int(df.iloc[0].tolist()[11]), int(df.iloc[0].tolist()[12])\n",
    "                Day_Ref = f\"{DD:02d}/{MM:02d}/{YY:02d}\"\n",
    "                dt = Time(Day_Ref, HM)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping file {file_path} because of invalid header data: {e}\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            #5.3 All redundant/useless information is removed\n",
    "            log_message(f\"Removing Measurement Index, Unknown column, Flipping Ratio, Uncertainty of Flipping Ratio and Time between measurements,...\")\n",
    "            df = df.iloc[2:].reset_index(drop=True)\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "            df = df.drop(df.columns[5], axis=1)\n",
    "            df = df.drop(df.columns[7], axis=1)\n",
    "            df = df.drop(df.columns[7], axis=1)\n",
    "            df = df.drop(df.columns[7], axis=1)\n",
    "            #log_message(f\"Saving only polarization values for the Spin Directions wanted in both Polarizer Cells, i.e. (+z,+z)\")\n",
    "\n",
    "\n",
    "            #5.5 Convert Miller index columns into integers. From string or object to float and if the float is close to an integer (tolerance is 1e-8) then save as integer. Otherwise remove row\n",
    "            cols_to_convert = [1, 2, 3]\n",
    "            df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce').astype(float)            \n",
    "            mask = np.isclose(df[cols_to_convert], np.round(df[cols_to_convert]), atol=1e-8)\n",
    "            df = df[mask.all(axis=1)].copy()\n",
    "            log_message(f\"All irrational Miller Indices removed. Adding DeltaTime\")\n",
    "            \n",
    "            #5.5 The time columns are converted into difference of time being the referenced time the first +z,+z measurement that has survived at this point\n",
    "            if df.shape[0] < 2:\n",
    "                log_message(f\"Not enough valid rows after filtering, skipping chunk\")\n",
    "                continue\n",
    "            df['DeltaTime'] = df.apply(\n",
    "                lambda row: deltatime(df[4].iloc[0], df[5].iloc[0], row[4], row[5]), axis=1 )\n",
    "            ref_dt = Time(df[4].iloc[0], df[5].iloc[0])\n",
    "            LabTime = int((dt - ref_dt).total_seconds())\n",
    "\n",
    "            #5.6 Rename the columns PolarizationD3, ErrPolarizationD3 (the polarization column and its uncertainty). The other one with name is DeltaTime. The rest are numbers (will be erased).\n",
    "            #Also we remove the time strings (with DeltaTime they have no new information)\n",
    "            log_message(f\"Renaming PolarizationD3 and ErrPolarizationD3\")\n",
    "            df.rename(columns={\n",
    "                df.columns[5]: 'PolarizationD3',\n",
    "                df.columns[6]: 'ErrPolarizationD3'\n",
    "            }, inplace=True)\n",
    "            df.drop(columns=[df.columns[3], df.columns[4]], inplace=True)\n",
    "            log_message(f\"Dropped Time Strings\")\n",
    "\n",
    "            \n",
    "            #5.7 Begin filtering and softening with previous functions\n",
    "            log_message(f\"Begin removal of Bad files and softening with Savitzky-Golay filter\")\n",
    "            filtered_df, PrettyCombination = filter_best_combination(i,\n",
    "                df,\n",
    "                filter_func=savgol_filter,\n",
    "                filter_column_idx=df.columns.get_loc('PolarizationD3'),\n",
    "                new_column_name='SoftPolarizationD3',\n",
    "                filter_params_func=savgol_params_func,\n",
    "                min_points_required=3,\n",
    "                tolerance=1e-8,\n",
    "                time_column_idx=df.columns.get_loc('DeltaTime'),\n",
    "                error_column_idx=df.columns.get_loc('ErrPolarizationD3')\n",
    "                )\n",
    "            #If nothing survived the filters/purge then use'continue' and go for the next experiment\n",
    "            if filtered_df is None and PrettyCombination is None:\n",
    "                log_message(f\"Chunk {i}: No suitable combination found. Skipping to next chunk or file.\")\n",
    "                log_message(f\"_______________________________________________________________\\n\")\n",
    "                continue  # skip to next chunk\n",
    "            \n",
    "            #5.8 Removal of Miller indices (we have all the information they could give us)\n",
    "            log_message(f\"Removing Miller Indices columns\")\n",
    "            #print(filtered_df)\n",
    "            filtered_df = filtered_df.iloc[:, 3:]\n",
    "            desired_order = [\"DeltaTime\", \"PolarizationD3\", \"SoftPolarizationD3\", \"ErrPolarizationD3\"]\n",
    "\n",
    "            \n",
    "            #5.9 Remove the points that won't be useful for the ML algorythm\n",
    "            columns_to_save = [col for col in desired_order if col in filtered_df.columns] # Keep only the columns that actually exist (in case something is missing)\n",
    "            df_SEMIFINAL = filtered_df[columns_to_save].copy()\n",
    "            df_FINAL = filtered_df = RemoveOutcast_FixUncertainty(df_SEMIFINAL, PrettyCombination, filename=f\"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}\", AcceptableMultiplier=1.3, ShowPlot=True)\n",
    "\n",
    "            #5.10 Plot the succesful experiments\n",
    "            log_message(f\"Plot of Data. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            T = pd.to_numeric(df_FINAL[\"DeltaTime\"], errors='coerce')\n",
    "            P = pd.to_numeric(df_FINAL[\"PolarizationD3\"], errors='coerce')\n",
    "            Err = pd.to_numeric(df_FINAL[\"ErrPolarizationD3\"], errors='coerce')\n",
    "            P_soft = pd.to_numeric(df_FINAL[\"SoftPolarizationD3\"], errors='coerce')\n",
    "            # Scatter plot\n",
    "            plt.scatter(T, P, linewidth=1, label='Original') \n",
    "            \n",
    "            # Dashed line connecting points\n",
    "            plt.plot(T, P, linestyle='--', color='blue', alpha=0.7)\n",
    "            plt.scatter(T, P, linewidth=1, label='Original') \n",
    "            \n",
    "            # Dashed line connecting points\n",
    "            plt.plot(T, P, linestyle='--', color='blue', alpha=0.7)\n",
    "            \n",
    "            # Error bars\n",
    "            plt.errorbar(T, P, yerr=Err, fmt='none', ecolor='gray', alpha=0.5)\n",
    "            \n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"PolarizationD3\")\n",
    "            plot_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}.png\"\n",
    "            plt.title(plot_filename)\n",
    "            plt.ylim(np.min(P - Err), np.max(P + Err))\n",
    "            plt.yticks(np.linspace(np.min(P - Err), np.max(P + Err), 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path = output_folder / plot_filename\n",
    "            plt.savefig(win_long_path(plot_path), dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            #if ShowPlot:\n",
    "            #    plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            log_message(f\"Plot of Filtered Data. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Softened\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            \n",
    "            # Scatter\n",
    "            plt.scatter(T, P_soft, linewidth=1, label='Filtered')\n",
    "            \n",
    "            # Dashed line\n",
    "            plt.plot(T, P_soft, linestyle='--', color='green', alpha=0.7)\n",
    "            \n",
    "            # Error bars\n",
    "            plt.errorbar(T, P_soft, yerr=Err, fmt='none', ecolor='gray', alpha=0.5)\n",
    "            \n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"SoftPolarizationD3\")\n",
    "            plot_filename_soft = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Softened.png\"\n",
    "            plt.title(plot_filename_soft)\n",
    "            plt.ylim(np.min(P_soft - Err), np.max(P_soft + Err))\n",
    "            plt.yticks(np.linspace(np.min(P_soft - Err), np.max(P_soft + Err), 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path_soft = output_folder / plot_filename_soft\n",
    "            plt.savefig(win_long_path(plot_path_soft), dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            #if ShowPlot:\n",
    "            #    plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            log_message(f\"Comparison Plot. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Comparison\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            \n",
    "            # Original\n",
    "            plt.scatter(T, P, linewidth=1, color='blue', alpha=0.6, label='Original')\n",
    "            plt.plot(T, P, linestyle='--', color='blue', alpha=0.5)\n",
    "            \n",
    "            # Filtered\n",
    "            plt.scatter(T, P_soft, linewidth=1, color='green', alpha=0.6, label='Filtered')\n",
    "            plt.plot(T, P_soft, linestyle='--', color='green', alpha=0.5)\n",
    "            \n",
    "            # Error bars (for both)\n",
    "            plt.errorbar(T, P, yerr=Err, fmt='none', ecolor='gray', alpha=0.3)\n",
    "            plt.errorbar(T, P_soft, yerr=Err, fmt='none', ecolor='gray', alpha=0.3)\n",
    "            \n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"Polarization\")\n",
    "            plot_filename_combined = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Combined.png\"\n",
    "            plt.title(plot_filename_combined)\n",
    "            min_y = min(np.min(P - Err), np.min(P_soft - Err))\n",
    "            max_y = max(np.max(P + Err), np.max(P_soft + Err))\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(min_y, max_y)\n",
    "            plt.yticks(np.linspace(min_y, max_y, 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path_combined = output_folder / plot_filename_combined\n",
    "            plt.savefig(win_long_path(plot_path_combined), dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            #if ShowPlot:\n",
    "            #    plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "            # 5.11 Save the files.\n",
    "            log_message(f\"Finally we save the chunk\")\n",
    "            csv_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}.txt\"\n",
    "            Parameter_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\"\n",
    "            csv_path = output_folder / csv_filename\n",
    "            \n",
    "            # Save the CSV file\n",
    "            df_FINAL.to_csv(win_long_path(csv_path), index=False, sep=',')\n",
    "            \n",
    "            # Also save the same txt file into MLDataBaseFolder\n",
    "            ml_txt_path = MLDataBaseFolder / csv_filename\n",
    "            df_FINAL['DeltaTime'] = df_FINAL['DeltaTime'] - df_FINAL['DeltaTime'].iloc[0]\n",
    "\n",
    "            df_FINAL.to_csv(win_long_path(ml_txt_path), index=False, sep=',')\n",
    "            log_message(f\"Saved: {csv_filename}\")\n",
    "        \n",
    "            # Second the parameter file\n",
    "            log_message(f\"Processing Parameter file...\")\n",
    "            ml_txt_path_param = MLDataBaseFolder / Parameter_filename\n",
    "            with open(win_long_path(ml_txt_path_param), 'w') as f:\n",
    "                # First row: column names\n",
    "                f.write(\"CellID,AnalyserID,PolariserPressure,AnalyserPressure,LabPolarization,LabTime\\n\")\n",
    "                # Second row: values, comma separated\n",
    "                f.write(f\"{PolariserID},{AnalyserID},{PolariserPressure},{AnalyserPressure},{LabPolarization},{LabTime}\")\n",
    "            log_message(f\"Saved: {Parameter_filename}\")\n",
    "            log_message(f\"Parameter and Array files saved to ML database: {MLDataBaseFolder}\\n{'_'*63}\\n\\n\")\n",
    "\n",
    "\n",
    "# 5.12 Remove unwanted folders and files\n",
    "for i in range(len(chunks)):\n",
    "    temp_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "    temp_path = output_folder / temp_filename\n",
    "    try:\n",
    "        temp_path.unlink()  # delete the file\n",
    "    except FileNotFoundError:\n",
    "        pass  # if somehow it doesn't exist, just skip        \n",
    "log_message(f\"Created and saved {len(chunks)} CSV files from file called {FileName}.\")\n",
    "\n",
    "# After processing all chunks for the file:\n",
    "if output_folder.exists() and not any(output_folder.iterdir()):\n",
    "    output_folder.rmdir()\n",
    "    log_message(f\"Removed empty folder: {win_long_path(output_folder)}\")\n",
    "log_message('\\n\\n')\n",
    "\n",
    "ml_database_folder = \"AmorphousMLDataBase\"\n",
    "\n",
    "# Find all txt files whose names end with Parameters.txt (case insensitive)\n",
    "parameter_files = glob.glob(os.path.join(ml_database_folder, '*Parameters.txt'))\n",
    "\n",
    "log_message(f\"Found {len(parameter_files)} parameter files.\")\n",
    "\n",
    "unique_id_pairs = set()\n",
    "\n",
    "for filepath in parameter_files:\n",
    "    try:\n",
    "        with open(win_long_path(filepath), 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 2:\n",
    "                second_row = lines[1].strip()\n",
    "                parts = second_row.split(',')\n",
    "                if len(parts) >= 2:\n",
    "                    polariser_id = parts[0]\n",
    "                    analyser_id = parts[1]\n",
    "                    unique_id_pairs.add((polariser_id, analyser_id))\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to read {filepath}: {e}\")\n",
    "\n",
    "# File in the same folder as the notebook\n",
    "unique_ids_file = Path.cwd() / \"AmorphousPolariserAndAnalyser_IDs.txt\"\n",
    "\n",
    "# Ensure parent folder exists\n",
    "unique_ids_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "# Write to unique_ids.txt\n",
    "with open(win_long_path(unique_ids_file), \"w\", encoding=\"utf-8\") as f:\n",
    "    for polariser_id, analyser_id in sorted(unique_id_pairs):\n",
    "        f.write(f\"{polariser_id},{analyser_id}\\n\")\n",
    "\n",
    "print(f\"Saved {len(unique_id_pairs)} unique polariser/analyser ID pairs to AmorphousPolariserAndAnalyser_IDs.txt.\")\n",
    "\n",
    "# Delete useless folders\n",
    "for folder_to_delete in [\"AmorphousSeparatedFolder\", \"AmorphousDataBase\"]:\n",
    "    folder_path = Path(folder_to_delete)\n",
    "    if folder_path.exists():\n",
    "        shutil.rmtree(win_long_path(folder_path))\n",
    "        log_message(f\"Folder '{folder_path}' has been deleted.\")\n",
    "    else:\n",
    "        log_message(f\"Folder '{folder_path}' does not exist.\")\n",
    "\n",
    "# Removal of Duplicates\n",
    "hash_map = defaultdict(list)\n",
    "\n",
    "def file_sha256(filepath, block_size=65536):\n",
    "    \"\"\"Compute SHA256 hash of a file (safe for large files).\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(win_long_path(filepath), \"rb\") as f:\n",
    "        while chunk := f.read(block_size):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "# Scan all .txt files (only base files without '_Parameters')\n",
    "for root, _, files in os.walk(ml_database_folder):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".txt\") and \"_parameters\" not in file.lower():\n",
    "            path = os.path.join(root, file)\n",
    "            file_hash_val = file_sha256(path)\n",
    "            hash_map[file_hash_val].append(path)\n",
    "\n",
    "# Report & delete duplicates\n",
    "duplicates_found = False\n",
    "for file_hash_val, paths in hash_map.items():\n",
    "    if len(paths) > 1:\n",
    "        duplicates_found = True\n",
    "        log_message(f\"\\nDuplicate group (hash={file_hash_val}):\")\n",
    "        log_message(f\"   Keeping: {paths[0]}\")\n",
    "\n",
    "        for p in paths[1:]:\n",
    "            base_name, ext = os.path.splitext(p)\n",
    "            param_file = f\"{base_name}_Parameters{ext}\"\n",
    "\n",
    "            try:\n",
    "                os.remove(win_long_path(p))\n",
    "                log_message(f\"   Deleted duplicate base file: {p}\")\n",
    "            except Exception as e:\n",
    "                log_message(f\"   Could not delete base file {p}: {e}\")\n",
    "\n",
    "            if os.path.exists(param_file):\n",
    "                try:\n",
    "                    os.remove(win_long_path(param_file))\n",
    "                    log_message(f\"   Deleted parameter file: {param_file}\")\n",
    "                except Exception as e:\n",
    "                    log_message(f\"   Could not delete parameter file {param_file}: {e}\")\n",
    "\n",
    "if not duplicates_found:\n",
    "    log_message(\"No duplicates found in MLDataBase!\")\n",
    "else:\n",
    "    log_message(\"\\nDuplicate cleanup complete!\")\n",
    "\n",
    "# Remove AmorphousCell_ID.txt if exists\n",
    "file_path = Path(\"AmorphousCell_ID.txt\")\n",
    "if file_path.exists():\n",
    "    file_path.unlink()\n",
    "    print(f\"{win_long_path(file_path)} has been deleted.\")\n",
    "else:\n",
    "    print(f\"{win_long_path(file_path)} does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970ffb5-4bf4-4d15-8702-8c363764429b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
