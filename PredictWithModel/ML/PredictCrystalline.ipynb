{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88e7222-6559-4fa6-b01c-7edcf4b4cb13",
   "metadata": {},
   "source": [
    "<h1>PredictCrystalline.ipynb </h1>\n",
    "\n",
    "It takes the files from CrystallineFileLecturePredict.ipynb and predicts with them. There is now only one function that can be used for prediction, Predict.\n",
    "**IMPORTANT, IN THE FOLDER CrystallineModels YOU HAVE TO MANUALLY PASTE THE FOLDERS WITH THE MODELS. IT IS A MANUAL PROCESS. THE REST OF THE PROCESS SHOULD BE AUTOMATED EXCEPT FOR THE TIME CONDITIONS THAT ARE ASKED TO THE USER**\n",
    "\n",
    "Here is a diagram of the folder output structure:\n",
    "        A folder structure with the predictions:\n",
    "        \n",
    "        - PredictCrystalline.ipynb\n",
    "        - CrystallinePredictionsFolder\n",
    "          |\n",
    "          |_Model_{NameOfModel 1}\n",
    "          |    |\n",
    "          |    |_Experiment_{NameOfExperiment 1}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |\n",
    "          |    |   |_Text file with the results. First column is the internal program time\n",
    "          |    |   |                             Second column is the time in real life of those predictions (YY-MM-DD HH:mm:SS\n",
    "          |    |   |                             Third column is the predicted polarization\n",
    "          |    |   |                             Fourth column is the uncertainty\n",
    "          |    |   |\n",
    "          |    |   |_PolarizationD3_{Name}_Paramteres.txt (original parameter file)\n",
    "          |    |   |_PolarizationD3_{Name}.txt (original data file)\n",
    "          |    |   \n",
    "          |    |_Experiment_{NameOfExperiment 2}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |\n",
    "          |    |   |_Text file with the results.  \n",
    "          |    |   |\n",
    "          |    |   |_Parameter and array original files\n",
    "          |    |\n",
    "          |    |_(...)\n",
    "          |\n",
    "          |_Model_{NameOfModel 2}\n",
    "          |    |\n",
    "          |    |_Experiment_{NameOfExperiment 1}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |_Text file with the results. First column is the internal program time\n",
    "          |    |   \n",
    "          |    |_Experiment_{NameOfExperiment 2}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |_Text file with the results.   \n",
    "          |    |   |_Parameter and array original files\n",
    "          |    |\n",
    "          |    |_(...)\n",
    "          |\n",
    "          |_Combined model\n",
    "          |    |\n",
    "          |    |_Experiment_{NameOfExperiment 1}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |_Text file with the results. First column is the internal program time\n",
    "          |    |   \n",
    "          |    |_Experiment_{NameOfExperiment 2}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |\n",
    "          |    |   |_Text file with the results.  \n",
    "          |    |   |\n",
    "          |    |   |_Parameter and array original files   \n",
    "          |    |\n",
    "          |    |_(...)\n",
    "          |_(...)\n",
    "\n",
    "\n",
    "\n",
    "The inputs of the code are:\n",
    "1. The point in the past to start predicting (e.g. if you don't want to start your predictions at the time of the first measurement you can write -123 and the predictions will start 123 seconds before the first experimental measurement). Accepts positive and negative values and I believe even floats (not tested for non-integer values but I don't think it makes sense to ask for time values with a precision smaller than what the files from D3 have)\n",
    "2. The time between steps (the time interval between predictions, e.g, if you want predictions every 4 seconds, write as the second input 4).\n",
    "3. The point in the future to stop predicting (e.g. if you don't want to stop at the last experimental measurement you can write 1024 and the last polarization will be the last step before t_final+1024). Accepts positive and negative values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___________________________________________________________________________________________\n",
    "\n",
    " \n",
    "OUTPUTS OF THE CODE (the important ones)\n",
    "\n",
    "1. **CrystallineLogFile_PredictingML.txt**\n",
    "It is a log with all the important steps that the code has done\n",
    "\n",
    "2. **AbsoluteTimes.txt**\n",
    "It is only created if you use Predict (not with PredictWithPolarizationTimeReference). It stores for each experiment (and model, if I have time I will remove those duplications) the name of the experiment, the time where the first polarization measurement was recorded (in an absolute format like \"2023-12-08 13:06:39\") and the time value that was used for the Predict function (it is just DiffractogramAbsoluteTime that was added so there is a txt file with all the information needed to change from time intervals with random reference points to absolute time strings if needed)\n",
    "\n",
    "3. **CrystallineExecution_times.txt**\n",
    "It records the time it took each individual experiment to be fully predicted. The time depends on the amount of points and model but 48 full predictions with 20 second gaps took less than 90 seconds (hopefully it is fast enough) \n",
    "\n",
    "4. **CrystallinePredictionsFolder**\n",
    "Stores all the predictions. The first subdivision of folders separate the information using the model\n",
    "\n",
    "    4.1 **Model\\_{Complexity}\\_{num\\_augmentations}**\n",
    "Depending of the model (characterized using two strings) you can obtain predictions for all the experiments. In each Model_{Complexity}_{num_augmentations} folder you will find folders for each experiment\n",
    "\n",
    "        4.1.1 Experiment_{base_name}\n",
    "For each model and {base_name} (or experiment name) you can find one of these folders. The contents contain the predictions\n",
    "\n",
    "            4.1.1.1 {Complexity}_{num_augmentations}_{base_name}.jpg\n",
    "Contains the pure ML predictions in red and the correction (in green) using the first and final polarization measurements. For more information read the explanation of what the code does.\n",
    " \n",
    "            4.1.1.2 PredictedData_{Complexity}_{num_augmentations}_{base_name}\n",
    "Contains the same information as the green curve but on a txt file. On the first column you have the relative time where t=0 corresponds to the first correct polarization measurement. On the second column you have the absolute time (in a format datetime.strptime(\"2023-12-08 13:06:39\", \"%Y-%m-%d %H:%M:%S\") ) and on the second and third columns you can find the polarization and uncertainty of polarization associated to those time values\n",
    "\n",
    "\n",
    "\n",
    "___________________________________________________________________________________________\n",
    "\n",
    "\n",
    "Information about the parts\n",
    "\n",
    "\n",
    "\n",
    "Here we have functions that train, validate and fit the models. Some models require the variables to be scaled or will scale them. Extra precautions need to be taken into account\n",
    "1. _PrintDebug_ is a flag that allows the code to output on screen all the steps. If it is set to false, it won´t show anything. However, all information will be properly logged whether this flag is set to true or false. The name of the log is determined by the variable *log_file_path*. The code runs faster if it is set to False.\n",
    "\n",
    "2. _ShowPlot_ is a similar flag that allows the code to show on screen all plots that are being produced. They are all stored independently of whether this flag is True or False. The code runs faster if it is set to False.\n",
    "\n",
    "3. _LogNoise_ is another flag that allows numerical values in the log. Most of these values are not worth keeping but if you want to see if there are no NaNs or zeros you can turn it on\n",
    "\n",
    "\n",
    "4. **log_message** is a function used for writting on the log file\n",
    "\n",
    "5. **win_long_path** is a function that \"fixes\" directory paths\n",
    "\n",
    "6. **load\\_experiments**. It uses the directory where the array (numerical values) and parameter files resides, picks one polarization column and prepares a list of experiments to feed the ML algorythm. The output is as follows:\n",
    "\n",
    "    *encoded\\_experiments=[(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 1}$,(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 2}$,...]* \n",
    "    \n",
    "    Note that the Cell Id is Hot Encoded. The type of cell did not affect greatly the predictions. However in the future, it may be useful to give more information to Cell_ID. As of 2025, these strings are Hot Encoded which means that the code finds all the different types, creates columns for each type and writes 0 or 1 (bool) depending of whether the cell was from one type or the other. This is the standard procedure to feed categorical variables to ML.\n",
    "    The size of the output list depends on the number of pairs of .txt files present in the folder. This means that it also works for isolated experiments.\n",
    "    \n",
    "7. **build_dataset** is a function that prepares the data base to be fed directly into a ML model for training and validation. There are a few import decisions taken here. The way this function is set up, it removes 2 rows of data per polariser cell studied. The reason why it was done is because we want the ML model to be able to predict polarization decay when given the specs of the cell (the static parameters) and the initial and final polarization values (with their associated time values). The reason why those two values are considered \"known values\" for each cell is beacuse they can be easily measured experimentally and they give as a good estimate about the overall behaviour. In some experiments, the environment of the studied sample is too fragile to move and place the Si crystal for polarization measurements. Therefore, a working ML algorithm whose inputs are the specs of the cell and the initial and final polarizations (measured before and after the sample is in place) would enable experiments that could not be done before without proper polarization efficiency corrections. \n",
    "Also, we remove those two values per polariser cell from the training arrays (Xt, y and err). The reason why it is done is to avoid data leaks in the model. If we give those values as training data and also give them as parameters, the ML algorithm can run into the risk of memorizing those pairs (over-fitting) and worsen any new predicitions. Uncertainties for the first and last polarization measurements are not added to the static features. This was a decision taken to avoid giving too much weight to two variables that don't have value on their own (they compliment the polarization values but if the ML architecture is as shallow as\n",
    "the one used here, they can be considered independent variables and reduce the weight and importance of the other variables). \n",
    "Another consideration taken here was that the static features get duplicated in Xs a lot of times. One could think that using a similar method that the one used for augmentations of the data sets could also diversify the data base. However, we wanted all measurements of a same session and cell to be coherent \n",
    "and it wouldn't make sense to have different static features. Therefore, the safest approach was to only duplicate these values. For the augmented experiments the only parameters that are changed are the initial and final times and polarizations. There is no incompatibility here to what we have just said as these work as \"hypothetical independent experiments\". This is why augmentation is done before this function gets used.\n",
    "         \n",
    "\n",
    "8. **nll_loss** ML algorythms require a way to tell the algorythm if it is learning or not. The most standard practice is with a Loss function. If the loss value goes down that means that the algorythm is learning and, if a step increases the loss, then it is punished and tries other approaches. When using uncertainties when teaching the model, the most common loss function is the NLL or Negative log-likelihood of a normal distribution \n",
    "NLL$=\\frac{1}{2}$log$(σ^2)+\\frac{(y−μ)^2}{2σ^2}$\n",
    "where $\\sigma$ is the uncertainty in the predictions, $y$ is the measured value and $\\mu$ the predicted value. Instead of predicting $σ^2$ directly, we obtain its logarithm to have a more stable process (and avoid accounting precision as $\\sigma^{-2}$ which is numerically unstable when uncertainties are low).\n",
    "\n",
    "However we want to avoid uncertainties that drift too far from the overall model predictions. To achieve that, we can get a rough estimate on what the uncertainty of a set predictions looks like.\n",
    "Let $\\vec{\\mu}=\\left(\\mu_1,\\ldots,\\mu_N\\right)^T$ be the vector of $N$ predicted values. It can be considered as a random vector of variance:\n",
    "$Var(\\vec{\\mu})=\\frac{1}{N}\\sum_{i=1}^N\\left(\\mu_i-E(\\vec{\\mu})\\right)$\n",
    "where $E(\\vec{\\mu})$ is the mean of the predicted values. We then have two different variances, one obtained as the sparseness of the predictions, (denoted as $Var\\left(\\vec{\\mu}\\right)$, and one obtained as a result of the internal ML calculations (denoted as $\\sigma^2$). A penalty can be added to the loss functions to force the model to try to reduce this differences. An easy way to model it is to obtain the difference of those variances and square the result (taking the absolute value was also a good estimate, but using squared values punished big discrepancies in a stronger way).\n",
    "\n",
    "$StrayPenalty = B \\cdot \\left[\\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}\\left(\\vec{\\mu}\\right)\\right)\\right]^2$\n",
    "where $B$ is a constant used to control the weight of this penalty. The reason why $Var\\left(\\vec{\\mu}\\right)$ was used and not $Var\\left(\\vec{y}\\right)$ (with $\\vec{y}$ the vector of measured values) was to avoid noise in the original data to tamper with the loss function. It would be physically clearer to use measured values sparseness as a way to guide the model but some experimental uncertainties are clearly underestimated and that would cause this penalty to dominate the loss and obscure the main loss protocol, the NLL.\n",
    "\n",
    "Also, we also want to punish the model if it tries overestimating $\\sigma$. If the model is unable to minimize $y-\\mu$, in order to lower NLL, it increases $\\sigma$. If no precautions are taken, this \"escape solution\" achieves bad predictions with inflated uncertainties that simulate a low loss value. A new penalty was added that punishes overestimation of the uncertainties more than underestimation (which never happened). The slope correction done further on the pipeline can \"fix\" this issue but what the model returns then is closer to a poorly calculated linear fit\n",
    "Therefore, an addition penalty was added.\n",
    "\n",
    "$OverestimatePenalty= C \\cdot \\max\\left(0, \\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}(\\vec{\\mu})\\right)\\right)$\n",
    "where $C$ is a constant used to control the weight of this penalty\n",
    "\n",
    "9. **model\\_fitting**. It is a function that logs and runs model.fit() on a two-input Keras model and returns the training history. It needs **static, time and polarization variables scaled**. Training is not done using the uncertainties of the data as it was decided that uncertainty information is encoded in the augmentations. Note: No validation is done anywhere in the code. Here are some of the reasons:\n",
    "    \n",
    "    9.1. The data base is very small. The amorphous data base contains only 199 points while the crystalline one contains 251. Removing a small percentage of those points for validation might leave the data base too small and underfitting might worsen the result more than fine tuning parametrs with validation.\n",
    "    \n",
    "    9.2. A randomized validation split may be physically wrong. Therefore it should be chronological, not shuffled. However, in crystalline experiments, there are decay experiments that have only four or five intermediate points. Even removing one point for validation is a massive hit on the experiment. Therefore, it is risky to add validation\n",
    "    \n",
    "    9.3. To find good models, a Leave-one-out approach was used. For a certain model structure, an experiment gets removed and the model and it trains on all the remaining experiments. Then, the model tries to predict this isolated experiment. Afterwards, the experiment is returned and a new one becomes isolated. This process loops for all experiments and an overall score of the model is computed. This process was done for 498 models for crystalline materials. This is a stronger (and more expensive) method than validation as it is not dependent on the validation splits and avoids possible information leaks.\n",
    "\n",
    "Also, eight randomly picked models were tested with and without validation and with and without an asymetric uncertainty-overestimated penalizing loss. The result showed that the Loss update was an improvement and validation did not increase performance (without validation, the results were slightly better).\n",
    "\n",
    "10. **model\\_prediction**. It is a funtion that predicts with a given model. It needs **static and time variables scaled**. This scaling must be coherent to the one done in the rest of the funtions.\n",
    "\n",
    "111. **train**. This function is the one responsible of scaling the inputs and training the model (it uses **model\\_fitting**)\n",
    "\n",
    "     11.1. It creates the independent arrays with all the encoded experiments (augmented or not) using build_dataset\n",
    "    \n",
    "     11.2. Then it scales the data. ML algorithms work better when the inputs and outputs are normalized. The reason why we don't normalize inside the function is to have those scaler defined globally and not locally\n",
    "    \n",
    "     11.3. It builds the model depending on the use\\_uncertainty bool. (It changes the loss function and the output).\n",
    "    \n",
    "     11.4. It trains the model and returns its history (the trained model)\n",
    "    \n",
    "12. **align_static_vectors**. It converts the columns not present on an isolated experiment to zeros.\n",
    "    \n",
    "13. **model_predict_sloped** It substracts a linear function to the predicted values. If done correctly this makes it so that the polarization predictions at the initial and final time points are the same as the measured polarization values at those times. This fixes a vertical shift and also an overall slope. As it is a correction done with experimental values, the algorithm is still \"universal\". However we can't fully say that it is a pure ML algorithm. The \"correctness\" of this method is subjective. It is a warning in the ML front that there is an issue with the data base but it is a valid fix for experimentalists.\n",
    "___________________________________________________________________________________________\n",
    "\n",
    "Process it follows:\n",
    "\n",
    "The steps it takes are explained here:\n",
    "1. Obtain the absolute time references for the experiments (year, month, day, hour, minute and second)\n",
    "2. Loop over all models that will be used\n",
    "3. Loop over all two-numerical-rowed experiments. It copies them (does't move them) to the final folders\n",
    "4. Build the structure that the ML model was trained with and scale all the experiment using the scalers that the model used in training\n",
    "5. Predict, correct the slope if Correction=True, plot everything and write the results on a .txt file\n",
    "\n",
    "Finally, it runs the _Predict_ function. The code will not progress until the user manually inputs the time values. It asks for three values, t_initial (which means that predictions start _t_initial_ seconds later (it can be negative)), _t_final_ (it changes the final time point for predictions by t_final seconds) and _t_step_ (the time steps between predictions)\n",
    "It also takes all previous predictions of all models and averages them using a weighted mean. That way we have a single prediction file that should be the best that ML can currently do.\n",
    "For every experiment, the code finds all the experiment files created by each model. Then, for every line it performs the following calculation. Let $P_1\\left(t\\right),\\ldots,P_n\\left(t\\right)$ be the polarization predictions of models $1,\\ldots,n$ at time $t$ with uncertainties $\\sigma_1\\left(t\\right),\\ldots,\\sigma_n\\left(t\\right)$. Then the weighted mean is \n",
    "\n",
    "$P\\left(t\\right)=\\frac{\\sum_{i=1}^n P_i\\left(t\\right)\\sigma_i^{-2}\\left(t\\right)}{\\sum_{i=1}^n \\sigma_i^{-2}\\left(t\\right)}$\n",
    "\n",
    "with\n",
    "\n",
    "$\\sigma^2\\left(t\\right)=\\frac{1}{\\sum_{i=1}^n \\sigma_i^{-2}\\left(t\\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fdcef2-af5b-4307-9d3e-f33a274f63e4",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f0c98-bedb-49b5-9978-a1151c8f04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import shutil\n",
    "import gc\n",
    "import joblib\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import time as pytime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras import Input, Model, regularizers\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd365771-876b-462c-a4fe-7df6c4380626",
   "metadata": {},
   "source": [
    "## 2. Auxiliary Functions and log file creation\n",
    "\n",
    "1. _PrintDebug_ is a flag that allows the code to output on screen all the steps. If it is set to false, it won´t show anything. However, all information will be properly logged whether this flag is set to true or false. The name of the log is determined by the variable *log_file_path*. The code runs faster if it is set to False.\n",
    "\n",
    "2. _ShowPlot_ is a similar flag that allows the code to show on screen all plots that are being produced. They are all stored independently of whether this flag is True or False. The code runs faster if it is set to False.\n",
    "\n",
    "3. _LogNoise_ is another flag that allows numerical values in the log. Most of these values are not worth keeping but if you want to see if there are no NaNs or zeros you can turn it on\n",
    "\n",
    "\n",
    "3. **log_message** is a function used for writting on the log file\n",
    "\n",
    "4. **win_long_path** is a function that \"fixes\" directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f503fd-2e16-411b-977f-9fe191a0c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintDebug = False #This Bool will determine if all logs should be printed on screen on the Python Notebook. The log writing is always on. If False the code will be faster.\n",
    "ShowPlot = False #This Bool works the same but with showing on screen the plots (they are always saved even with this variable being False). Reduces program cost if False\n",
    "LogNoise = False #This Bool allows numerical values in the log. Most of these values are not worth keeping but if you want to see if there are no NaNs or zeros you can turn it on\n",
    "log_file_path = \"CrystallineLogFile_PredictingML.txt\"\n",
    "def log_message(message):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        message (string): The text that will be logged\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    Notes:\n",
    "        It will write the string \"message\" in the log file.\n",
    "        If PrintDebug==True then it will also print the string\n",
    "    \"\"\"\n",
    "    message = str(message)\n",
    "    if PrintDebug:\n",
    "        print(message)\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "        log_file.write(str(message) + \"\\n\")\n",
    "        \n",
    "################################################################\n",
    "\n",
    "def long_path(path):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        path (path): The path that needs to be converted\n",
    "    \n",
    "    Returns:\n",
    "        The updated path string or path depending on the platform used\n",
    "        \n",
    "    Notes:\n",
    "        To avoid Windows 260 character limit for Windows paths, a special \"prefix\" is added.\n",
    "        It also unifies how directories are managed.\n",
    "        Also works with Linux and Mac\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert to Path and resolve to absolute\n",
    "    path = Path(path).resolve()\n",
    "    \n",
    "    #Windows only:  \n",
    "    if os.name == \"nt\":\n",
    "        path_str = str(path)\n",
    "        if not path_str.startswith(\"\\\\\\\\?\\\\\"):\n",
    "            # UNC paths need special handling\n",
    "            if path_str.startswith(\"\\\\\\\\\"):\n",
    "                path_str = \"\\\\\\\\?\\\\UNC\\\\\" + path_str[2:]\n",
    "            else:\n",
    "                path_str = \"\\\\\\\\?\\\\\" + path_str\n",
    "            return path_str\n",
    "    \n",
    "    return path\n",
    "\n",
    "to_erase = [\n",
    "    \"CrystallineLogFile_PredictingML.txt\",\n",
    "    \"CrystallineExecution_times.txt\",\n",
    "    \"CrystallinePredictionsFolder\"]\n",
    "\n",
    "for item in to_erase:\n",
    "    path = os.path.abspath(item)\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                log_message(f\"Deleted file: {path}\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                log_message(f\"Deleted folder: {path}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\" Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"Not found (skipped): {path}\")\n",
    "        \n",
    "with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(\"=== Log started ===\\n\")\n",
    "    log_file.write(\"All outputs from functions have a string at the beginning to show the origin:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32403d26-ca58-4b10-8142-a60698f86aa3",
   "metadata": {},
   "source": [
    "## 3. Functions\n",
    "\n",
    "1. **load\\_experiments**. It uses the directory where the array (numerical values) and parameter files resides, picks one polarization column and prepares a list of experiments to feed the ML algorythm. The output is as follows:\n",
    "\n",
    "    *encoded\\_experiments=[(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 1}$,(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 2}$,...]* \n",
    "    \n",
    "    Note that the Cell Id is Hot Encoded. The type of cell did not affect greatly the predictions. However in the future, it may be useful to give more information to Cell_ID. As of 2025, these strings are Hot Encoded which means that the code finds all the different types, creates columns for each type and writes 0 or 1 (bool) depending of whether the cell was from one type or the other. This is the standard procedure to feed categorical variables to ML.\n",
    "    The size of the output list depends on the number of pairs of .txt files present in the folder. This means that it also works for isolated experiments.\n",
    "    \n",
    "3. **build_dataset** is a function that prepares the data base to be fed directly into a ML model for training and validation. There are a few import decisions taken here. The way this function is set up, it removes 2 rows of data per polariser cell studied. The reason why it was done is because we want the ML model to be able to predict polarization decay when given the specs of the cell (the static parameters) and the initial and final polarization values (with their associated time values). The reason why those two values are considered \"known values\" for each cell is beacuse they can be easily measured experimentally and they give as a good estimate about the overall behaviour. In some experiments, the environment of the studied sample is too fragile to move and place the Si crystal for polarization measurements. Therefore, a working ML algorithm whose inputs are the specs of the cell and the initial and final polarizations (measured before and after the sample is in place) would enable experiments that could not be done before without proper polarization efficiency corrections. \n",
    "Also, we remove those two values per polariser cell from the training arrays (Xt, y and err). The reason why it is done is to avoid data leaks in the model. If we give those values as training data and also give them as parameters, the ML algorithm can run into the risk of memorizing those pairs (over-fitting) and worsen any new predicitions. Uncertainties for the first and last polarization measurements are not added to the static features. This was a decision taken to avoid giving too much weight to two variables that don't have value on their own (they compliment the polarization values but if the ML architecture is as shallow as\n",
    "the one used here, they can be considered independent variables and reduce the weight and importance of the other variables). \n",
    "Another consideration taken here was that the static features get duplicated in Xs a lot of times. One could think that using a similar method that the one used for augmentations of the data sets could also diversify the data base. However, we wanted all measurements of a same session and cell to be coherent \n",
    "and it wouldn't make sense to have different static features. Therefore, the safest approach was to only duplicate these values. For the augmented experiments the only parameters that are changed are the initial and final times and polarizations. There is no incompatibility here to what we have just said as these work as \"hypothetical independent experiments\". This is why augmentation is done before this function gets used.\n",
    "         \n",
    "\n",
    "4. **nll_loss** ML algorythms require a way to tell the algorythm if it is learning or not. The most standard practice is with a Loss function. If the loss value goes down that means that the algorythm is learning and, if a step increases the loss, then it is punished and tries other approaches. When using uncertainties when teaching the model, the most common loss function is the NLL or Negative log-likelihood of a normal distribution \n",
    "NLL$=\\frac{1}{2}$log$(σ^2)+\\frac{(y−μ)^2}{2σ^2}$\n",
    "where $\\sigma$ is the uncertainty in the predictions, $y$ is the measured value and $\\mu$ the predicted value. Instead of predicting $σ^2$ directly, we obtain its logarithm to have a more stable process (and avoid accounting precision as $\\sigma^{-2}$ which is numerically unstable when uncertainties are low).\n",
    "\n",
    "However we want to avoid uncertainties that drift too far from the overall model predictions. To achieve that, we can get a rough estimate on what the uncertainty of a set predictions looks like.\n",
    "Let $\\vec{\\mu}=\\left(\\mu_1,\\ldots,\\mu_N\\right)^T$ be the vector of $N$ predicted values. It can be considered as a random vector of variance:\n",
    "$Var(\\vec{\\mu})=\\frac{1}{N}\\sum_{i=1}^N\\left(\\mu_i-E(\\vec{\\mu})\\right)$\n",
    "where $E(\\vec{\\mu})$ is the mean of the predicted values. We then have two different variances, one obtained as the sparseness of the predictions, (denoted as $Var\\left(\\vec{\\mu}\\right)$, and one obtained as a result of the internal ML calculations (denoted as $\\sigma^2$). A penalty can be added to the loss functions to force the model to try to reduce this differences. An easy way to model it is to obtain the difference of those variances and square the result (taking the absolute value was also a good estimate, but using squared values punished big discrepancies in a stronger way).\n",
    "\n",
    "$StrayPenalty = B \\cdot \\left[\\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}\\left(\\vec{\\mu}\\right)\\right)\\right]^2$\n",
    "where $B$ is a constant used to control the weight of this penalty. The reason why $Var\\left(\\vec{\\mu}\\right)$ was used and not $Var\\left(\\vec{y}\\right)$ (with $\\vec{y}$ the vector of measured values) was to avoid noise in the original data to tamper with the loss function. It would be physically clearer to use measured values sparseness as a way to guide the model but some experimental uncertainties are clearly underestimated and that would cause this penalty to dominate the loss and obscure the main loss protocol, the NLL.\n",
    "\n",
    "Also, we also want to punish the model if it tries overestimating $\\sigma$. If the model is unable to minimize $y-\\mu$, in order to lower NLL, it increases $\\sigma$. If no precautions are taken, this \"escape solution\" achieves bad predictions with inflated uncertainties that simulate a low loss value. A new penalty was added that punishes overestimation of the uncertainties more than underestimation (which never happened). The slope correction done further on the pipeline can \"fix\" this issue but what the model returns then is closer to a poorly calculated linear fit\n",
    "Therefore, an addition penalty was added.\n",
    "\n",
    "$OverestimatePenalty= C \\cdot \\max\\left(0, \\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}(\\vec{\\mu})\\right)\\right)$\n",
    "where $C$ is a constant used to control the weight of this penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3ea5e-65a0-469f-ae05-ad4917d7acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiments(data_dir, polarization_column):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data_dir (str): The direction to the folder where all files to be loaded will be found\n",
    "        polarization_column: It is the name in the header (in the .txt files) of the column that will be read.\n",
    "                             It can be 'SoftPolarizationD3' or 'PolarizationD3'. All results were obtained with 'PolarizationD3'\n",
    "    Output: \n",
    "        A list, for all polarization decays like:\n",
    "        encoded_experiments = [\n",
    "            (static_values,   # list of static parameters (per experiment)\n",
    "            Deltatime,       # 1D numpy array of time values\n",
    "            polarization,    # 1D numpy array of polarization values\n",
    "            Uncertainty      # 1D numpy array of uncertainty values), ...]\n",
    "        A pd object with the Hot encoded Ids and the rest of the parameters per experiment (each experiment in a different row)\n",
    "    \n",
    "    Notes:\n",
    "    The steps the code does are the following:\n",
    "    1. Finds all array files (the ones with the numerical values of the decay) and loops over all of them\n",
    "    2. For each array files it reconstructs the name of the parameter file. It concatenates all parameter files into one pd structure.\n",
    "    3. Finally it loops over all array files appending the parameters, time, polarization and uncertainty of every experiment to a common list\n",
    "    \"\"\"\n",
    "    \n",
    "    log_message(f\"    load_experiments: Finding all Array Files...\")\n",
    "    # Step 1:\n",
    "\n",
    "    arrays_files = sorted(\n",
    "        glob.glob(os.path.join(data_dir, \"*.txt\"))) #Find all files that are .txt\n",
    "    arrays_files = [f for f in arrays_files if not f.endswith(\"_Parameters.txt\")] #Keep only the Arrays (not the parameters)\n",
    "\n",
    "    encoded_experiments = []\n",
    "    all_static_df = []\n",
    "    static_columns = ['CellID', 'Pressure', 'LabPolarization', 'LabTime'] #Parameter header\n",
    "    for arrays_path in arrays_files:\n",
    "        base = os.path.basename(arrays_path)\n",
    "        # Build parameters filename by adding _Parameters before .txt\n",
    "        name_without_ext = os.path.splitext(base)[0]\n",
    "        parameters_filename = f\"{name_without_ext}_Parameters.txt\"\n",
    "        parameters_path = os.path.join(data_dir, parameters_filename)\n",
    "\n",
    "        # Read parameters file\n",
    "        try:\n",
    "            parameters_df = pd.read_csv(parameters_path) #Import the parameter file\n",
    "            if LogNoise:\n",
    "                log_message(f\"    load_experiments: Reading parameters file: {parameters_filename}\") #Clutter logging\n",
    "\n",
    "            #Get the first row as static data\n",
    "            static_row = parameters_df.iloc[0][static_columns] #Get the parameter numerical values\n",
    "            all_static_df.append(static_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            log_message(f\"    ****load_experiments: Failed to read parameters file: {parameters_filename}, error: {e}\")\n",
    "            continue\n",
    "\n",
    "    log_message(f\"    load_experiments: Create combined DataFrame for static parameters...\")\n",
    "    static_df = pd.DataFrame(all_static_df) #Combine all static rows into a Dataframe\n",
    "\n",
    "    log_message(f\"    load_experiments: Collected static data:\")\n",
    "    \"\"\"\n",
    "    The type of cell did not affect greatly the predictions. However in the future, it may be useful to give more information to the Cell_IDs.\n",
    "    As of 2025, these strings are Hot Encoded which means that the code finds all the different types, creates columns for each type and writes\n",
    "    0 or 1 (bool) depending of whether the cell was from one type or the other. This is the standard procedure to feed categorical varaibles to ML.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_message(f\"    load_experiments: Static dataframe columns:, {static_df.columns.tolist()}\")\n",
    "    log_message(f\"    load_experiments: Static dataframe shape:, {static_df.shape}\")\n",
    "\n",
    "    log_message(f\"    load_experiments: Hot encoding CellID.\")\n",
    "    categorical_cols = ['CellID']\n",
    "    static_df = pd.get_dummies(static_df, columns=categorical_cols, prefix=['CellID'])\n",
    "    # Now second pass: read arrays and create encoded_experiments with encoded static params\n",
    "    for i, arrays_path in enumerate(arrays_files):\n",
    "        base = os.path.basename(arrays_path)\n",
    "        name_without_ext = os.path.splitext(base)[0]\n",
    "        parameters_filename = f\"{name_without_ext}_Parameters.txt\"\n",
    "        parameters_path = os.path.join(data_dir, parameters_filename)\n",
    "        # log_message(f\"Reading arrays file: {base}\") #Clutter log\n",
    "        arrays_df = pd.read_csv(arrays_path) # Reads the time series data \n",
    "    \n",
    "        static_values = static_df.iloc[i].to_list() #Fetches the static parameters corresponding to this experiment\n",
    "    \n",
    "        Deltatime = arrays_df[\"DeltaTime\"].values\n",
    "        polarization = arrays_df[polarization_column].values #Extracts the time array and the selected polarization column.\n",
    "        #Save the uncertainty even if it is not used afterwards\n",
    "        Uncertainty = arrays_df[\"ErrPolarizationD3\"].values\n",
    "        #if len(Deltatime) > 2:\n",
    "        encoded_experiments.append((static_values, Deltatime, polarization, Uncertainty))\n",
    "        # log_message(f\"Creating Encoded Experiments (appending parameters, time array, polarization array and uncertainty array)\") #Clutter log\n",
    "    log_message(f\"    load_experiments: Loaded {len(encoded_experiments)} experiments.\")\n",
    "    return encoded_experiments, static_df.columns.tolist()\n",
    "\n",
    "\n",
    "#######################################################################################3\n",
    "\n",
    "def build_dataset(experiments, mode=\"PolarizationD3\"):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        experiments (list): It is a list like the output of load_experiments or augment_experiments\n",
    "            experiments = [\n",
    "                (static_values,   # list of static parameters (per experiment)\n",
    "                Deltatime,       # 1D numpy array of time values\n",
    "                polarization,    # 1D numpy array of polarization values\n",
    "                Uncertainty      # 1D numpy array of uncertainty values), ...]\n",
    "        mode (str): The column of polarization that will be used. It can be either 'SoftPolarizationD3' or 'PolarizationD3'.\n",
    "    \n",
    "    Output:\n",
    "        1. Xs. An array of lists. Shape (number_of_samples, number_static_features) Each list contains all the original static parameters (hot encoded) plus the first and last polarization values with uncertainty.\n",
    "           To be precise, they get added in this order: static parameters + initial time + initial polarization + final time + final polarization.\n",
    "           For all samples, the static features get added to this list. This means that, if an experiment has M measurements, two get added to the parameter\n",
    "           list and then those parameter features get written M-2 times in this array\n",
    "        2. Xt. An array of time. Shape (number_of_samples, 1). All time values that are not used as parameters get added to this array. However they are not\n",
    "           saved directly. If they are for example 0,120,250 then they get saved as [0],[120],[250]. The reason is compatibility with Keras/TensorFlow.\n",
    "        3. y. An array of polarization. It is the same as Xt but with polarization values (the type of polarization is determined by _mode_)\n",
    "        4. err. An array of polarization uncertainties. It is the same as Xt and y but with polarization uncertainties.\n",
    "    Notes:\n",
    "        If there are only two rows then the file gets skipped. It shouldn't happen but there is logic for it. The reason why it gets skipped is because\n",
    "        there would not be any values left to use for training or validation    \n",
    "    \"\"\"\n",
    "\n",
    "    Xs, Xt, y, u = [], [], [], []\n",
    "    log_message(f\"    build_dataset: Starting build_dataset for column {mode} \")\n",
    "    log_message(f\"    build_dataset: Number of experiments to process: {len(experiments)} (should be (num_augmentations+1)*(number_of_experiments-1)\")\n",
    "    \n",
    "    for exp_idx, (static_params, delta_time, polarization, uncertainty) in enumerate(experiments):\n",
    "        if len(delta_time) < 2:\n",
    "            log_message(f\"    ****build_dataset:       Skipping experiment {exp_idx}: too few data points (len={len(delta_time)})\")\n",
    "            continue\n",
    "            \n",
    "\n",
    "        \n",
    "        if LogNoise:\n",
    "            log_message(f\"    build_dataset: Adding First and Last Polarization (with time) values as static parameters\") #Clutter log\n",
    "        init_idx = 0\n",
    "        final_idx = -1\n",
    "        initial_dt, initial_p = delta_time[init_idx], polarization[init_idx]\n",
    "        final_dt, final_p = delta_time[final_idx], polarization[final_idx]\n",
    "\n",
    "        static_vector = static_params + [\n",
    "            initial_dt, initial_p,\n",
    "            final_dt, final_p\n",
    "        ]\n",
    "        if LogNoise:\n",
    "            log_message(f\"    build_dataset: Experiment {exp_idx}: static_vector length={len(static_vector)} (should be 10 (three parameters, CellID hot encoded creates three posibilities, four for the initial and final polarization) \") #Clutter log\n",
    "        if LogNoise:\n",
    "            log_message(f\"    build_dataset: Building samples Static+time+polarization\") #Clutter log\n",
    "        \n",
    "        for t, p, err in zip(delta_time[1:-1], polarization[1:-1], uncertainty[1:-1]): \n",
    "            Xs.append(static_vector)\n",
    "            Xt.append([t])\n",
    "            y.append(p)\n",
    "            u.append(err) #We will ignore always uncertainty in parameters and even if they are not used, we will keep uncertainties in the data sets(same dimensions everywhere)\n",
    "\n",
    "    log_message(f\"    build_dataset: Number of experiments processed for mode '{mode}': {len(experiments)}\")\n",
    "    log_message(f\"    build_dataset: Final dataset shapes: Xs: {np.array(Xs).shape}, Xt: {np.array(Xt).shape}, y: {np.array(y).reshape(-1, 1).shape}\")\n",
    "    return np.array(Xs), np.array(Xt), np.array(y).reshape(-1, 1), np.array(u).reshape(-1, 1)\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def split_experiments(experiments, val_fraction=0.2, seed=42):\n",
    "    # Tests of validation. Currently unused\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_exp = len(experiments)\n",
    "    indices = rng.permutation(n_exp)\n",
    "\n",
    "    n_val = int(val_fraction * n_exp)\n",
    "    val_idx = indices[:n_val]\n",
    "    train_idx = indices[n_val:]\n",
    "\n",
    "    train_experiments = [experiments[i] for i in train_idx]\n",
    "    val_experiments   = [experiments[i] for i in val_idx]\n",
    "\n",
    "    log_message(f\"Split experiments: {len(train_experiments)} train / {len(val_experiments)} val\")\n",
    "    return train_experiments, val_experiments\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def nll_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        y_true (array): An array of the real values.\n",
    "        y_pred (array): An array of the predicted values\n",
    "    Output:\n",
    "     A scalar tensorflow tensor ( () ) with the value of the loss as the the mean Gaussian negative log-likelihood over the batch\n",
    "    Notes:\n",
    "        This loss function is not just a pure Negative Loss Likelyhood (NLL).\n",
    "        ML algorythms require a way to tell the algorythm if it is learning or not. The most standard practice is with a Loss function.\n",
    "        If the loss value goes down that means that the algorythm is learning and, if a step increases the loss, then it is punished and\n",
    "        tries other approaches. When using uncertainties when teaching the model, the most common loss function is the NLL or Negative\n",
    "        log-likelihood of a normal distribution \n",
    "        \n",
    "        NLL$=\\frac{1}{2}$log$(σ^2)+\\frac{(y−μ)^2}{2σ^2}$\n",
    "        \n",
    "        where $\\sigma$ is the uncertainty in the predictions, $y$ is the measured value and $\\mu$ the predicted value.\n",
    "        Instead of predicting $σ^2$ directly, we obtain its logarithm to have a more stable process (and avoid accounting precision as\n",
    "        $\\sigma^{-2}$ which is numerically unstable when uncertainties are low).\n",
    "\n",
    "        However we want to avoid uncertainties that drift too far from the overall model predictions. To achieve that, we can get a rough\n",
    "        estimate on what the uncertainty of a set predictions looks like. Let $\\vec{\\mu}=\\left(\\mu_1,\\ldots,\\mu_N\\right)^T$ be the vector\n",
    "        of $N$ predicted values. It can be considered as a random vector of variance:\n",
    "        \n",
    "        $Var(\\vec{\\mu})=\\frac{1}{N}\\sum_{i=1}^N\\left(\\mu_i-E(\\vec{\\mu})\\right)$\n",
    "        \n",
    "        where $E(\\vec{\\mu})$ is the mean of the predicted values. We then have two different variances, one obtained as the sparseness of\n",
    "        the predictions, (denoted as $Var\\left(\\vec{\\mu}\\right)$, and one obtained as a result of the internal ML calculations (denoted as\n",
    "        $\\sigma^2$). A penalty can be added to the loss functions to force the model to try to reduce this differences. An easy way to model\n",
    "        it is to obtain the difference of those variances and square the result (taking the absolute value was also a good estimate, but\n",
    "        using squared values punished big discrepancies in a stronger way).\n",
    "\n",
    "        $StrayPenalty = B \\cdot \\left[\\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}\\left(\\vec{\\mu}\\right)\\right)\\right]^2$\n",
    "        \n",
    "        where $B$ is a constant used to control the weight of this penalty. The reason why $Var\\left(\\vec{\\mu}\\right)$ was used and not\n",
    "        $Var\\left(\\vec{y}\\right)$ (with $\\vec{y}$ the vector of measured values) was to avoid noise in the original data to tamper with the\n",
    "        loss function. It would be physically clearer to use measured values sparseness as a way to guide the model but some experimental\n",
    "        uncertainties are clearly underestimated and that would cause this penalty to dominate the loss and obscure the main loss protocol, the NLL.\n",
    "\n",
    "        Also, we also want to punish the model if it tries overestimating $\\sigma$. If the model is unable to minimize $y-\\mu$, in order to\n",
    "        lower NLL, it increases $\\sigma$. If no precautions are taken, this \"escape solution\" achieves bad predictions with inflated\n",
    "        uncertainties that simulate a low loss value. A new penalty was added that punishes overestimation of the uncertainties more than\n",
    "        underestimation (which never happened). The slope correction done further on the pipeline can \"fix\" this issue but what the model\n",
    "        returns then is closer to a poorly calculated linear fit. Therefore, an addition penalty was added.\n",
    "\n",
    "        $OverestimatePenalty= C \\cdot \\max\\left(0, \\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}(\\vec{\\mu})\\right)\\right)$\n",
    "        \n",
    "        where $C$ is a constant used to control the weight of this penalty\n",
    "    \"\"\"\n",
    "\n",
    "    mu = y_pred[:, 0:1]\n",
    "    log_var = y_pred[:, 1:2]\n",
    "\n",
    "    # Base Gaussian NLL\n",
    "    precision = tf.exp(-log_var)\n",
    "    nll = tf.reduce_mean(0.5 * (log_var + tf.square(y_true - mu) * precision))\n",
    "\n",
    "\n",
    "    mu_centered = mu - tf.reduce_mean(mu)\n",
    "    sigma_ref = tf.sqrt(tf.reduce_mean(tf.square(mu_centered)) + 1e-6)\n",
    "    Stray_penalizer = 1e-2 #Parameter used to limit how far the uncertainty predictions stray from the experimental data\n",
    "    OverUncert_penalizer  = 5e-3 #Parameter used for punishing overestimated uncertainties\n",
    "    log_var_prior = tf.math.log(sigma_ref ** 2) #Order of magnitude of what uncertainties should look like\n",
    "\n",
    "    # Prior penalty. It penalizes if uncertainty strays too much from the experimental value\n",
    "    #It avoids underestimation of uncertainty and overestimation\n",
    "    penalty_stray = Stray_penalizer * tf.reduce_mean(tf.square(log_var - log_var_prior))\n",
    "\n",
    "    # Asymmetric penalty: punish overestimation more than underestimation\n",
    "    penalty_overestimate = OverUncert_penalizer * tf.reduce_mean(tf.nn.relu(log_var - log_var_prior))\n",
    "\n",
    "    return nll + penalty_stray + penalty_overestimate  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462655ab-5de9-473d-9a27-5a2fc778f070",
   "metadata": {},
   "source": [
    "## 5. Model specific functions\n",
    "\n",
    "Here we have functions that train, validate and fit the models. Some models require the variables to be scaled or will scale them. Extra precautions need to be taken into account\n",
    "\n",
    "1. **model\\_fitting**. It is a function that logs and runs model.fit() on a two-input Keras model and returns the training history. It needs **static, time and polarization variables scaled**. Training is not done using the uncertainties of the data as it was decided that uncertainty information is encoded in the augmentations. Note: No validation is done anywhere in the code. Here are some of the reasons:\n",
    "    \n",
    "    1.1. The data base is very small. The amorphous data base contains only 199 points while the crystalline one contains 251. Removing a small percentage of those points for validation might leave the data base too small and underfitting might worsen the result more than fine tuning parametrs with validation.\n",
    "    \n",
    "    1.2. A randomized validation split may be physically wrong. Therefore it should be chronological, not shuffled. However, in crystalline experiments, there are decay experiments that have only four or five intermediate points. Even removing one point for validation is a massive hit on the experiment. Therefore, it is risky to add validation\n",
    "    \n",
    "    1.3. To find good models, a Leave-one-out approach was used. For a certain model structure, an experiment gets removed and the model and it trains on all the remaining experiments. Then, the model tries to predict this isolated experiment. Afterwards, the experiment is returned and a new one becomes isolated. This process loops for all experiments and an overall score of the model is computed. This process was done for 498 models for crystalline materials. This is a stronger (and more expensive) method than validation as it is not dependent on the validation splits and avoids possible information leaks.\n",
    "\n",
    "Also, eight randomly picked models were tested with and without validation and with and without an asymetric uncertainty-overestimated penalizing loss. The result showed that the Loss update was an improvement and validation did not increase performance (without validation, the results were slightly better).\n",
    "\n",
    "2. **model\\_prediction**. It is a funtion that predicts with a given model. It needs **static and time variables scaled**. This scaling must be coherent to the one done in the rest of the funtions.\n",
    "\n",
    "3. **train**. This function is the one responsible of scaling the inputs and training the model (it uses **model\\_fitting**)\n",
    "\n",
    "     3.1. It creates the independent arrays with all the encoded experiments (augmented or not) using build_dataset\n",
    "    \n",
    "     3.2. Then it scales the data. ML algorithms work better when the inputs and outputs are normalized. The reason why we don't normalize inside the function is to have those scaler defined globally and not locally\n",
    "    \n",
    "     3.3. It builds the model depending on the use\\_uncertainty bool. (It changes the loss function and the output).\n",
    "    \n",
    "     3.4. It trains the model and returns its history (the trained model)\n",
    "    \n",
    "4. **align_static_vectors**. It converts the columns not present on an isolated experiment to zeros.\n",
    "    \n",
    "5. **model_predict_sloped** It substracts a linear function to the predicted values. If done correctly this makes it so that the polarization predictions at the initial and final time points are the same as the measured polarization values at those times. This fixes a vertical shift and also an overall slope. As it is a correction done with experimental values, the algorithm is still \"universal\". However we can't fully say that it is a pure ML algorithm. The \"correctness\" of this method is subjective. It is a warning in the ML front that there is an issue with the data base but it is a valid fix for experimentalists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec9900-7af7-48f7-aaa7-f421d85fc147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fitting(model, X_static_scaled, X_time_scaled, y_scaled, epochs, batch_size, verbose, callbacks=None):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        model (keras model): This is the object that the build_model function returns.\n",
    "        X_static_scaled (array): It is an array os size (total number of intermediate measured points (let's call them 'samples'), number of static features). \n",
    "                                 Using the variables form build_dataset, the number of static features is 4+len(static_parameters(hot encoded)).\n",
    "                                 To be precise, it is just Xs scaled (an array of lists with the static features) but reshpaed into a 2d array\n",
    "        X_time_scaled (array): A 2d array of shape that reshapes Xt scaled from an array of lists (of size 1 like np.array([1],[13],[16],...)) to a 2D array\n",
    "        y_scaled (array): The same as X_time_scaled but with polarization values\n",
    "        epochs (int): Number of training epochs (times the model tries to validate the data and recalculates its parameters)\n",
    "        batch_size (int): The number of samples for every gradient update\n",
    "        verbose (int): It limits how much training output is printed\n",
    "        callbacks (str): It allows certain Keras callbacks (special code properties of keras). EarlyStopping, ReduceLROnPlateau or ModelCheckpoint are examples\n",
    "    Outputs:\n",
    "       A keras.callbacks.History object\n",
    "    Note:\n",
    "        It logs and runs model.fit() on a two-input Keras model and returns the training history.\n",
    "        IT REQUIRES THE INPUTS (static, time and polarization) TO BE NORMALIZED/SCALED\n",
    "        Training is not done using the uncertainties of the data.  \n",
    "    \"\"\"\n",
    "    log_message(f\"    Model_fitting: Training the model.\")\n",
    "    return model.fit(\n",
    "        [X_static_scaled, X_time_scaled],\n",
    "        y_scaled,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "\n",
    "def model_prediction(model, X_static_scaled, X_time_scaled):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X_static_scaled (array): A 2d array of Xs (samples, number of static features)\n",
    "        X_time_scaled (array): A 2d array of shape that reshapes Xt scaled from an array of lists (of size 1 like np.array([1],[13],[16],...)) to a 2D array\n",
    "        y_scaled (array): The same as X_time_scaled but with polarization values. It doesn't have to be the same as the one used in training\n",
    "    Output:\n",
    "        A 2d array with the predictions for those time values. It contains a column of the polarization predictions and another with the log of the variance\n",
    "    Notes:\n",
    "        IT REQUIRES THE INPUTS (static and time) TO BE NORMALIZED/SCALED\n",
    "    \"\"\"\n",
    "    log_message(f\"    model_prediction: Predicting {len(X_time_scaled)} time points\")\n",
    "    return model.predict([X_static_scaled, X_time_scaled], verbose=0)\n",
    "\n",
    "def train(encoded_experiments, scaler_static, scaler_time, scaler_y, use_uncertainty):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        encoded_experiments (list): A list like this:\n",
    "            encoded_experiments = [\n",
    "                (static_values,   # list of static parameters (per experiment)\n",
    "                Deltatime,       # 1D numpy array of time values\n",
    "                polarization,    # 1D numpy array of polarization values\n",
    "                Uncertainty      # 1D numpy array of uncertainty values), ...] \n",
    "        scaler_static: It is a scikit-learn scaler for the static features. Only MinMaxScaler (normalizes uniformly using the maximum and the minimum)\n",
    "        scaler_time: Analogously to scaler_static\n",
    "        scaler_y: Analogously to scaler_static\n",
    "        use_uncertainty (bool): It toggles whether it uses uncertainty for predictions or not. This changes the output of the model (mean and log_var or just the mean)\n",
    "    Output:\n",
    "        A fully trained model that can be used by model_predictions\n",
    "    Notes:\n",
    "        This function is the one responsible of scaling the inputs and training the model\n",
    "        1. It creates the independent arrays with all the encoded experiments (augmented or not) using build_dataset\n",
    "        2. Then it scales the data. ML algorithms work better when the inputs and outputs are normalized.\n",
    "           The reason why we don't normalize inside the function is to have those scaler defined globally and not locally\n",
    "        3. It builds the model depending on the use_uncertainty bool. (It changes the loss function and the output).\n",
    "        4. It trains the model and returns its history (the trained model)\n",
    "    \n",
    "    \"\"\"\n",
    "    log_message(f\"    train: Begin training and scaling with {len(encoded_experiments)} experiments.\")\n",
    "    X_static_all, X_time_all, y_all, u_all = build_dataset(encoded_experiments)\n",
    "    log_message(f\"Scaling all data\")\n",
    "    X_static_scaled = scaler_static.transform(X_static_all) #It only fits to the scaler. IT DOESN'T OVERWRITE THEM. WHAT A WASTE OF 20 DAYS OF MY LIFE >:(\n",
    "    X_time_scaled = scaler_time.transform(X_time_all)\n",
    "    y_scaled = scaler_y.transform(y_all)\n",
    "    \n",
    "    model = build_model(X_static_all.shape[1], use_uncertainty=use_uncertainty)\n",
    "    epochs = 300\n",
    "    batch_size = 32\n",
    "    log_message(f\"    train: Training final model with epochs={epochs}, batch_size={batch_size} and use_uncertainty = {use_uncertainty}\")\n",
    "    model_fitting(model, X_static_scaled, X_time_scaled, y_scaled, epochs, batch_size, verbose=0)\n",
    "    return model\n",
    "\n",
    "############################################################\n",
    "\n",
    "def align_static_vectors(experiments, static_columns_training, static_columns_isolated):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        experiments (list): A list like this:\n",
    "            experiments = [\n",
    "                (static_values,   # list of static parameters (per experiment)\n",
    "                Deltatime,       # 1D numpy array of time values\n",
    "                polarization,    # 1D numpy array of polarization values\n",
    "                Uncertainty      # 1D numpy array of uncertainty values), ...] \n",
    "        static_columns_training (list): A list of strings of all the static feature names used for training\n",
    "        static_columns_isolated (list): A list of strings of all the static feature names of the isolated experiment\n",
    "    Output:\n",
    "        aligned_experiments is a list of experiments with static vectors aligned to the training feature order.\n",
    "        To be precise (aligned_static, delta_time, polarization, uncertainty) where on the parameters where static_columns_isolated\n",
    "        had no values get turned into zeros. So, if static_columns_isolated doesn´t have a parameter 'Parameter 6', then, in the\n",
    "        original experiments lists, all numbers associated to 'Parameter 6' get turned to zero\n",
    "\n",
    "    Notes:\n",
    "        It helps the ML algorithm to focus on the important parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    aligned_experiments = []\n",
    "    for static, delta_time, polarization, uncertainty in experiments:\n",
    "        static_dict = dict(zip(static_columns_isolated, static))\n",
    "        aligned_static = [static_dict.get(col, 0.0) for col in static_columns_training]\n",
    "        aligned_experiments.append((aligned_static, delta_time, polarization, uncertainty))\n",
    "    log_message(f\"    align_static_vectors: Vectors aligned\")\n",
    "    return aligned_experiments\n",
    "\n",
    "def model_predict_sloped(model, X_static_scaled, X_time_scaled, m, n, scaler_y, scaler_time):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: A trained model\n",
    "        X_static_scaled (array). A 2d array of Xs (samples, number of static features)\n",
    "        X_time_scaled (array). A 2d array of shape that reshapes Xt scaled from an array of lists (of size 1 like np.array([1],[13],[16],...)) to a 2D array\n",
    "        m (float): The slope used for correction\n",
    "        n (float): The polarization shift for correction\n",
    "        scaler_time: It is a scikit-learn scaler for the time arrays. Only MinMaxScaler (normalizes uniformly using the maximum and the minimum)\n",
    "        scaler_y: Analogously to scaler_static\n",
    "    Outputs\n",
    "        y_pred_corrected_scaled is a 2D array where each column is the corrected predicted values (scaled) and the second one is the log of the variance (unchanged, a.k.a scaled)\n",
    "    Notes:\n",
    "        1. It predicts the polarization values\n",
    "        2. It inverse transforms the predicted values and the time points (not the uncertainty)\n",
    "        3. It calculates the correction curve in real units and corrects the predicted values.\n",
    "        4. Finally, it scales back the corrected polarization values and joins them with the unchanged logarithm of the variance\n",
    "        In summary, it substracts a linear function to the predicted values. If done correctly this makes it so that the polarization\n",
    "        predictions at the time points stored as static features are the same as the measured polarization values at those times.\n",
    "        This fixes a vertical shift and also an overall slope. As it is a correction done with experimental values, the algorithm is still\n",
    "        universal. However we can't fully say that it is a pure ML algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_message(f\"    model_predict_sloped: Correcting {len(X_time_scaled)} points by subtracting P(t) = {float(np.squeeze(m)):.3e} * t + {float(np.squeeze(n)):.3e}\")    # Step 1: Get scaled predictions from model\n",
    "    y_pred_scaled = model_prediction(model, X_static_scaled, X_time_scaled)\n",
    "    mean_scaled = y_pred_scaled[:, 0:1]  # shape (N,1)\n",
    "    mean_real = scaler_y.inverse_transform(mean_scaled)  # shape (N,1)\n",
    "    time_real = scaler_time.inverse_transform(X_time_scaled)  # shape (N,1)\n",
    "    \n",
    "    correction = m * time_real + n  # shape (N,1)\n",
    "    mean_corrected_real = mean_real - correction  # shape (N,1)\n",
    "    \n",
    "    mean_corrected_scaled = scaler_y.transform(mean_corrected_real)  # shape (N,1)\n",
    "    log_var_scaled = y_pred_scaled[:, 1:2]  # shape (N,1)\n",
    "    y_pred_corrected_scaled = np.hstack([mean_corrected_scaled, log_var_scaled])  # shape (N,2)\n",
    "    \n",
    "    return y_pred_corrected_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b48b7d5-9fa0-43d0-b644-6f949720990b",
   "metadata": {},
   "source": [
    "## 6 Predict function\n",
    "The funtion that predicts using all stored models and all experiments.\n",
    "    Outputs:\n",
    "        A folder structure with the predictions:\n",
    "        \n",
    "        - PredictCrystalline.ipynb\n",
    "        - CrystallinePredictionsFolder\n",
    "          |\n",
    "          |_Model_{NameOfModel 1}\n",
    "          |    |\n",
    "          |    |_Experiment_{NameOfExperiment 1}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |\n",
    "          |    |   |_Text file with the results. First column is the internal program time\n",
    "          |    |   |                             Second column is the time in real life of those predictions (YY-MM-DD HH:mm:SS\n",
    "          |    |   |                             Third column is the predicted polarization\n",
    "          |    |   |                             Fourth column is the uncertainty\n",
    "          |    |   |\n",
    "          |    |   |_PolarizationD3_{Name}_Paramteres.txt (original parameter file)\n",
    "          |    |   |_PolarizationD3_{Name}.txt (original data file)\n",
    "          |    |   \n",
    "          |    |_Experiment_{NameOfExperiment 2}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |\n",
    "          |    |   |_Text file with the results.  \n",
    "          |    |   |\n",
    "          |    |   |_Parameter and array original files\n",
    "          |    |\n",
    "          |    |_(...)\n",
    "          |\n",
    "          |_Model_{NameOfModel 2}\n",
    "          |    |\n",
    "          |    |_Experiment_{NameOfExperiment 1}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |_Text file with the results. First column is the internal program time\n",
    "          |    |   \n",
    "          |    |_Experiment_{NameOfExperiment 2}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |_Text file with the results.   \n",
    "          |    |   |_Parameter and array original files\n",
    "          |    |\n",
    "          |    |_(...)\n",
    "          |\n",
    "          |_Combined model\n",
    "          |    |\n",
    "          |    |_Experiment_{NameOfExperiment 1}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |_Text file with the results. First column is the internal program time\n",
    "          |    |   \n",
    "          |    |_Experiment_{NameOfExperiment 2}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |\n",
    "          |    |   |_Text file with the results.  \n",
    "          |    |   |\n",
    "          |    |   |_Parameter and array original files   \n",
    "          |    |\n",
    "          |    |_(...)\n",
    "          |_(...)\n",
    "    Notes:\n",
    "    The steps it takes are explained here:\n",
    "        1. Obtain the absolute time references for the experiments (year, month, day, hour, minute and second)\n",
    "        2. Loop over all models that will be used\n",
    "        3. Loop over all two-numerical-rowed experiments. It copies the original file to the final subfolders but doesn't move them\n",
    "        4. Build the structure that the ML model was trained with and scale all the experiment using the scalers\n",
    "           that the model used in training\n",
    "        5. Predict, correct the slope if Correction=True, plot everything and write the results on a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f6ee1-fdea-4197-a840-ba8d7019d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(predict_dir, model_dir, output_folder, time_dir, use_uncertainty, Correction, t_initial, t_final, t_step):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        predict_dir (str): String or path (Linux&Mac or Windows respectively) that points to the folder where the experiments that will be predicted are\n",
    "        model_dir (str): String or path (Linux&Mac or Windows respectively) that points to the folder where the trained models are\n",
    "        output_folder (str): String or path (Linux&Mac or Windows respectively) that points to the folder where the predicted experiments will be \n",
    "        time_dir (str): String or path (Linux&Mac or Windows respectively) that points to the file with the absolute time references\n",
    "        use_unceratinty (bool): A bool that toggles whether uncertainty is used or not (not sure if it works on False but data without uncertainty is worthless)\n",
    "        Correction (bool): A bool that toggles whether the initial and final point force gets done (works on True and False)\n",
    "        t_initial (float): A float that controls the time point where predictions begin. For example, -1000 makes the predictions to start 1000 seconds before the first polarization measurement\n",
    "        t_final (float): A float that controls the time point where predictions stop. For example, +1000 makes the predictions to stop 1000 seconds after the last polarization measurement\n",
    "        t_step (float): A float that controls the time intervals between predictions (if it is 5, then there are predictions every 5 seconds)\n",
    "    Outputs:\n",
    "        A folder structure with the predictions:\n",
    "        \n",
    "        - PredictCrystalline.ipynb\n",
    "        - CrystallinePredictionsFolder\n",
    "          |\n",
    "          |_Model_{NameOfModel 1}\n",
    "          |    |\n",
    "          |    |_Experiment_{NameOfExperiment 1}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |\n",
    "          |    |   |_Text file with the results. First column is the internal program time\n",
    "          |    |   |                             Second column is the time in real life of those predictions (YY-MM-DD HH:mm:SS\n",
    "          |    |   |                             Third column is the predicted polarization\n",
    "          |    |   |                             Fourth column is the uncertainty\n",
    "          |    |   |\n",
    "          |    |   |_PolarizationD3_{Name}_Paramteres.txt (original parameter file)\n",
    "          |    |   |_PolarizationD3_{Name}.txt (original data file)\n",
    "          |    |   \n",
    "          |    |_Experiment_{NameOfExperiment 2}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |\n",
    "          |    |   |_Text file with the results.  \n",
    "          |    |   |\n",
    "          |    |   |_Parameter and array original files\n",
    "          |    |\n",
    "          |    |_(...)\n",
    "          |\n",
    "          |_Model_{NameOfModel 2}\n",
    "          |    |\n",
    "          |    |_Experiment_{NameOfExperiment 1}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |_Text file with the results. First column is the internal program time\n",
    "          |    |   \n",
    "          |    |_Experiment_{NameOfExperiment 2}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |_Text file with the results.   \n",
    "          |    |   |_Parameter and array original files\n",
    "          |    |\n",
    "          |    |_(...)\n",
    "          |\n",
    "          |_Combined model\n",
    "          |    |\n",
    "          |    |_Experiment_{NameOfExperiment 1}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |_Text file with the results. First column is the internal program time\n",
    "          |    |   \n",
    "          |    |_Experiment_{NameOfExperiment 2}\n",
    "          |    |   |\n",
    "          |    |   |_Graph with predictions and original data (.png)\n",
    "          |    |   |\n",
    "          |    |   |_Text file with the results.  \n",
    "          |    |   |\n",
    "          |    |   |_Parameter and array original files   \n",
    "          |    |\n",
    "          |    |_(...)\n",
    "          |_(...)\n",
    "    Notes:\n",
    "    The steps it takes are explained here:\n",
    "        1. Obtain the absolute time references for the experiments (year, month, day, hour, minute and second)\n",
    "        2. Loop over all models that will be used\n",
    "        3. Loop over all two-numerical-rowed experiments. It copies them (does't move them) to the final folders\n",
    "        4. Build the structure that the ML model was trained with and scale all the experiment using the scalers\n",
    "           that the model used in training\n",
    "        5. Predict, correct the slope if Correction=True, plot everything and write the results on a .txt file\n",
    "    \"\"\"\n",
    "    #Second run issues. Not important but needed\n",
    "    def safe_copy(src: Path, dst: Path):\n",
    "        src = src.resolve()\n",
    "        dst = dst.resolve()\n",
    "        if src == dst:\n",
    "            return  # silently skip\n",
    "        shutil.copy2(long_path(src), long_path(dst))\n",
    "\n",
    "    model_path = Path(model_dir)\n",
    "    predict_path = Path(predict_dir)\n",
    "    output_folder_path = Path(output_folder)\n",
    "    time_path = Path(time_dir)\n",
    "    mode = \"PolarizationD3\"\n",
    "    log_file = Path.cwd() / \"CrystallineExecution_times.txt\"\n",
    "    log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(log_file)\n",
    "    # 1. Read all polarization time references\n",
    "    log_message(f\"Predict: Obtain time references for all experiments\")\n",
    "    time_path_str = long_path(time_path) if os.name == \"nt\" else time_path \n",
    "    with open(time_path_str, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_times = [datetime.strptime(line.strip(), \"%Y-%m-%d %H:%M:%S\") for line in f]\n",
    "\n",
    "    # 2. Loop over all subdirectories inside model_dir (a.k.a obtain all models that will be used)\n",
    "    for modelsubfolder in model_path.iterdir():\n",
    "        log_message(f\"Predict: Reading model {modelsubfolder}\")\n",
    "        if modelsubfolder.is_dir():\n",
    "            try:\n",
    "                folder_str = long_path(modelsubfolder)\n",
    "                # 3. Find model files\n",
    "                model_files = glob.glob(str(folder_str + \"/model_*.keras\"))\n",
    "                scaler_static_files = glob.glob(str(folder_str + \"/scaler_static_*.pkl\"))\n",
    "                scaler_time_files   = glob.glob(str(folder_str + \"/scaler_time_*.pkl\"))\n",
    "                scaler_y_files      = glob.glob(str(folder_str + \"/scaler_y_*.pkl\"))\n",
    "                model_file_path       = Path(model_files[0])\n",
    "                scaler_static_path    = Path(scaler_static_files[0])\n",
    "                scaler_time_path      = Path(scaler_time_files[0])\n",
    "                scaler_y_path         = Path(scaler_y_files[0])\n",
    "                \n",
    "                model_file_path = Path(model_files[0])  # take the first matching model as a structure reference\n",
    "                model = tf.keras.models.load_model(long_path(model_file_path), custom_objects={\"nll_loss\": nll_loss})\n",
    "                scaler_static = joblib.load(long_path(scaler_static_path))\n",
    "                scaler_time   = joblib.load(long_path(scaler_time_path))\n",
    "                scaler_y      = joblib.load(long_path(scaler_y_path))\n",
    "                log_message(f\"\\nPredict: Loaded model and scalers from {modelsubfolder}\\n\")\n",
    "\n",
    "                #4. Get all *_Parameters.txt files to derive base names\n",
    "                param_files = glob.glob(str(long_path(predict_path) + \"/*_Parameters.txt\"))\n",
    "                param_files = [Path(f) for f in param_files] \n",
    "                base_names = [f.stem.replace(\"_Parameters\", \"\") for f in param_files]\n",
    "                output_midfolder_path = Path(output_folder) / modelsubfolder.name\n",
    "                output_midfolder_path.mkdir(parents=True, exist_ok=True)  \n",
    "                log_message(f\"Predict: Created {output_midfolder_path}\")\n",
    "                output_midfolder_dir = long_path(output_midfolder_path)\n",
    "                \n",
    "                #5. Loop over all experiments \n",
    "                for i, base_name in enumerate(base_names):\n",
    "                    start_time = pytime.time()    \n",
    "\n",
    "                    file_param_path = Path(predict_dir) / f\"{base_name}_Parameters.txt\"\n",
    "                    file_data_path  = Path(predict_dir) / f\"{base_name}.txt\"\n",
    "                    PolarizationAbsoluteTime = all_times[i]  # pick the correct row of absolute times\n",
    "                    log_message(f\"Predict: [Experiment {i}] Polarization Reference = {PolarizationAbsoluteTime}\")\n",
    "\n",
    "                    parts = base_name.split(\"_\")\n",
    "                    short_name = \"_\".join(parts[1:6])  # take elements 1 through 5 only\n",
    "                    output_subfolder_path = Path(output_midfolder_path) / f\"Experiment_{short_name}\"\n",
    "                    Path(long_path(output_subfolder_path)).mkdir(parents=True, exist_ok=True)\n",
    "                    if output_subfolder_path.exists():\n",
    "                        for f in output_subfolder_path.glob(\"*\"):\n",
    "                            f.unlink()\n",
    "                    else:\n",
    "                        output_subfolder_path.mkdir(parents=True)\n",
    "                    # 6 Move files to the experiment-specific folder. With this isolation, prediction begins\n",
    "                    safe_copy(file_param_path, output_subfolder_path / file_param_path.name)\n",
    "                    safe_copy(file_data_path,  output_subfolder_path / file_data_path.name)\n",
    "\n",
    "\n",
    "                    log_message(f\"Predict: Moved {base_name} files into {output_subfolder_path}\")                \n",
    "                    #7 Load isolated experiment. Predictions will be done on this experiment. Aligning is required\n",
    "                    isolated_experiments, static_columns_isolated = load_experiments(output_subfolder_path, polarization_column=mode)\n",
    "                    isolated_experiments_aligned = align_static_vectors(\n",
    "                        isolated_experiments,\n",
    "                        static_columns_training = ['Pressure', 'LabPolarization', 'LabTime', 'CellID_ge18004', 'CellID_ge18006', 'CellID_ge18012'],\n",
    "                        static_columns_isolated=static_columns_isolated\n",
    "                    ) #unscaled\n",
    "                    if LogNoise:\n",
    "                        log_message(f\"      Number of isolated experiments aligned (should be one): {len(isolated_experiments_aligned)}\")\n",
    "                        log_message(f\"      Example static vector (pre-scale): {isolated_experiments_aligned[0][0]}\")\n",
    "                        log_message(f\"      Example time vector (pre-scale): {isolated_experiments_aligned[0][1]}\")\n",
    "                        log_message(f\"      Example polarization vector (pre-scale): {isolated_experiments_aligned[0][2]}\")\n",
    "\n",
    "\n",
    "\n",
    "                    #8. Building manually the dataset for the isolated experiment\n",
    "                    for static, timeVec, pol, unc in isolated_experiments_aligned: #all unscaled\n",
    "                        log_message(f\"{len(timeVec)}\")\n",
    "                        if len(timeVec) < 2:\n",
    "                            log_message(f\"**** Predict: Skipping experiment due to insufficient time points\")\n",
    "                            continue  # skip too-short experiments\n",
    "\n",
    "                        #8.1 Extract the initial and final points, add them as parameters and fit the entire static vector\n",
    "                        initial_dt, initial_p = timeVec[0], pol[0]\n",
    "                        final_dt, final_p = timeVec[-1], pol[-1]\n",
    "                        static_vector = static + [initial_dt, initial_p, final_dt, final_p] #unscaled\n",
    "                        if LogNoise:\n",
    "                            log_message(f\"  Experiment static vector length (pre-scale): {len(static_vector)}\")\n",
    "                            log_message(f\"  Experiment static vector (pre-scale): {static_vector}\")\n",
    "                        static_scaled = scaler_static.transform(np.array(static_vector).reshape(1, -1)).flatten() #scaled\n",
    "                        if LogNoise:\n",
    "                            log_message(f\"  Experiment static vector (scaled): {static_scaled}\")\n",
    "\n",
    "                        #8.2 Scale time and polarization arrays (only intermediate points)\n",
    "                        time_intermediate = np.array(timeVec).reshape(-1, 1) # unscaled\n",
    "                        time_scaled = scaler_time.transform(time_intermediate).flatten() #scaled\n",
    "                        if LogNoise:\n",
    "                            log_message(f\"  Experiment time (pre-scale): {time_intermediate.flatten()}\")\n",
    "                            log_message(f\"  Experiment time (scaled): {time_scaled}\")\n",
    "                        pol_intermediate = np.array(pol).reshape(-1, 1) #unscaled\n",
    "                        pol_scaled = scaler_y.transform(pol_intermediate).flatten() #scaled\n",
    "                        if LogNoise:\n",
    "                            log_message(f\"  Experiment polarization (pre-scale): {pol_intermediate.flatten()}\")\n",
    "                            log_message(f\"  Experiment polarization (scaled): {pol_scaled}\")\n",
    "                        unc_intermediate = np.array(unc).reshape(-1, 1) #unscaled\n",
    "                        if LogNoise:\n",
    "                            log_message(f\"  Experiment uncertainty: {unc_intermediate.flatten()}\")\n",
    "\n",
    "                        #8.3 Combine all structures so that the shape is correct for prediction\n",
    "                        scaled_isolated_experiments = []\n",
    "                        scaled_isolated_experiments.append((\n",
    "                            static_scaled, time_scaled, pol_scaled, unc_intermediate.flatten()\n",
    "                        )) #scaled\n",
    "\n",
    "\n",
    "                        #8.4 Check for shapes and create the final NumPy arrays of the experiment\n",
    "                        X_static, X_time, y_true, u = [], [], [], []\n",
    "                        for j, (static_scaled, time_scaled, pol_scaled, unc_intermediate) in enumerate(scaled_isolated_experiments):\n",
    "                            if LogNoise:\n",
    "                                log_message(f\"   static_scaled.shape = {static_scaled.shape}\")\n",
    "                                log_message(f\"   time_scaled.shape = {time_scaled.shape}\")\n",
    "                                log_message(f\"   pol_scaled.shape = {pol_scaled.shape}\")\n",
    "                                log_message(f\"   unc_intermediate.shape = {unc_intermediate.shape}\")\n",
    "                            assert len(time_scaled) == len(pol_scaled) == len(unc_intermediate), \"Mismatch in lengths!\"\n",
    "\n",
    "                        for static_scaled, time_scaled, pol_scaled, unc_intermediate in scaled_isolated_experiments:\n",
    "                            for t, p, err in zip(time_scaled, pol_scaled, unc_intermediate):\n",
    "                                X_static.append(static_scaled)   # already includes appended time/polarization info\n",
    "                                X_time.append([t])               # wrap to preserve 2D structure for Conv1D\n",
    "                                y_true.append(p)\n",
    "                                u.append(err)\n",
    "                        X_static = np.array(X_static) #scaled\n",
    "                        X_time = np.array(X_time) #scaled\n",
    "                        y_true = np.array(y_true).reshape(-1, 1) #scaled\n",
    "                        u = np.array(u).reshape(-1, 1) #unscaled\n",
    "                        if LogNoise:\n",
    "                            log_message(f\"  X_static.shape: {X_static.shape}\")\n",
    "                            log_message(f\"  X_time.shape: {X_time.shape}\")\n",
    "                            log_message(f\"  y_true.shape: {y_true.shape}\")\n",
    "                            log_message(f\"  X_static[0]: {X_static[0]} (scaled)\")\n",
    "\n",
    "                        #8.5 Extract the initial and final polarizations and keep a scaled and unscaled version of all. Then predict at those time points. \n",
    "                        #Unscaling needs to be done with static_scaler but then we need to scale them as polarizations or as time points with their respective scalers so that we can use model_predict\n",
    "                        X_static_raw = scaler_static.inverse_transform(X_static)\n",
    "                        # Extract static input (already scaled)\n",
    "                        x_static_single = X_static[0:1]  # shape (1, D_static)\n",
    "                        t0_raw = X_static_raw[0, -4].reshape(1, -1)\n",
    "                        p0_raw = X_static_raw[0, -3].reshape(1, -1)\n",
    "                        tT_raw = X_static_raw[0, -2].reshape(1, -1)\n",
    "                        pT_raw = X_static_raw[0, -1].reshape(1, -1)\n",
    "\n",
    "                        t0_scaled_correct = scaler_time.transform(t0_raw)\n",
    "                        tT_scaled_correct = scaler_time.transform(tT_raw)\n",
    "                        p0_scaled_correct = scaler_y.transform(p0_raw)\n",
    "                        pT_scaled_correct = scaler_y.transform(pT_raw)\n",
    "\n",
    "                        # Predict scaled polarizations at those time points\n",
    "                        pred_initial_scaled = model_prediction(model, x_static_single, t0_scaled_correct)\n",
    "                        pred_final_scaled   = model_prediction(model, x_static_single, tT_scaled_correct)\n",
    "\n",
    "                        # Inverse transform predictions to physical (real) polarizations\n",
    "                        p0_pred = scaler_y.inverse_transform(pred_initial_scaled[:, 0:1])\n",
    "                        pT_pred = scaler_y.inverse_transform(pred_final_scaled[:, 0:1])\n",
    "\n",
    "                        if LogNoise:\n",
    "                            log_message(f\"t0 (real) = {t0_raw}, tT (real) = {tT_raw}\")\n",
    "                            log_message(f\"p0_pred (real) = {p0_pred}, pT_pred (real) = {pT_pred}\")\n",
    "                            log_message(f\"p0 (true) = {p0_raw}, pT (true) = {pT_raw}\")\n",
    "\n",
    "\n",
    "                        #8.6 Prepare the linear correction and predict in the time points where we have data\n",
    "                        m_raw = ( (p0_pred-p0_raw) - (pT_pred-pT_raw) ) / (t0_raw-tT_raw)\n",
    "                        n_raw = (p0_pred-p0_raw) - m_raw * t0_raw\n",
    "\n",
    "\n",
    "                        if Correction == True:\n",
    "                            log_message(f\"Predict:   Fixing initial and final points by substracting P(t) = {float(np.squeeze(m_raw)):.3e} * t + {float(np.squeeze(n_raw)):.3e}\")\n",
    "                            y_pred_raw = model_predict_sloped(model, X_static, X_time, m_raw, n_raw, scaler_y, scaler_time) #Input es scaled, parametros unsacles, output es scaled también\n",
    "                        else:\n",
    "                            y_pred_raw = model_prediction(model, X_static, X_time)\n",
    "\n",
    "                        # 8.6 To obtain the results we separate the value predictions form the log variance. The results need to be unscaled to real units while the uncertainty needs to be \n",
    "                        #exponenciated (to obtain a variance), multiplied by the same factor that is used in MinMaxScaler and finally convert to uncertainty (take the square root)\n",
    "                        mean_pred = scaler_y.inverse_transform(y_pred_raw[:, 0:1]).flatten()\n",
    "                        log_var_scaled = y_pred_raw[:, 1]  # Keep in original scale\n",
    "                        var_scaled = np.exp(log_var_scaled)\n",
    "                        scale_y = scaler_y.data_max_[0] - scaler_y.data_min_[0]  # scale factor from MinMaxScaler \n",
    "                        # Variance rescales by the square of the scale factor\n",
    "                        var_rescaled = var_scaled * (scale_y ** 2)\n",
    "                        pred_sigma = np.sqrt(var_rescaled)\n",
    "                        pred_polar = mean_pred\n",
    "\n",
    "                        #Plots:\n",
    "                        Grid_time_OG = np.arange(0+t_initial, float(tT_raw[0][0])+t_final, t_step).reshape(-1, 1)\n",
    "                        static_scaled = np.tile(X_static[0], (len(Grid_time_OG), 1)) #SCALED\n",
    "                        Grid_time_SC = scaler_time.transform(Grid_time_OG)\n",
    "\n",
    "                        if Correction == True:\n",
    "                            Grid_polar_Logvar_Sloped_SC = model_predict_sloped(model, static_scaled, Grid_time_SC, m_raw, n_raw, scaler_y, scaler_time) #scaled\n",
    "                            Grid_polar_Sloped_OG = scaler_y.inverse_transform(Grid_polar_Logvar_Sloped_SC[:, 0:1]) #unscaled\n",
    "                            Grid_Logvar_Sloped_SC = Grid_polar_Logvar_Sloped_SC[:, 1:2]  # Don't transform this\n",
    "                            Grid_Var_Sloped_SC = np.exp(Grid_Logvar_Sloped_SC)\n",
    "                            # Variance rescales by the square of the scale factor\n",
    "                            Grid_Var_Sloped_OG = Grid_Var_Sloped_SC * (scale_y ** 2)\n",
    "                            Grid_Uncert_Sloped_OG = np.sqrt(Grid_Var_Sloped_OG) #unscaled    \n",
    "                        else:\n",
    "                            Grid_polar_Logvar_SC = model_prediction(model, static_scaled, Grid_time_SC) #scaled\n",
    "                            Grid_polar_OG = scaler_y.inverse_transform(Grid_polar_Logvar_SC[:, 0:1]) #unscaled\n",
    "                            Grid_Logvar_SC = Grid_polar_Logvar_SC[:, 1:2]  # Don't transform this\n",
    "                            Grid_Var_SC = np.exp(Grid_Logvar_SC)\n",
    "                            # Variance rescales by the square of the scale factor\n",
    "                            Grid_Var_OG = Grid_Var_SC * (scale_y ** 2)\n",
    "                            Grid_Uncert_OG = np.sqrt(Grid_Var_OG) #unscaled   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        fig, ax = plt.subplots(figsize=(8, 5)) \n",
    "                        title = f\"{modelsubfolder.name}_{base_name}\"\n",
    "                        parts = title.split(\"_\")\n",
    "                        clean_title = \"_\".join([parts[1], parts[2]] + parts[4:9])  # keep a structure like \"Average_3\" + [234, polar1, 8, 12, 23, 0]\n",
    "                        ax.set_title(clean_title)\n",
    "                        ax.grid(True)\n",
    "\n",
    "                        \n",
    "                        if Correction == True:\n",
    "                            ax.scatter(Grid_time_OG.flatten(), Grid_polar_Sloped_OG.flatten(), color='green', alpha=0.7, label=\"Predicted & Corrected\", marker='x', s=10)\n",
    "                            ax.fill_between(\n",
    "                                Grid_time_OG.flatten(),\n",
    "                                (Grid_polar_Sloped_OG - Grid_Uncert_Sloped_OG).flatten(),\n",
    "                                (Grid_polar_Sloped_OG + Grid_Uncert_Sloped_OG).flatten(),\n",
    "                                color='green',\n",
    "                                alpha=0.1,\n",
    "                                label=\"Prediction ±1σ\")                            \n",
    "                            \n",
    "                             \n",
    "                        else:\n",
    "                            ax.scatter(Grid_time_OG.flatten(), Grid_polar_OG.flatten(), color='red', alpha=0.7, label=\"Predicted\", marker='x', s=40)\n",
    "                            ax.fill_between(\n",
    "                                Grid_time_OG.flatten(),\n",
    "                                (Grid_polar_OG - Grid_Uncert_OG).flatten(),\n",
    "                                (Grid_polar_OG + Grid_Uncert_OG).flatten(),\n",
    "                                color='red',\n",
    "                                alpha=0.1,\n",
    "                                label=\"Prediction ±1σ\")\n",
    "\n",
    "                        #Plot the initial and final points too\n",
    "                        pred_times = np.array([initial_dt, final_dt])\n",
    "                        pred_values = np.array([initial_p, final_p])\n",
    "                        pred_uncert = np.array([u[0][0],u[1][0]]) \n",
    "                        ax.errorbar(\n",
    "                            pred_times,\n",
    "                            pred_values,\n",
    "                            yerr=pred_uncert,\n",
    "                            fmt='o',           # circle markers\n",
    "                            color='black',\n",
    "                            label='Original points ±1σ',\n",
    "                            capsize=5,         # little caps on error bars\n",
    "                            markersize=8\n",
    "                        )\n",
    "                        \n",
    "                        # Tight Y-limits\n",
    "                        #y_min = min(np.min(true_polar), np.min(mu_pred_grid)) - 0.05\n",
    "                        y_min = float(pT_raw[0][0]) - 0.02\n",
    "                        #y_max = max(np.max(true_polar), np.max(mu_pred_grid)) + 0.05\n",
    "                        y_max = float(p0_raw[0][0]) + 0.02\n",
    "                        ax.set_ylim([y_min, y_max])\n",
    "\n",
    "                        ax.legend()\n",
    "                        fig.tight_layout()\n",
    "\n",
    "                        # Save figures\n",
    "                        full_name = f\"{modelsubfolder.name}_{base_name}\"\n",
    "                        parts = full_name.split(\"_\")\n",
    "\n",
    "                        # Keep \"Average_3\" (parts[1], parts[2]) and the chunk [234, polar1, 8, 12, 23, 0] (parts[4:10])\n",
    "                        short_name = \"_\".join([parts[1], parts[2]] + parts[4:9])\n",
    "                        fig_filename = short_name\n",
    "                        fig_path = output_subfolder_path / fig_filename\n",
    "\n",
    "                        predicted_data_path = output_subfolder_path / f\"PredictedData_{short_name}.txt\"\n",
    "\n",
    "                        log_message(f\"Predict: Saved figure to: {fig_path}\")\n",
    "                        fig.savefig(long_path(fig_path.with_suffix(\".png\")), dpi=200)\n",
    "                        plt.close(fig)\n",
    "                        del fig, ax\n",
    "                        gc.collect()\n",
    "                        log_message(f\"Predict: Saved figure to: {fig_path.with_suffix('.jpg')}\")\n",
    "\n",
    "\n",
    "                        #9.Save data to text file\n",
    "\n",
    "                        base_dir = os.path.dirname(os.getcwd())  # parent folder of ML\n",
    "                        dst_path = long_path(os.path.join(os.getcwd(), \"CopyPolarizationTimeReference.txt\"))\n",
    "\n",
    "                        \n",
    "                        with open(long_path(predicted_data_path), 'w') as f:\n",
    "                            f.write(\"GridTime,Time,PredictedPolarizationD3,ErrPredictedPolarizationD3\\n\")\n",
    "                            if Correction == True:\n",
    "                                for t, p, s in zip(Grid_time_OG.flatten(), Grid_polar_Sloped_OG.flatten(), Grid_Uncert_Sloped_OG.flatten()):\n",
    "                                    # Compute absolute time as datetime + seconds\n",
    "                                    abs_time = PolarizationAbsoluteTime + timedelta(seconds=float(t))\n",
    "                                    f.write(f\"{t:.6f},{abs_time.strftime('%Y-%m-%d %H:%M:%S')},{p:.6f},{s:.6f}\\n\")\n",
    "                            else: \n",
    "                                for t, p, s in zip(Grid_time_OG.flatten(), Grid_polar_OG.flatten(), Grid_Uncert_OG.flatten()):\n",
    "                                    # Compute absolute time as datetime + seconds\n",
    "                                    abs_time = PolarizationAbsoluteTime + timedelta(seconds=float(t))\n",
    "                                    f.write(f\"{t:.6f},{abs_time.strftime('%Y-%m-%d %H:%M:%S')},{p:.6f},{s:.6f}\\n\")\n",
    "\n",
    "                        log_message(f\"Predict: Saved predicted data to: {predicted_data_path}\")\n",
    "                        end_time = pytime.time()\n",
    "                        elapsed_time = end_time - start_time\n",
    "                        with open(long_path(log_file), \"a\") as f:\n",
    "                            f.write(f\"Execution time={elapsed_time:.6f} seconds \\n\")\n",
    "\n",
    "                del model, scaler_static, scaler_time, scaler_y\n",
    "\n",
    "            except StopIteration:\n",
    "                log_message(f\"****Predict: Skipping {modelsubfolder} beacause the model files were not found.\")                        \n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeeb81f-298e-426d-9e82-fc4646a06a78",
   "metadata": {},
   "source": [
    "## 7. Main code and combined model\n",
    "\n",
    "Runs the _Predict_ function. The code will not progress until the user manually inputs the time values. It asks for three values, t_initial (which means that predictions start _t_initial_ seconds later (it can be negative)), _t_final_ (it changes the final time point for predictions by t_final seconds) and _t_step_ (the time steps between predictions)\n",
    "It also takes all previous predictions of all models and averages them using a weighted mean. That way we have a single prediction file that should be the best that ML can currently do.\n",
    "For every experiment, the code finds all the experiment files created by each model. Then, for every line it performs the following calculation. Let $P_1\\left(t\\right),\\ldots,P_n\\left(t\\right)$ be the polarization predictions of models $1,\\ldots,n$ at time $t$ with uncertainties $\\sigma_1\\left(t\\right),\\ldots,\\sigma_n\\left(t\\right)$. Then the weighted mean is \n",
    "\n",
    "$P\\left(t\\right)=\\frac{\\sum_{i=1}^n P_i\\left(t\\right)\\sigma_i^{-2}\\left(t\\right)}{\\sum_{i=1}^n \\sigma_i^{-2}\\left(t\\right)}$\n",
    "\n",
    "with\n",
    "\n",
    "$\\sigma\\left(t\\right)=\\frac{1}{\\sum_{i=1}^n \\sigma_i^{-2}\\left(t\\right)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0aee4-cc82-475f-99ec-b424256fe9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories relative to the script\n",
    "model_dir = long_path(Path(\"./CrystallineModels/\").resolve()) #Where models are stored\n",
    "predict_dir = long_path(Path.cwd() / \"CrystallineToPredictFiles\") #Where experiments to predict are\n",
    "output_folder_path = Path(\"CrystallinePredictionsFolder\").resolve()\n",
    "output_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "output_folder_dir = long_path(output_folder_path)\n",
    "\n",
    "base_dir = Path.cwd().parent\n",
    "time_dir = base_dir / \"FileReadingStoring\" / \"PolarizationTimeReference.txt\" #Where the absolute time of the experiments are\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(predict_dir, exist_ok=True)\n",
    "os.makedirs(output_folder_dir, exist_ok=True)\n",
    "if not os.listdir(model_dir):  \n",
    "    raise RuntimeError(f\"CrystallineModels is empty. Please add models to check again if the directory where models are is adequate\")\n",
    "if not os.listdir(predict_dir):  \n",
    "    raise RuntimeError(f\"CrystallineToPredictFiles is empty. Please add experiments to be predicted there like the ones in the example folder\")\n",
    "if not time_dir.exists():\n",
    "    raise RuntimeError(f\"No PolarizationTimeReference file found at {time_dir}. Run CrystallineFileLeacturePredict.ipynb again.\")\n",
    "\n",
    "Correction = True\n",
    "use_uncertainty = True\n",
    "def ask_number(prompt, cast=float): #Just to force the user to write the time values it wants to use for prediction\n",
    "    while True:\n",
    "        try:\n",
    "            return cast(input(prompt))\n",
    "        except ValueError:\n",
    "            print(\"Invalid input, try again.\")\n",
    "\n",
    "t_initial = ask_number(\"Enter initial time (seconds): \", float)\n",
    "t_final   = ask_number(\"Enter final time (seconds): \", float)\n",
    "t_step    = ask_number(\"Enter time step (seconds): \", float)\n",
    "\n",
    "Predict(predict_dir, model_dir, output_folder_dir, time_dir, use_uncertainty, Correction, t_initial, t_final, t_step)\n",
    "\n",
    "# 1. Get all model prediction folders except the combined one\n",
    "output_folder_path = Path(output_folder_path).resolve()  # absolute path\n",
    "model_folders = [\n",
    "    f for f in output_folder_path.iterdir()\n",
    "    if f.is_dir() and f.name != \"CombinedModel\"\n",
    "]\n",
    "combined_dir = output_folder_path / \"CombinedModel\"\n",
    "\n",
    "combined_dir = Path(combined_dir).resolve()\n",
    "combined_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not model_folders:\n",
    "    raise RuntimeError(f\"No model prediction folders found in {output_folder_path}\")\n",
    "\n",
    "# 2. Read sub-subfolders using the first model folder as example\n",
    "example_subs = model_folders[0] \n",
    "subsubfolders = [f for f in example_subs.iterdir() if f.is_dir()]\n",
    "\n",
    "if not subsubfolders:\n",
    "    raise RuntimeError(f\"No experiment subfolders found in {example_subs}\")\n",
    "\n",
    "# 3. Load prediction files from all models\n",
    "for subsub in subsubfolders:  # loop over each experiment folder\n",
    "    data_frames = []\n",
    "    \n",
    "    # Load all .txt files for this subsubfolder across models\n",
    "    for model_folder in model_folders:\n",
    "        folder_path = model_folder / subsub.name\n",
    "        txt_files = glob.glob(str(folder_path / \"PredictedData_*.txt\"))\n",
    "        txt_files = [Path(long_path(f)) for f in txt_files]\n",
    "        \n",
    "        if len(txt_files) != 1:\n",
    "            print(f\"Skipping {folder_path}, expected 1 prediction file, found {len(txt_files)}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_csv(long_path(txt_files[0]))\n",
    "        data_frames.append(df)\n",
    "\n",
    "    # Verify all have same GridTime and Time columns\n",
    "    grid_time = data_frames[0][\"GridTime\"]\n",
    "    time_col = data_frames[0][\"Time\"]\n",
    "\n",
    "    # Stack predictions and errors\n",
    "    values = np.array([df[\"PredictedPolarizationD3\"].values for df in data_frames])\n",
    "    errors = np.array([df[\"ErrPredictedPolarizationD3\"].values for df in data_frames])\n",
    "\n",
    "    # Weighted mean\n",
    "    weights = 1 / (errors ** 2)\n",
    "    weighted_mean = np.sum(weights * values, axis=0) / np.sum(weights, axis=0)\n",
    "    weighted_uncertainty = np.sqrt(1 / np.sum(weights, axis=0))\n",
    "\n",
    "    # Create combined DataFrame\n",
    "    combined_df = pd.DataFrame({\n",
    "        \"GridTime\": grid_time,\n",
    "        \"Time\": time_col,\n",
    "        \"PredictedPolarizationD3\": weighted_mean,\n",
    "        \"ErrPredictedPolarizationD3\": weighted_uncertainty\n",
    "    })\n",
    "\n",
    "    # Save in CombinedModel folder\n",
    "    out_dir = combined_dir / subsub.name  \n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / \"PredictedPolarizationValues.txt\"\n",
    "    combined_df.to_csv(long_path(out_path), index=False)\n",
    "\n",
    "    combined_df.to_csv(long_path(out_path), index=False)\n",
    "    log_message(f\"Saved combined file for {subsub.name}\")\n",
    "\n",
    "print(\"\\nAll combined predictions created successfully!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac9b6ae5-59c3-46fe-b2bb-5985bb5667a1",
   "metadata": {},
   "source": [
    "models_dir = Path(\"CrystallineModels\")\n",
    "\n",
    "def Predict(DeltatimePast, StepLength, DeltatimeFuture):\n",
    "\n",
    "    \"\"\"\n",
    "    11. MAIN CODE FUNCTION\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Extract one file, send to Isolated folder train with the rest, predict on the isolated one and repeat for all. Requires the model type, the direction with the data files. an outut folder\n",
    "    name (to create the subfolders of each experiment in it), the number of augmentations to train the model with and finally a bool (Correction) to force the initial and final polarization\n",
    "    predictions to be the measured values\n",
    "    \"\"\"\n",
    "    def predict_experiments(predict_dir, model_dir, model_type, output_folder, use_uncertainty, Correction):\n",
    "        model_dir = Path(model_dir) #where the models are\n",
    "        predict_dir = Path(predict_dir) #Where the two row experiments are\n",
    "        output_folder = Path(output_folder) #Where the predictions will be stored\n",
    "        log_message(f\"Taking files from {predict_dir}\")\n",
    "        log_message(f\"Looking for models in: {model_dir}\")\n",
    "        mode = \"PolarizationD3\"\n",
    "        # --- Read all polarization reference times once ---\n",
    "        base_dir = Path(os.getcwd()).parent  # parent folder of ML\n",
    "        src_path = base_dir / \"FileReadingStoring\" / \"PolarizationTimeReference.txt\"\n",
    "\n",
    "        # Make sure parent folder exists\n",
    "        src_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_times = [datetime.strptime(line.strip(), \"%Y-%m-%d %H:%M:%S\") for line in f]\n",
    "\n",
    "        # Loop over all subdirectories inside model_dir\n",
    "        for modelsubfolder in model_dir.iterdir():\n",
    "            log_message(f\"Reading model {modelsubfolder}\")\n",
    "            if modelsubfolder.is_dir():\n",
    "                try:\n",
    "                    # === Detect files\n",
    "                    model_path = next(modelsubfolder.glob(\"model_*.keras\"))\n",
    "                    scaler_static_path = next(modelsubfolder.glob(\"scaler_static_*.pkl\"))\n",
    "                    scaler_time_path   = next(modelsubfolder.glob(\"scaler_time_*.pkl\"))\n",
    "                    scaler_y_path      = next(modelsubfolder.glob(\"scaler_y_*.pkl\"))\n",
    "        \n",
    "                    # === Load them\n",
    "                    def nll_loss(y_true, y_pred):\n",
    "                        mu = y_pred[:, 0:1]\n",
    "                        log_var = y_pred[:, 1:2]\n",
    "                        precision = K.exp(-log_var)\n",
    "                        return K.mean(0.5 * (log_var + K.square(y_true - mu) * precision))\n",
    "                    \n",
    "                    model = tf.keras.models.load_model(win_long_path(str(model_path)), custom_objects={\"nll_loss\": nll_loss})\n",
    "                    scaler_static = joblib.load(win_long_path(str(scaler_static_path)))\n",
    "                    scaler_time   = joblib.load(win_long_path(str(scaler_time_path)))\n",
    "                    scaler_y      = joblib.load(win_long_path(str(scaler_y_path)))\n",
    "        \n",
    "                    log_message(f\"\\n Loaded model and scalers from {modelsubfolder}\\n\")\n",
    "        \n",
    "                    #1. For each model read the files for predicting\n",
    "                \n",
    "                    # Get all *_Parameters.txt files to derive base names\n",
    "                    param_files = list(predict_dir.glob(\"*_Parameters.txt\"))\n",
    "                    base_names = [f.stem.replace(\"_Parameters\", \"\") for f in param_files]\n",
    "                    output_midfolder = output_folder / f\"{modelsubfolder.name}\"\n",
    "                    output_midfolder.mkdir(parents=True, exist_ok=True)\n",
    "                    log_message(f\"Created {output_midfolder}\")\n",
    "                    \n",
    "                    for i, base_name in enumerate(base_names):\n",
    "                        start_time = time.time()    \n",
    "\n",
    "                        file_param = predict_dir / f\"{base_name}_Parameters.txt\"\n",
    "                        file_data = predict_dir / f\"{base_name}.txt\"\n",
    "                        PolarizationAbsoluteTime = all_times[i]  # pick the correct row\n",
    "                        log_message(f\"[Experiment {i}] Polarization Reference = {PolarizationAbsoluteTime}\")\n",
    "\n",
    "                    \n",
    "                        parts = base_name.split(\"_\")\n",
    "                        short_name = \"_\".join(parts[1:6])  # take elements 1 through 5\n",
    "                        output_subfolder = output_midfolder / f\"Experiment_{short_name}\"\n",
    "                        output_subfolder.mkdir(exist_ok=True)\n",
    "    \n",
    "                    \n",
    "                        # Move files to the experiment-specific folder\n",
    "                        shutil.move(win_long_path(str(file_param)), win_long_path(str(output_subfolder / file_param.name)))\n",
    "                        shutil.move(win_long_path(str(file_data)), win_long_path(str(output_subfolder / file_data.name)))\n",
    "                    \n",
    "                        log_message(f\"Moved {base_name} files into {output_subfolder}\")                \n",
    "                        #12. Load isolated experiment. Predictions will be done on this experiment. Aligning is required\n",
    "                        isolated_experiments, static_columns_isolated = load_all_experiments(output_subfolder, polarization_column=mode)\n",
    "                        isolated_experiments_aligned = align_static_vectors(\n",
    "                            isolated_experiments, \n",
    "                            static_columns_training=\t['Pressure', 'LabPolarization', 'LabTime', 'CellID_ge18004', 'CellID_ge18006', 'CellID_ge18012'],\n",
    "                            static_columns_isolated=static_columns_isolated\n",
    "                        ) #unscaled\n",
    "                        if LogNoise:\n",
    "                            log_message(f\"      Number of isolated experiments aligned (should be one): {len(isolated_experiments_aligned)}\")\n",
    "                            log_message(f\"      Example static vector (pre-scale): {isolated_experiments_aligned[0][0]}\")\n",
    "                            log_message(f\"      Example time vector (pre-scale): {isolated_experiments_aligned[0][1]}\")\n",
    "                            log_message(f\"      Example polarization vector (pre-scale): {isolated_experiments_aligned[0][2]}\")\n",
    "                \n",
    "                \n",
    "\n",
    "                        #13. Building manually the dataset for the isolated experiment\n",
    "                        for static, timeVec, pol, unc in isolated_experiments_aligned: #all unscaled\n",
    "                            log_message(f\"{len(timeVec)}\")\n",
    "                            if len(timeVec) < 2:\n",
    "                                log_message(f\"Skipping experiment due to insufficient time points\")\n",
    "                                continue  # skip too-short experiments\n",
    "                        \n",
    "                            #13.1 Extract the initial and final points, add them as parameters and fit the entire static vector\n",
    "                            initial_dt, initial_p = timeVec[0], pol[0]\n",
    "                            final_dt, final_p = timeVec[-1], pol[-1]\n",
    "                            static_vector = static + [initial_dt, initial_p, final_dt, final_p] #unscaled\n",
    "                            if LogNoise:\n",
    "                                log_message(f\"  Experiment static vector length (pre-scale): {len(static_vector)}\")\n",
    "                                log_message(f\"  Experiment static vector (pre-scale): {static_vector}\")\n",
    "                            static_scaled = scaler_static.transform(np.array(static_vector).reshape(1, -1)).flatten() #scaled\n",
    "                            if LogNoise:\n",
    "                                log_message(f\"  Experiment static vector (scaled): {static_scaled}\")\n",
    "                            \n",
    "                            #13.2 Scale time and polarization arrays (only intermediate points)\n",
    "                            time_intermediate = np.array(timeVec).reshape(-1, 1) # unscaled\n",
    "                            time_scaled = scaler_time.transform(time_intermediate).flatten() #scaled\n",
    "                            if LogNoise:\n",
    "                                log_message(f\"  Experiment time (pre-scale): {time_intermediate.flatten()}\")\n",
    "                                log_message(f\"  Experiment time (scaled): {time_scaled}\")\n",
    "                            pol_intermediate = np.array(pol).reshape(-1, 1) #unscaled\n",
    "                            pol_scaled = scaler_y.transform(pol_intermediate).flatten() #scaled\n",
    "                            if LogNoise:\n",
    "                                log_message(f\"  Experiment polarization (pre-scale): {pol_intermediate.flatten()}\")\n",
    "                                log_message(f\"  Experiment polarization (scaled): {pol_scaled}\")\n",
    "                            unc_intermediate = np.array(unc).reshape(-1, 1) #unscaled\n",
    "                            if LogNoise:\n",
    "                                log_message(f\"  Experiment uncertainty: {unc_intermediate.flatten()}\")\n",
    "            \n",
    "                            \n",
    "            \n",
    "                            #13.3 Combine all structures so that the shape is correct for prediction\n",
    "                            scaled_isolated_experiments = []\n",
    "                            scaled_isolated_experiments.append((\n",
    "                                static_scaled, time_scaled, pol_scaled, unc_intermediate.flatten()\n",
    "                            )) #scaled\n",
    "                            \n",
    "                            \n",
    "                            #14 Check for shapes and create the final NumPy arrays of the experiment\n",
    "                            X_static, X_time, y_true, u = [], [], [], []\n",
    "                            for j, (static_scaled, time_scaled, pol_scaled, unc_intermediate) in enumerate(scaled_isolated_experiments):\n",
    "                                if LogNoise:\n",
    "                                    log_message(f\"   static_scaled.shape = {static_scaled.shape}\")\n",
    "                                    log_message(f\"   time_scaled.shape = {time_scaled.shape}\")\n",
    "                                    log_message(f\"   pol_scaled.shape = {pol_scaled.shape}\")\n",
    "                                    log_message(f\"   unc_intermediate.shape = {unc_intermediate.shape}\")\n",
    "                                assert len(time_scaled) == len(pol_scaled) == len(unc_intermediate), \"Mismatch in lengths!\"\n",
    "            \n",
    "                            for static_scaled, time_scaled, pol_scaled, unc_intermediate in scaled_isolated_experiments:\n",
    "                                for t, p, err in zip(time_scaled, pol_scaled, unc_intermediate):\n",
    "                                    X_static.append(static_scaled)   # already includes appended time/polarization info\n",
    "                                    X_time.append([t])               # wrap to preserve 2D structure for Conv1D\n",
    "                                    y_true.append(p)\n",
    "                                    u.append(err)\n",
    "                            X_static = np.array(X_static) #scaled\n",
    "                            X_time = np.array(X_time) #scaled\n",
    "                            y_true = np.array(y_true).reshape(-1, 1) #scaled\n",
    "                            u = np.array(u).reshape(-1, 1) #unscaled\n",
    "            \n",
    "                            if LogNoise:\n",
    "                                log_message(f\"  X_static.shape: {X_static.shape}\")\n",
    "                                log_message(f\"  X_time.shape: {X_time.shape}\")\n",
    "                                log_message(f\"  y_true.shape: {y_true.shape}\")\n",
    "                                log_message(f\"  X_static[0]: {X_static[0]} (scaled)\")\n",
    "                \n",
    "                            #15 Extract the initial and final polarizations and keep a scaled and unscaled version of all. Then predict at those time points. \n",
    "                            #Unscaling needs to be done with static_scaler but then we need to scale them as polarizations or as time points with their respective scalers so that we can use model_predict\n",
    "                            X_static_raw = scaler_static.inverse_transform(X_static)\n",
    "                            # Extract static input (already scaled)\n",
    "                            x_static_single = X_static[0:1]  # shape (1, D_static)\n",
    "                            t0_raw = X_static_raw[0, -4].reshape(1, -1)\n",
    "                            p0_raw = X_static_raw[0, -3].reshape(1, -1)\n",
    "                            tT_raw = X_static_raw[0, -2].reshape(1, -1)\n",
    "                            pT_raw = X_static_raw[0, -1].reshape(1, -1)\n",
    "                \n",
    "                            t0_scaled_correct = scaler_time.transform(t0_raw)\n",
    "                            tT_scaled_correct = scaler_time.transform(tT_raw)\n",
    "                            p0_scaled_correct = scaler_y.transform(p0_raw)\n",
    "                            pT_scaled_correct = scaler_y.transform(pT_raw)\n",
    "                            \n",
    "                            # Predict scaled polarizations at those time points\n",
    "                            pred_initial_scaled = model_prediction(model, x_static_single, t0_scaled_correct)\n",
    "                            pred_final_scaled   = model_prediction(model, x_static_single, tT_scaled_correct)\n",
    "                            \n",
    "                            # Inverse transform predictions to physical (real) polarizations\n",
    "                            p0_pred = scaler_y.inverse_transform(pred_initial_scaled[:, 0:1])\n",
    "                            pT_pred = scaler_y.inverse_transform(pred_final_scaled[:, 0:1])\n",
    "                \n",
    "                            if LogNoise:\n",
    "                                log_message(f\"t0 (real) = {t0_raw}, tT (real) = {tT_raw}\")\n",
    "                                log_message(f\"p0_pred (real) = {p0_pred}, pT_pred (real) = {pT_pred}\")\n",
    "                                log_message(f\"p0 (true) = {p0_raw}, pT (true) = {pT_raw}\")\n",
    "                \n",
    "                \n",
    "                            #16 Prepare the linear correction and predict in the time points where we have data\n",
    "                            m_raw = ( (p0_pred-p0_raw) - (pT_pred-pT_raw) ) / (t0_raw-tT_raw)\n",
    "                            n_raw = (p0_pred-p0_raw) - m_raw * t0_raw\n",
    "                            \n",
    "                            \n",
    "                            if Correction == True:\n",
    "                                y_pred_raw = model_predict_corrected(model, X_static, X_time, m_raw, n_raw, scaler_y, scaler_time) #Input es scaled, parametros unsacles, output es scaled también\n",
    "                            else:\n",
    "                                y_pred_raw = model_prediction(model, X_static, X_time)\n",
    "                                \n",
    "                            #To obtain the results we separate the value predictions form the log variance. The results need to be unscaled to real units while the uncertainty needs to be \n",
    "                            #exponenciated (to obtain a variance), multiplied by the same factor that is used in MinMaxScaler and finally convert to uncertainty (take the square root)\n",
    "                            mean_pred = scaler_y.inverse_transform(y_pred_raw[:, 0:1]).flatten()\n",
    "                            log_var_scaled = y_pred_raw[:, 1]  # Keep in original scale\n",
    "                            var_scaled = np.exp(log_var_scaled)\n",
    "                            scale_y = scaler_y.data_max_[0] - scaler_y.data_min_[0]  # scale factor from MinMaxScaler \n",
    "                            # Variance rescales by the square of the scale factor\n",
    "                            var_rescaled = var_scaled * (scale_y ** 2)\n",
    "                            pred_sigma = np.sqrt(var_rescaled)\n",
    "                            pred_polar = mean_pred\n",
    "                \n",
    "                \n",
    "    \n",
    "                \n",
    "                            # === Define grid for red line \n",
    "                \n",
    "                            grid_times = np.arange(0+DeltatimePast, float(tT_raw[0][0])+DeltatimeFuture, StepLength).reshape(-1, 1)\n",
    "                            \n",
    "                            static_scaled = np.tile(X_static[0], (len(grid_times), 1)) #SCALED\n",
    "                \n",
    "                            time_scaled = scaler_time.transform(grid_times)\n",
    "                \n",
    "                            # === Predict on grid\n",
    "                            if Correction == True:\n",
    "                                pred_scaled_grid = model_predict_corrected(model, static_scaled, time_scaled, m_raw, n_raw, scaler_y, scaler_time) #scaled\n",
    "                                # The old non-corrected prediction just for show\n",
    "                                non_corrected_grid = model_prediction(model, static_scaled, time_scaled) #scaled\n",
    "                                mu_non = scaler_y.inverse_transform(non_corrected_grid[:, 0:1]) #unscaled\n",
    "                                log_non_var_grid_scaled = pred_scaled_grid[:, 1:2]  # Don't transform this #scaled\n",
    "                                non_sigma = np.sqrt( np.exp(log_non_var_grid_scaled) * (scale_y ** 2) ) #unscaled\n",
    "                            else:\n",
    "                                pred_scaled_grid = model_prediction(model, static_scaled, time_scaled)\n",
    "                            mu_pred_grid = scaler_y.inverse_transform(pred_scaled_grid[:, 0:1]) #unscaled\n",
    "                            log_var_grid_scaled = pred_scaled_grid[:, 1:2]  # Don't transform this\n",
    "                            var_grid_scaled = np.exp(log_var_grid_scaled)\n",
    "                            # Variance rescales by the square of the scale factor\n",
    "                            var_grid_rescaled = var_grid_scaled * (scale_y ** 2)\n",
    "                            sigma_grid = np.sqrt(var_grid_rescaled) #unscaled\n",
    "                \n",
    "                           \n",
    "                            # Convert scaled inputs back to real units for plotting\n",
    "                \n",
    "                            \n",
    "                            fig, ax = plt.subplots(figsize=(8, 5)) \n",
    "                            title = f\"{modelsubfolder.name}_{base_name}\"\n",
    "                            parts = title.split(\"_\")\n",
    "                            clean_title = \"_\".join([parts[1], parts[2]] + parts[4:9])  # keep \"Average_3\" + [234, polar1, 8, 12, 23, 0]\n",
    "                            ax.set_title(clean_title)\n",
    "                            ax.grid(True)\n",
    "                            \n",
    "                            # Red line + band for prediction grid\n",
    "                            ax.scatter(grid_times.flatten(), mu_pred_grid.flatten(), color='green', alpha=0.7, label=\"Pred Points\", marker='x', s=40)\n",
    "                            \n",
    "                            if Correction == True:\n",
    "                \n",
    "                                ax.scatter(grid_times.flatten(), mu_non.flatten(), color='red', alpha=0.7, label=\"No Correction\", marker='x', s=40)\n",
    "                                ax.fill_between(\n",
    "                                    grid_times.flatten(),\n",
    "                                    (mu_non - non_sigma).flatten(),\n",
    "                                    (mu_non + non_sigma).flatten(),\n",
    "                                    color='red',\n",
    "                                    alpha=0.1,\n",
    "                                    label=\"No correction ±1σ\")\n",
    "                            ax.fill_between(\n",
    "                                grid_times.flatten(),\n",
    "                                (mu_pred_grid - sigma_grid).flatten(),\n",
    "                                (mu_pred_grid + sigma_grid).flatten(),\n",
    "                                color='green',\n",
    "                                alpha=0.2,\n",
    "                                label=\"Predicted ±1σ\"\n",
    "                            )\n",
    "                \n",
    "                            # Tight Y-limits\n",
    "                            #y_min = min(np.min(true_polar), np.min(mu_pred_grid)) - 0.05\n",
    "                            y_min = float(pT_raw[0][0]) - 0.02\n",
    "                            #y_max = max(np.max(true_polar), np.max(mu_pred_grid)) + 0.05\n",
    "                            y_max = float(p0_raw[0][0]) + 0.02\n",
    "                            ax.set_ylim([y_min, y_max])\n",
    "                            \n",
    "                            ax.legend()\n",
    "                            fig.tight_layout()\n",
    "                            \n",
    "                            # === Save Figure\n",
    "                            # Split the long name into parts\n",
    "                            full_name = f\"{modelsubfolder.name}_{base_name}\"\n",
    "                            parts = full_name.split(\"_\")\n",
    "                            \n",
    "                            # Keep \"Average_3\" (parts[1], parts[2]) and the chunk [234, polar1, 8, 12, 23, 0] (parts[4:10])\n",
    "                            short_name = \"_\".join([parts[1], parts[2]] + parts[4:9])\n",
    "                            \n",
    "                            # Use this shortened version for files\n",
    "                            fig_filename = short_name\n",
    "                            fig_path = output_subfolder / fig_filename\n",
    "                            \n",
    "                            predicted_data_path = output_subfolder / f\"PredictedData_{short_name}.txt\"\n",
    "    \n",
    "                            log_message(f\"Saved figure to: {fig_path}\")\n",
    "                            fig.savefig(win_long_path(str(fig_path.with_suffix(\".png\"))), dpi=200)\n",
    "                            plt.close(fig)\n",
    "                            del fig, ax\n",
    "                            gc.collect()\n",
    "                            log_message(f\"Saved figure to: {fig_path.with_suffix('.jpg')}\")\n",
    "                \n",
    "    \n",
    "                            #19. Save all predictions and raw data in text files === Save data to text file\n",
    "    \n",
    "                            \n",
    "                            \n",
    "                            # --- Paths ---\n",
    "                            base_dir = os.path.dirname(os.getcwd())  # parent folder of ML\n",
    "                            dst_path = os.path.join(os.getcwd(), \"CopyPolarizationTimeReference.txt\")  # copy inside ML\n",
    "\n",
    "                            \n",
    "                            # --- Step 4: Compute difference (assuming DiffractogramAbsoluteTime exists) ---\n",
    "                            \n",
    "                            \n",
    "                            # --- Step 5: Shift grid_times ---\n",
    "                            # Assuming grid_times already exists, created like:\n",
    "                            \n",
    "                            \n",
    "                            # Optional: check a few values\n",
    "                            \n",
    "\n",
    "                            # Save red line predictions\n",
    "                            with open(win_long_path(str(predicted_data_path)), 'w') as f:\n",
    "                                f.write(\"GridTime,Time,PredictedPolarizationD3,ErrPredictedPolarizationD3\\n\")\n",
    "                                \n",
    "                                for t, p, s in zip(grid_times.flatten(), mu_pred_grid.flatten(), sigma_grid.flatten()):\n",
    "                                    # Compute absolute time as datetime + seconds\n",
    "                                    abs_time = PolarizationAbsoluteTime + timedelta(seconds=float(t))\n",
    "                                    f.write(f\"{t:.6f},{abs_time.strftime('%Y-%m-%d %H:%M:%S')},{p:.6f},{s:.6f}\\n\")\n",
    "                            \n",
    "                            log_message(f\"Saved predicted data to: {predicted_data_path}\")\n",
    "                            # --- After computing time_difference_sec ---\n",
    "                            \n",
    "                            # Append short_name + both times\n",
    "                            end_time = time.time()\n",
    "                            elapsed_time = end_time - start_time\n",
    "                            log_file = Path(\"CrystallineExecution_times.txt\")\n",
    "                            # Ensure the parent directory exists\n",
    "                            log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                            \n",
    "                            # Open with Windows long path safety\n",
    "                            with open(win_long_path(str(log_file)), \"a\") as f:\n",
    "                                f.write(f\"Execution time={elapsed_time:.6f} seconds \\n\")\n",
    "\n",
    "                \n",
    "                        # 21. Move all experiments back to the original folder\n",
    "                        shutil.move(win_long_path(str(output_subfolder / file_param.name)), win_long_path(str(file_param)))\n",
    "                        shutil.move(win_long_path(str(output_subfolder / file_data.name)), win_long_path(str(file_data)))\n",
    "                        log_message(f\"Returned {base_name}\")\n",
    "                        log_message(f\"\\n ______________________________________________ \\n\")\n",
    "        \n",
    "                    # === Cleanup (optional, to free GPU/CPU memory)\n",
    "                    del model, scaler_static, scaler_time, scaler_y\n",
    "                    log_message(f\"\\n =============================================== \\n\")\n",
    "        \n",
    "                except StopIteration:\n",
    "                    log_message(f\" Skipping {modelsubfolder}, files not found.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    12. MAIN CODE\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    First we make sure all folders are created and then we run the isolate_experiemnts function that will loop over all experiments\n",
    "    \"\"\"\n",
    "    # Define your paths\n",
    "    model_dir = Path(\"./CrystallineModels/\").resolve()\n",
    "    # Folder name\n",
    "    predict_dir = Path.cwd() / \"CrystallineToPredictFiles\"\n",
    "    # Recreate output folder\n",
    "    output_folder = Path(f\"CrystallinePredictionsFolder\").resolve()\n",
    "    predict_dir_long = Path(win_long_path(str(predict_dir)))\n",
    "    \n",
    "    os.makedirs(win_long_path(model_dir), exist_ok=True)\n",
    "    os.makedirs(win_long_path(output_folder), exist_ok=True) \n",
    "    Correction = True\n",
    "    # Move all files back\n",
    "    \n",
    "    # Run main function\n",
    "    use_uncertainty = True\n",
    "    Correction = True\n",
    "    predict_experiments(\n",
    "        predict_dir=win_long_path(predict_dir),\n",
    "        model_dir=win_long_path(model_dir),\n",
    "        model_type=model_type,\n",
    "        output_folder=win_long_path(output_folder),\n",
    "        use_uncertainty=use_uncertainty,\n",
    "        Correction=Correction)\n",
    "    \n",
    "Predict(t_initial,t_step,t_final)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Main folder\n",
    "base_dir = \"CrystallinePredictionsFolder\"\n",
    "combined_dir = os.path.join(base_dir, \"CombinedModel\")\n",
    "\n",
    "# Identify model subfolders (exclude CombinedModel)\n",
    "model_folders = [\n",
    "    f for f in os.listdir(base_dir)\n",
    "    if os.path.isdir(os.path.join(base_dir, f)) and f != \"CombinedModel\"\n",
    "]\n",
    "\n",
    "# Get sub-subfolder names from the first model folder\n",
    "example_subs = os.path.join(base_dir, model_folders[0])\n",
    "subsubfolders = [\n",
    "    f for f in os.listdir(example_subs)\n",
    "    if os.path.isdir(os.path.join(example_subs, f))\n",
    "]\n",
    "\n",
    "# Process each sub-subfolder\n",
    "for subsub in subsubfolders:\n",
    "    data_frames = []\n",
    "    \n",
    "    # Load all 5 .txt files for this subsubfolder\n",
    "    for model in model_folders:\n",
    "        file_path = os.path.join(base_dir, model, subsub)\n",
    "        txt_files = [f for f in os.listdir(file_path) if f.endswith(\".txt\")]\n",
    "        \n",
    "        if len(txt_files) != 1:\n",
    "            print(f\"Skipping {file_path}, expected 1 .txt file but found {len(txt_files)}.\")\n",
    "            continue\n",
    "        \n",
    "        file = os.path.join(file_path, txt_files[0])\n",
    "        df = pd.read_csv(file)\n",
    "        data_frames.append(df)\n",
    "    \n",
    "    if len(data_frames) != len(model_folders):\n",
    "        print(f\"Skipping {subsub}: missing some model files.\")\n",
    "        continue\n",
    "\n",
    "    # Verify all have same GridTime and Time\n",
    "    grid_time = data_frames[0][\"GridTime\"]\n",
    "    time_col = data_frames[0][\"Time\"]\n",
    "\n",
    "    # Stack all predicted values and errors into numpy arrays\n",
    "    values = np.array([df[\"PredictedPolarizationD3\"].values for df in data_frames])\n",
    "    errors = np.array([df[\"ErrPredictedPolarizationD3\"].values for df in data_frames])\n",
    "\n",
    "    # Compute weights and weighted means\n",
    "    weights = 1 / (errors ** 2)\n",
    "    weighted_mean = np.sum(weights * values, axis=0) / np.sum(weights, axis=0)\n",
    "    weighted_uncertainty = np.sqrt(1 / np.sum(weights, axis=0))\n",
    "\n",
    "    # Create combined DataFrame\n",
    "    combined_df = pd.DataFrame({\n",
    "        \"GridTime\": grid_time,\n",
    "        \"Time\": time_col,\n",
    "        \"PredictedPolarizationD3\": weighted_mean,\n",
    "        \"ErrPredictedPolarizationD3\": weighted_uncertainty\n",
    "    })\n",
    "\n",
    "    # Save inside CombinedModel/SubSubX\n",
    "    out_dir = os.path.join(combined_dir, subsub)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, \"PredictedPolarizationValues.txt\")\n",
    "\n",
    "    combined_df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved combined file for {subsub}\")\n",
    "\n",
    "print(\"\\nAll combined predictions created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87939f94-7a4d-45fa-aded-8b644bb6fb4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6730b98-a555-40dc-9942-65506a744b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
