{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd7452b-da74-4a36-9663-8758d350adf7",
   "metadata": {},
   "source": [
    "CrystalineFileLecturePredict.ipynb\n",
    "\n",
    "Reads from D3Files all the files it is going to process\n",
    "\n",
    "Outputs the following folders and files:\n",
    "\n",
    "1. CrystallineLog_Predicting_Creation.txt\n",
    "Logs all the prints and every step the code does. If you trust the code, it is irrelevant. If you don't trust it or want to change it then this txt file will tell you how each experiment file has been processed and where there might have been issues.\n",
    "\n",
    "\n",
    "2. Crystalline_CellID\n",
    "Contains the Cell IDs found on all the files. Needed for the ML code.\n",
    "\n",
    "\n",
    "3. CrystallineSeparatedFolder\n",
    "It will create a folder for each subfolder where .fli files that could be used were found. Only the numerical giles are saved, a.k.a, {base_name}.txt. They contain DeltaTime (the time of the measurement measured from the first VALID polarization measurement), PolarizationD3, SoftPolarizationD3 (the polarization after using a Savitzky-Golay filter) and ErrPolarizationD3 (the uncertainty). The parameter files are directly sent to the next folder. Despite having duplicates of the information, this folder has been saved as it also has any intermediate file that has not been fully processed. If one of the .fli files has any issues, the file is saved as is before running to the issue. \n",
    "\n",
    "4. ML/CrystallinePredictFiles\n",
    "This folder is not inside the folder you are currently in (FileReadingStoring) but on another one at the same level called ML. It contains the files nneded for the ML predictions.\n",
    "\n",
    "4.1 {base_name}.txt\n",
    "Contain DeltaTime (the time of the measurement measured from the first VALID polarization measurement), PolarizationD3, SoftPolarizationD3 (the polarization after using a Savitzky-Golay filter) and ErrPolarizationD3 (the uncertainty).\n",
    "\n",
    "4.2 {base_name}_Parameters.txt\n",
    "Contains the CellID, Pressure, LabPolarization (the polarization measured at the lab) and LabTimeCellID (the time when it was measured)\n",
    "\n",
    "\n",
    "5. CrystallineDataBase\n",
    "Contains all the .fli files that were attempted to be read before manipulating them\n",
    "\n",
    "\n",
    "6. CrystallineBadFiles\n",
    "Contains all the .fli separated in experiment sets folders that were rejected (not enough points, negative polarizations, etc.)\n",
    "\n",
    "7. PolarizationTImeReference\n",
    "The models where trained using relative time only. This means that the first valid polarization measurement has been considered as time zero for each experiment and the rest of measurements have their associated time values as a variation of time since that reference. This way all experiments have the same structure. However this reference time is not absolute and the measurements of the diffractograms may have an absolute time reference different from the ones used in the models. By saving the string \"Year-Month-Day Hour:Minute:Second\" we can safely change the time reference\n",
    "\n",
    "\n",
    "_________________________________________________________________________________________\n",
    "\n",
    "Process it follows:\n",
    "\n",
    "1. REMOVAL OF PREVIOUS ITERATIONS\n",
    "To avoid leaks and duplications, all files are erased before running the code file\n",
    "\n",
    "2. ZIP FOLDER TREATMENT\n",
    "The code will take all the zip files, extract them, remove duplicates using the name AND hash sha256.\n",
    "\n",
    "3. SEPARATION OF FLI FILES ACCORDING TO EXPERIMENTS\n",
    "\n",
    "Some fli files have the wrong structure (they are not polarization measurementes) and if they are polarization files they can have more than one experiment per file.\n",
    "For evey fli file we will read the contents and try to find the header (a string in an entire line). This symbolizes the beginning of an experiment\n",
    "If there are numerical values before the first header, that means that the process of saving the file occured before changing something of the experiment. These data rows will be skipped\n",
    "A correct fli file will have the following structure:\n",
    "    polariser cell info ge18004 pressure/init. polar 2.29 0.79 initial date/time 17 09 23 @ 10:39\n",
    "    37391   4.000   0.000   1.000 18/09/23 06:20:44     155.03  +z +z     0.8391    0.0156   11.4270    1.2031     120.00\n",
    "    37392   4.000   0.000   1.000 18/09/23 06:26:49     155.05  +x +x     0.8255    0.0110   10.4610    0.7211     300.00\n",
    "    ...\n",
    "\n",
    "Which corresponds to the following information:\n",
    "    String:'polariser cell info', CellID, String:'pressure/init. polar', Pressure(unknown units), InitialLabPolarization, String:'date/time', Day, Month, Year, String:'@', Hour:Minute\n",
    "    Measurement Number, First Miller Index, Second Miller Index, Third Miller Index, Date Of Measurement, Time Of Measurement, Temperature [Kelvin],\n",
    "                        Direction Of Polarization In The First Polarizer Cell (Direction of the quantum operator S_x,S_y,S_z), Direction Of Polarization In The Second Polarizer Cell,\n",
    "                        Polarization, Polarization uncertainty, Flipping Ratio, FlippingRatio Uncertainty, Duration of the measurement\n",
    "\n",
    "The direction +z is chosen to be pointing away from the ground.\n",
    "The direction +x is the direction of the flow of neutrons, i.e, the direction of Scattering.\n",
    "The direction +y is the orthogonal to both of them.\n",
    "D3 uses two polariser cells, one between the reactor and the sample and a second between the sample and the sensor. The first one guarantees that only neutrons with the correct spin direction\n",
    "interacts with the sample. The second one guarantees that only the neutrons that have unchanged spin direction after interacting with the sample are detected by the sensor. This is\n",
    "the reason why the directions (+z,+y,+x,-z,-y,-x) appear twice.\n",
    "We have considered that temperature is not a relevant factor and the flipping ratio has no new information that polarization alrady posseses.\n",
    "First, the code will first locate the first header (ignoring eveything before) and save all the data afterwards (until the next header or end of the document) in a file with the suffix Array_{i} (i is the number of headers already processed in that fli file)\n",
    "Second, it will save the header as a file with the suffix Parameters.\n",
    "Third, the header row and the columns of Measurement Number, Temperature, Flipping Ratio, FlippingRatio Uncertainty and Time Between Measurements will be erased\n",
    "Fourth, as all data measurement uses the +z,+z combination, all other combinations are erased\n",
    "Fifth, not all data from all Miller Index combinations are polarization measurements. Even some of the ones that are polarization measurements are tampered (playing with magnetic fields for example).\n",
    "This means that there needs to be a way to select the correct combination. For starters, irrational Miller indices are not used for measurements with the samples (they need to be discarded)\n",
    "The integer Miller indices combination will be put to the test by all the functions defined before.\n",
    "Sixth, It computes a score depending on how many derivatives are negative, (200 / (200 - percent_neg) - 1) to be precise. This is a normalized score (0-1) with a 1/x evolution. Also it computes a score depending of the size of N, 2 * (-0.5 + 1 / (1 + needed_N / 8.54e-2)) to be precise. 8.54e-2 is the maximum of the data set. If a new maximum is achieved, the score wont be normalized (0-1) but won't break. The final score combines both of these values (addition). Manually I have seen that 1.4 is a good threshold. If Score>1.4 the file will be accepted. If it is smaller it will be discarded by the main code (a False will be returned). It does the m<0 test, writes everything in Summary_txt (filename, Score associated with N, Score associated with Derivatives nad the total Score). If the file was chosen to be forcefully accepted or rejected, a string will appear in the .txt file. Finally it will save plots of both filters in PlotResults if it is correct and in FailureTest if it is considered a bad file. Again, if a new file is added it may be wise to check your experiment in these folders. For more info read the description of FilteringMethodInt = 0 inside the code\n",
    "Seventh, it will try each Miller index combination for a set i value, apply a filter, and return filtered df + PrettyCombination. it will only add the filtered column if enough points & data changes significantly. If it doesn't change too much, the column PolarizationD3 will be duplicated with the new name\n",
    "Eight, it repeats the process of obtaining the area in m*x+n-N < y < m*x+n+N where 75% of the points are inside the area. It also multiplies the value of N by a factor AcceptableMultiplier and erases all points outisde this bigger area. As uncertainties are clearly underestimated I tried to make them reasonable (looking at the dispersion of the points it is clear there is systematic uncertainties. Under the hypothesis that the polarization curve should be a soft curve (at least C^1) we will try to use χ^2 to add a provisional uncertainty margin fitting to a linear expression. This is a very inaccurate uncertainty increase but it is an improvement of the underestimated uncertainties (and the lack of ways to quantify the systematic uncertainty sources). The enlarged area will be plotted and saved in PlotResults for both the normal data and the softened data (Savitzky–Golay filter)    \n",
    "    \n",
    "4. CellID SAVING\n",
    "It will safely store in a txt file all the cell ids so that the code in ML can use them\n",
    "\n",
    "5. DUPLICATION REMOVAL\n",
    "It will check if the files created for the ML algorithm are duplicates and erases them in that case\n",
    "\n",
    "\n",
    "________________________________________________________________________________________\n",
    "\n",
    "VERY VERY IMPORTANT\n",
    "\n",
    "The other code files should not have an issue with the structure inside the fli files as long as you download them from the online data base and keep them zipped. However here we expect a very specific structure. PLEASE HAVE THE FILES INSIDE THE ZIP FILES HAVE THE FOLLOWING STRUCTURE (perhaps in the future we can change it so it is also automatic).\n",
    "1. Only two rows of numerical values per 'polariser cell info' row\n",
    "2. The same miller indices for those two rows\n",
    "3. Direction +z,+z\n",
    "Here you have an example:\n",
    "\n",
    "polariser cell info ge18004 pressure/init. polar 2.30 0.78 initial date/time 06 06 25 @ 15:15\n",
    "   59685   0.000   0.000   2.000 06/06/25 16:20:21     228.88  +z +z     0.8460    0.0007   11.9879    0.0558     180.00\n",
    "   60273   0.000   0.000   2.000 10/06/25 15:36:07     170.95  +z +z     0.5454    0.0023    3.3998    0.0215      60.00\n",
    "polariser cell info ge18012 pressure/init. polar 2.30 0.78 initial date/time 10 06 25 @ 15:45\n",
    "   60275   0.000   0.000   2.000 10/06/25 15:47:23     170.14  +z +z     0.8363    0.0011   11.2157    0.0764      60.00\n",
    "   60343   0.000   0.000   2.000 11/06/25 14:30:33     169.83  +z +z     0.8016    0.0021    9.0804    0.1047      60.00\n",
    "polariser cell info ge18004 pressure/init. polar 2.30 0.78 initial date/time 11 06 25 @ 14:30\n",
    "   60345   2.000   0.000   0.000 11/06/25 14:33:57     169.83  +z +z     0.8019    0.0021    9.0935    0.1039      60.00\n",
    "   60565   2.000   0.000   0.000 13/06/25 09:43:00     179.66  +z +z     0.6776    0.0033    5.2033    0.0629      60.00\n",
    "polariser cell info ge18012 pressure/init. polar 2.30 0.78 initial date/time 13 06 25 @ 09:44\n",
    "   60567   2.000   0.000   0.000 13/06/25 09:49:03     179.66  +z +z     0.8345    0.0018   11.0877    0.1322      60.00\n",
    "   60886   2.000   0.000   0.000 15/06/25 07:23:31      75.02  +z +z     0.5979    0.0036    3.9735    0.0443      60.00\n",
    "polariser cell info ge18004 pressure/init. polar 2.30 0.79 initial date/time 15 06 25 @ 10:40\n",
    "   60888   2.000   0.000   0.000 15/06/25 10:49:27      75.02  +z +z     0.8467    0.0019   12.0495    0.1627      60.00\n",
    "   60908   2.000   0.000   0.000 16/06/25 05:42:08      75.02  +z +z     0.8059    0.0024    9.3026    0.1256      60.00\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f7936-1d25-4259-95a2-67447178f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "\"\"\"\n",
    "1- LIBRARIES\n",
    "\"\"\" \n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import curve_fit\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "import hashlib\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "2- PRINTING AND LOG DETAILS. LONG PATH CORRECTIONS\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Here we have a custom function for log_messageing and logging everything on a .txt file to be able to know what has happened on the code\n",
    "\n",
    "\"\"\"\n",
    "PrintDebug = True #This Bool will determine if all logs should be log_messageed on screen on the Python Notebook. The log writing is always on. If False the code will be faster.\n",
    "ShowPlot = False #This Bool works the same but with showing on screen the plots (they are always saved even with this variable being False). Reduces program cost if False\n",
    "log_file_path = os.path.join(\".\", \"CrystallineLog_Reading_Creation.txt\")\n",
    "# Initialize log file at the start of the script\n",
    "with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(\"=== Log started ===\\n\")\n",
    "\n",
    "def log_message(message):\n",
    "    if PrintDebug:\n",
    "        print(message)\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "        log_file.write(str(message) + \"\\n\")\n",
    "\n",
    "\n",
    "def win_long_path(path):\n",
    "    # Convert to Path and resolve to absolute\n",
    "    path = Path(path).resolve()\n",
    "\n",
    "    # Convert to string\n",
    "    path_str = str(path)\n",
    "\n",
    "    # Prepend \\\\?\\ if not already present\n",
    "    if not path_str.startswith(\"\\\\\\\\?\\\\\"):\n",
    "        path_str = \"\\\\\\\\?\\\\\" + path_str\n",
    "\n",
    "    return path_str\n",
    "\n",
    "\"\"\"\n",
    "3- FUNCTIONS\n",
    "\"\"\"\n",
    "#3.1-Time function for the conversion of time into seconds\n",
    "def Time(Day_Ref, Hour_Ref):\n",
    "    \"\"\"\n",
    "    Expected variables (strings) should be in the form\n",
    "     Day_Ref = 'DD/MM/YY' a.k.a Day/Month/Year\n",
    "     Hour_Ref = 'HH:MM' a.k.a Hour:Month\n",
    "    Seconds will be ignored if fed to the function (there is a check before the call of the function in the main code that erases the seconds).\n",
    "    The function takes this strings and converts them to seconds\n",
    "    Could be an improvement to consider seconds but the headers don't use seconds, there is no information to prove that the time variables are precise to the second and ~30 seconds is negligible when working with time periods of up to 70000 seconds\n",
    "    \"\"\"\n",
    "    match = re.match(r\"(\\d+)/(\\d+)/(\\d+)\", Day_Ref)\n",
    "    if match:\n",
    "        DD = int(match.group(1))\n",
    "        MM = int(match.group(2))\n",
    "        YY = int(match.group(3))\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid date format: {Day_Ref}\")\n",
    "\n",
    "    # Parse time\n",
    "    match = re.match(r\"(\\d+):(\\d+):(\\d+)\", Hour_Ref)\n",
    "    if match:\n",
    "        Hour = int(match.group(1))\n",
    "        Minute = int(match.group(2))\n",
    "        Second = int(match.group(3))\n",
    "    else:\n",
    "        # Try HH:MM\n",
    "        match = re.match(r\"(\\d+):(\\d+)\", Hour_Ref)\n",
    "        if match:\n",
    "            Hour = int(match.group(1))\n",
    "            Minute = int(match.group(2))\n",
    "            Second = 0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time format: {Hour_Ref}\")\n",
    "\n",
    "    return datetime(YY + 2000 if YY < 100 else YY, MM, DD, Hour, Minute, Second)\n",
    "\n",
    "#3.2- Time function to compute the difference in time between two sets fo time\n",
    "def deltatime(AIni,BIni, AFin,BFin):\n",
    "    \"\"\"\n",
    "    There is nothing as \"Absolute time\". We want to compare durations or intervals of time.\n",
    "    The A variables are of the type 'DD/MM/YY' and the B ones are 'HH:MM'\n",
    "     A is for the time moment considered as reference\n",
    "     B is for the time moment that has been used for measuring something\n",
    "    \"\"\"\n",
    "    time1 = Time(AIni, BIni)\n",
    "    time2 = Time(AFin, BFin)\n",
    "    return( int((time2 - time1).total_seconds()))\n",
    "\n",
    "#3.3 Conversion to integers\n",
    "def format_combination(comb):\n",
    "    \"\"\"\n",
    "    The Miller Indeces in the are writen as strings with float like numbers in them, for example (4.0,1.0,2.0)\n",
    "    As only integer Miller Indices are required we can convert them to integers with this function.\n",
    "    \"\"\"\n",
    "    if comb is None:\n",
    "        return \"(None)\"\n",
    "    ints = tuple(int(float(x)) for x in comb)\n",
    "    return f\"({','.join(map(str, ints))})\"\n",
    "\n",
    "#3.4 Folder name changes\n",
    "def sanitize(name):\n",
    "    \"\"\"\n",
    "    Remove invalid characters for folder names. [<>:\"/\\\\|?*] all turn to _\n",
    "    \"\"\"\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', name)\n",
    "\n",
    "#3.5 Savitzky–Golay filter window parameters\n",
    "def savgol_params_func(n_points):\n",
    "    \"\"\"\n",
    "    Receives tha number of points of the file that the filter will be tried to be used on\n",
    "    Window length can't be greater than the number of points. To avoid Python Errors this function gives the appropiate window length and order of the polygone\n",
    "    \"\"\"\n",
    "    window_length = min(default_window_length, n_points)\n",
    "    if window_length % 2 == 0:\n",
    "        window_length -= 1\n",
    "    if window_length < polyorder + 2:\n",
    "        window_length = polyorder + 2\n",
    "        if window_length % 2 == 0:\n",
    "            window_length += 1\n",
    "    return {'window_length': window_length, 'polyorder': polyorder}\n",
    "\n",
    "#3.6 Filtering Methods\n",
    "def Overall_Decrease (df_filtered, filename):\n",
    "    \"\"\"\n",
    "    The function that accpets files if they are decreasing sets of data (polarization evolution is a decay ALWAYS)\n",
    "    Will save plots of good files in PlotResults so that the user can see what has been accepted\n",
    "    Will save plots of bad files in FailuresTest so that the user can see the polts being discarded.\n",
    "    IMPORTANT: IF YOU ADD A NEW EXPERIMENT AND YOU ARE CONFIDENT IT IS A GOOD FILE BUT IS SENT TO FailuresTest PLEASE MANUALLY ADD THE TXT FILE TO THE force_accept_files. The program should rename eveything in SeparatedFolder so you can get the name from there  adding _Filtered if it is missing\n",
    "    \"\"\"\n",
    "    FilteringMethodInt = 12 \n",
    "    \"\"\"\n",
    "    This int chooses the method for filtering the wrong data. The options are:\n",
    "        FilteringMethodInt = 0   \n",
    "        FilteringMethodInt = 1\n",
    "        FilteringMethodInt = 2\n",
    "        FilteringMethodInt = 12\n",
    "        FilteringMethodInt = 3\n",
    "    \"\"\"\n",
    "    \n",
    "    force_accept_files = [\n",
    "        \"PolarizationD3_EuAgAs_29_8_23_0_MillerIndex_(0,0,4)_Filtered.txt\",\n",
    "        \"PolarizationD3_EuAgAs_1_30_8_23_5_MillerIndex_(3,0,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_SmI3_26_9_23_1_MillerIndex_(0,3,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_SmI3_1_28_9_23_3_MillerIndex_(0,3,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_hor_1_22_3_24_1_MillerIndex_(3,0,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_hor_15_3_24_0_MillerIndex_(0,0,2)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_ver_14_3_24_1_(1,0,0)_Filtered.txt\"]\n",
    "    force_reject_files = [\n",
    "        \"PolarizationD3_EuAgAs_1_29_8_23_0_MillerIndex_(1,1,1)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_ver_14_3_24_1_MillerIndex_(1,0,0)_Filtered.txt\"] #The values were in the 0.3 to 0.2 range. Not a polarization\n",
    "    \n",
    "    \"\"\"\n",
    "    Some files failed the FilteringMethodInt = 12 method despite being considered good files. Others passed the test but were clearly tampered.\n",
    "    These lists override the testing and accepts/rejets them immediately\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "            \n",
    "    if FilteringMethodInt == 12:\n",
    "        \"\"\"\n",
    "        This method combine method 1 and 2 (that is the reason why it is called 12)\n",
    "        It computes a score depending on how many derivatives are negative, (200 / (200 - percent_neg) - 1) to be precise. This is a normalized score (0-1) with a 1/x evolution\n",
    "        It computes a score depending of the size of N, 2 * (-0.5 + 1 / (1 + needed_N / 8.54e-2)) to be precise. 8.54e-2 is the maximum of the data set. If a new maximum is achieved, the score wont be normalized (0-1) but won't break\n",
    "        The final score combines both of these values (addition). Manually I have seen that 1.4 is a good threshold. If Score>1.4 the file will be accepted. If it is smaller it will be discarded by the main code (a False will be returned)\n",
    "        Does the m<0 test\n",
    "        Write eveythin in Summary_txt (filename, Score associated with N, Score associated with Derivatives nad the total Score). If the file was chosen to be forcefully accepted or rejected, a string will appear in the .txt file.\n",
    "        Saves plots of both filters in PlotResults if it is correct and in FailureTest if it is considered a bad file. Again, if a new file is added it may be wise to check your experiment in these folders. For more info read the description of FilteringMethodInt = 0\n",
    "        \"\"\"\n",
    "        output_folder = Path.cwd() / \"CrystallinePlotResults\"\n",
    "\n",
    "        failures_folder = Path.cwd() / \"CrystallineFailuresTest\"\n",
    "        failures_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "        # Ensure paths are Windows long-path safe\n",
    "        output_folder = Path(win_long_path(output_folder))\n",
    "        failures_folder = Path(win_long_path(failures_folder))\n",
    "\n",
    "  \n",
    "\n",
    "        is_good = True\n",
    "\n",
    "\n",
    "    \n",
    "        def make_clean_name(filename: str) -> str:\n",
    "            \"\"\"\n",
    "            Strip unnecessary prefixes/suffixes from filenames.\n",
    "            Example:\n",
    "            PolarizationD3_155K_2_18_9_23_1_MillerIndex_(4,0,1)_Filtered.txt\n",
    "              -> 155K_2_18_9_23_1_(4,0,1)\n",
    "            \"\"\"\n",
    "            base = Path(filename).stem  # remove extension\n",
    "            if base.startswith(\"PolarizationD3_\"):\n",
    "                base = base[len(\"PolarizationD3_\"):]\n",
    "            if base.endswith(\"_Filtered\"):\n",
    "                base = base[:-len(\"_Filtered\")]\n",
    "            base = base.replace(\"MillerIndex_\", \"\")\n",
    "            return base\n",
    "\n",
    "        # Build a clean name for titles / saving\n",
    "        clean_name = (\n",
    "            filename.replace(\"PolarizationD3_\", \"\")\n",
    "                    .replace(\"MillerIndex_\", \"\")\n",
    "                    .replace(\"_Filtered.txt\", \"\")\n",
    "        )\n",
    "\n",
    "        def clean_plot_filename(filename: str, needed_N: Optional[float], plot_folder: Path) -> Path:\n",
    "            \"\"\"\n",
    "            Build a clean filename for linear fit plots.\n",
    "            Example:\n",
    "            155K_2_18_9_23_1_(4,0,1)_N_4.30e-03.png\n",
    "            or\n",
    "            155K_2_18_9_23_1_(4,0,1)_NoNFound.png\n",
    "            \"\"\"\n",
    "            base = make_clean_name(filename)\n",
    "            suffix = f\"_N_{needed_N:.2e}\" if needed_N else \"_NoNFound\"\n",
    "            return plot_folder / f\"{base}{suffix}.png\"\n",
    "        \n",
    "        def derivative_plot_filename(filename: str, plot_folder: Path) -> Path:\n",
    "            \"\"\"\n",
    "            Build a clean filename for derivative plots.\n",
    "            Example:\n",
    "            CaFeAl_13_7_6_24_2_(0,0,2)_Derivatives.png\n",
    "            \"\"\"\n",
    "            base = make_clean_name(filename)\n",
    "            return plot_folder / f\"{base}_Derivatives.png\"\n",
    "        x = df_filtered['DeltaTime'].values\n",
    "        y = df_filtered['SoftPolarizationD3'].values\n",
    "        Err = df_filtered['ErrPolarizationD3'].values if 'ErrPolarizationD3' in df_filtered.columns else np.zeros_like(y)\n",
    "\n",
    "        \n",
    "        def make_clean_name(filename: str) -> str:\n",
    "            \"\"\"\n",
    "            Turn e.g.\n",
    "              PolarizationD3_CaFeAl_13_7_6_24_2_MillerIndex_(0,0,2)_Filtered.txt\n",
    "            into:\n",
    "              CaFeAl_13_7_6_24_2_(0,0,2)\n",
    "            and handle cases where filename contains '/' or '\\\\' (dates like DD/MM/YY).\n",
    "            \"\"\"\n",
    "            s = str(filename).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")  # prevent path splitting\n",
    "            base = Path(s).stem  # remove extension if present\n",
    "            if base.startswith(\"PolarizationD3_\"):\n",
    "                base = base[len(\"PolarizationD3_\"):]\n",
    "            if base.endswith(\"_Filtered\"):\n",
    "                base = base[:-len(\"_Filtered\")]\n",
    "            base = base.replace(\"MillerIndex_\", \"\")\n",
    "            return base\n",
    "        \n",
    "        def extended_area_plot_filename(filename: str) -> str:\n",
    "            \"\"\"EuAgAs_5_31_10_23_0_(3,0,0)_ExtendedArea.png\"\"\"\n",
    "            return f\"{make_clean_name(filename)}_ExtendedArea.png\"\n",
    "\n",
    "        return is_good\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "#3.7 Function that decides the correct set of Miller Indices    \n",
    "def filter_best_combination(i,\n",
    "    df,\n",
    "    filter_func,\n",
    "    filter_column_idx,\n",
    "    new_column_name,\n",
    "    filter_params_func,\n",
    "    min_points_required=3,\n",
    "    tolerance=1e-8,\n",
    "    time_column_idx=None,\n",
    "    error_column_idx=None):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        i is the integer that separates the array files (the original .fli files have more than one experiment with different headers. Each experiment is a different i.\n",
    "        df is the pandas dataframe (the data)\n",
    "        filter_func is the type of filter to smooth the data. Only savgol_filter can be used\n",
    "        filter_column_idx is the name of the column in df that will be used for filtering ad all the other tests. Always choose 'PolarizationD3'\n",
    "        new_column name is the name that will be added to the new column. If you cange it from SoftPolarizationD3 you might need to change manually this name in the rest of the code\n",
    "        filter_params_func asks for the parameters for the filter. The function savgol_params_func was made especifically for this \n",
    "        tolerance is a measurement to know it the filter was changed anything or not\n",
    "        time_column_idx works like filter_column_idx. Please keep it as 'DeltaTime'\n",
    "        error_column_idx works like filter_column_idx. Please keep it as 'ErrPolarizationD3'\n",
    "    Outputs:\n",
    "        df_filtered is the df dataframe with the new column for SoftPolarization and only the data points of the Miller Inidices Combination that has passed all the filters\n",
    "        PrettyCombination is the Miller Indices combination that has passed the filters. Should be something like (4,1,0) \n",
    "    Try each Miller index combination for a set i value, apply a filter, and return filtered df + PrettyCombination.\n",
    "    Only adds the filtered column if enough points & data changes significantly. If it doesn't change too much, the column PolarizationD3 will be duplicated with the new name\n",
    "    \n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    folder_name = FileName.replace(\".fli\", \"\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Group by first three columns (Miller indices)\n",
    "    combination_counts = (\n",
    "        df.groupby([df.columns[0], df.columns[1], df.columns[2]])\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    log_message(f\"Analyzing combinations in file: {folder_name}_Array_{i}.fli\")\n",
    "    #Read the three numbers from the .fli file\n",
    "    for comb, count in combination_counts.items(): \n",
    "        log_message(f\"Combination {comb} occurs {count} times in file {folder_name}.fli. Trying this combination\")\n",
    "        mask = (\n",
    "            (df.iloc[:,0] == comb[0]) &\n",
    "            (df.iloc[:,1] == comb[1]) &\n",
    "            (df.iloc[:,2] == comb[2])\n",
    "        )\n",
    "        PrettyCombination = format_combination(comb)\n",
    "        \n",
    "\n",
    "        filtered_df = df.loc[mask].copy()\n",
    "        # Requisites for the Combination to be valid:\n",
    "        # Requisite 1: Have data in the data\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      {PrettyCombination} has no data\")\n",
    "            continue\n",
    "        \n",
    "        # Requisite 2: Check if data column exists\n",
    "        if filtered_df.shape[1] <= filter_column_idx:\n",
    "            log_message(f\"      Expected column index {filter_column_idx} not found. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert to numeric all columns (all columns are considered as object type)\n",
    "        filtered_df = filtered_df.apply(pd.to_numeric, errors='coerce')\n",
    "        filtered_df = filtered_df.dropna()  # drops any rows with NaNs introduced by coercion\n",
    "        \n",
    "        # Check dtypes\n",
    "        all_numeric = all(dtype.kind in ('f', 'i') for dtype in filtered_df.dtypes)\n",
    "        \n",
    "        if all_numeric:\n",
    "            log_message(f\"      All columns have been successfully converted to numbers.\")\n",
    "        else:\n",
    "            log_message(f\"      Not all columns are numbers. Current dtypes:\")\n",
    "            log_message(f\"      {filtered_df.dtypes}\")\n",
    "            log_message(f\"      Expect Error Message from Python. Perhaps removing this file might be wise unless all files have the same issue\")\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      All rows dropped after conversion to numeric. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Requisite 3: Polarization is ALWAYS positive. If any is negative, that is not a polarization. Immediately sent to the Bad Files Folder\n",
    "        if (filtered_df.iloc[:, filter_column_idx] < 0).any():\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "            badfile_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "            filtered_df.to_csv(win_long_path(badfiles_txt_path), index=False, sep='\\t')\n",
    "            log_message(f\"      {PrettyCombination} has negative polarization values. Sent to BadFiles with name {filename}. Skipping to next Combination\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        \n",
    "        # Requisite 4: Have at least three rows (otherwise we can't teach the ML algorithm anything).\n",
    "        if len(filtered_df) < 2:\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "            badfile_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "            filtered_df.to_csv(win_long_path(badfiles_txt_path), index=False, sep='\\t')\n",
    "            log_message(\n",
    "                f\"      {PrettyCombination} has only {len(filtered_df)} rows (< {min_points_required}). \"\n",
    "                f\"Sent to BadFiles with name {filename}. Skipping to next Combination\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Requisite 5: Be worthy of having the filter used.\n",
    "        y = filtered_df.iloc[:, filter_column_idx].values\n",
    "        filter_params = filter_params_func(len(y))\n",
    "        try:\n",
    "            y_filtered = filter_func(y, **filter_params)\n",
    "            diff = np.abs(y - y_filtered)\n",
    "            changed_count = np.sum(diff > tolerance)\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "            if changed_count > 0:\n",
    "                log_message(f\"      Filter changed {changed_count}/{len(y)} points. Adding column '{new_column_name}'.\")\n",
    "            else:\n",
    "                log_message(f\"      Filter applied but data unchanged. Adding '{new_column_name}' as duplicated values.\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error applying filter to combination {comb}: {e}\")\n",
    "            log_message(f\"      Adding '{new_column_name}' as duplicated values to proceed anyway.\")\n",
    "            # Just duplicate the original column\n",
    "            y_filtered = y.copy()\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "        #log_message(filtered_df)\n",
    "        # Requisite 5: Add the Derivative and BandWidth filtering logic\n",
    "        if Overall_Decrease(filtered_df, f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"):\n",
    "            log_message(f\"      {PrettyCombination} has surpassed all tests. Proceding with it.\")\n",
    "            return filtered_df, PrettyCombination\n",
    "        else:\n",
    "            log_message(f\"      {PrettyCombination} failed the Overall_Decrease test. Trying next combination.\")\n",
    "            continue\n",
    "    return None, None\n",
    "\n",
    "#3.8 Function that removes the points considered bad and fixes the underestimated Uncertainty  \n",
    "def RemoveOutcast_FixUncertainty(df_filtered, PrettyCombination, filename, AcceptableMultiplier=2.0, ShowPlot=False):\n",
    "    \"\"\"\n",
    "    Repeats the process of obtaining the area in m*x+n-N < y < m*x+n+N where 75% of the points are inside the area.\n",
    "    Multiplies the value of N by a factor AcceptableMultiplier and erases all points outisde this bigger area.\n",
    "    As uncertainties are clearly underestimated I tried to make them reasonable (looking at the dispersion of the points it is clear there is systematic uncertainties\n",
    "    Under the hypothesis that the polarization curve should be a soft curve (at least C^1) we will try to use χ^2 to add a provisional uncertainty margin fitting to a linear expression.\n",
    "    This is a very inaccurate uncertainty increase but it is an improvement of the underestimated uncertainties (and the lack of ways to quantify the systematic uncertainty sources)\n",
    "    The enlarged area will be plotted and saved in PlotResults for both the normal data and the softened data (Savitzky–Golay filter)\n",
    "    \"\"\"\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. ZIP FOLDER TREATMENT\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "To make it easier for the user to use, you can just download the \"processed\" files in the ILL data base using the zip file. To avoid duplications\n",
    "and other issues, the files won't be deleted. This reduces speed as it needs to reprocess all the files but we make sure that it works\n",
    "This code and the next will take all the zip files, extract them, remove duplicates using the name AND hash sha256.\n",
    "\"\"\"\n",
    "to_erase = [\n",
    "    \"CrystallineLog_Predicting_Creation.txt\",\n",
    "    \"CrystallinePolariserAndAnalyser_IDs.txt\",\n",
    "    \"CrystallineSeparatedFolder\",\n",
    "    \"CrystallinePlotResults\",\n",
    "    \"CrystallineMLDataBase\",\n",
    "    \"PolarizationTimeReference.txt\",\n",
    "    \"CrystallineDataBase\",\n",
    "    \"CrystallineBadFiles\", \n",
    "    \"CrystallineFailuresTest\", \n",
    "    \"CrystallineCell_ID.txt\"\n",
    "]\n",
    "for item in to_erase:\n",
    "    path = os.path.abspath(item)  # full path\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                log_message(f\"Deleted file: {path}\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                log_message(f\"Deleted folder: {path}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\" Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"Not found (skipped): {path}\")\n",
    "# Path to CrystallinePredictFiles inside ML\n",
    "predict_path = os.path.abspath(os.path.join(\"..\", \"ML\", \"CrystallinePredictFiles\"))\n",
    "\n",
    "if os.path.exists(predict_path):\n",
    "    try:\n",
    "        if os.path.isdir(predict_path):\n",
    "            shutil.rmtree(predict_path)\n",
    "            log_message(f\"Deleted folder: {predict_path}\")\n",
    "        else:\n",
    "            os.remove(predict_path)\n",
    "            log_message(f\"Deleted file (unexpected, was not a folder): {predict_path}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Could not delete {predict_path}: {e}\")\n",
    "else:\n",
    "    log_message(f\"Not found (skipped): {predict_path}\")        \n",
    "def file_hash(filepath, algo=\"sha256\", block_size=65536):\n",
    "    \"\"\"Compute hash of a file (default SHA256).\"\"\"\n",
    "    h = hashlib.new(algo)\n",
    "    with open(win_long_path(filepath), \"rb\") as f:\n",
    "        for block in iter(lambda: f.read(block_size), b\"\"):\n",
    "            h.update(block)\n",
    "    return h.hexdigest()\n",
    "\n",
    "folder = Path(\"D3Files\")  # Folder where all the raw data folders reside\n",
    "zip_files = [f.name for f in folder.glob(\"*.zip\")]  # List all zip files\n",
    "\n",
    "log_message(f\"Reading ZIP files. Checking for true duplicates by content...\")\n",
    "base_names = set()\n",
    "seen_hashes = {}\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    zip_path = folder / zip_file\n",
    "    name = Path(zip_file).stem\n",
    "    ext = Path(zip_file).suffix\n",
    "\n",
    "\n",
    "    # Compute hash of this file\n",
    "    filehash = file_hash(zip_path)\n",
    "\n",
    "    if filehash in seen_hashes:\n",
    "        log_message(f\"Duplicate confirmed by hash! Removing: {zip_file} (same as {seen_hashes[filehash]})\")\n",
    "        #os.remove(win_long_path(zip_path))   # Safe long path\n",
    "    else:\n",
    "        seen_hashes[filehash] = zip_file\n",
    "        base_names.add(name)\n",
    "\n",
    "log_message(f\"\\n All duplicates (by content) removed. Begin unzipping...\\n\")\n",
    "\n",
    "# Refresh zip_files list after removals\n",
    "zip_files = [f.name for f in folder.glob(\"*.zip\")]\n",
    "\n",
    "# Unzip and remove original zip files\n",
    "for zip_file in zip_files:\n",
    "    zip_path = folder / zip_file\n",
    "    if zipfile.is_zipfile(win_long_path(zip_path)):\n",
    "        folder_name = sanitize(zip_file.stem if isinstance(zip_file, Path) else os.path.splitext(zip_file)[0])\n",
    "        extract_dir = folder / folder_name\n",
    "        log_message(f\"Unzipping: {zip_file} -> {extract_dir}\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(win_long_path(zip_path), 'r') as zip_ref:\n",
    "                zip_ref.extractall(win_long_path(extract_dir))\n",
    "        except Exception as e:\n",
    "            log_message(f\"WARNING: Error extracting {zip_file}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"WARNING: Skipping invalid zip file: {zip_file}\")\n",
    "\n",
    "log_message(f\"\\n Finished Unzipping. Experiments stored in individual folders substituting the zip files\\n\")\n",
    "\n",
    "# --- Extraction of .fli files ---\n",
    "source_folder = Path(\"D3Files\")\n",
    "# Destination folder at the same level as the notebook\n",
    "database_folder = (Path.cwd() / \"CrystallineDataBase\").resolve()\n",
    "\n",
    "# Ensure parent exists\n",
    "database_folder.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Now create the folder\n",
    "os.makedirs(win_long_path(database_folder), exist_ok=True)\n",
    "\n",
    "log_message(f\"\\n\\n\\n Scanning all folders for .fli files...\\n \")\n",
    "for item in os.listdir(win_long_path(source_folder)):\n",
    "    item_path = source_folder / item  # Path object\n",
    "    if item_path.is_dir():\n",
    "        log_message(f\"Processing folder: {item_path.name}\")\n",
    "        for root, dirs, files in os.walk(win_long_path(item_path)):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(\".fli\"):\n",
    "                    src_file = Path(root) / file\n",
    "                    dest_file = database_folder / file\n",
    "\n",
    "                    # Handle duplicate names\n",
    "                    counter = 1\n",
    "                    base_name = Path(file).stem\n",
    "                    ext = Path(file).suffix\n",
    "                    while (database_folder / f\"{base_name}_{counter}{ext}\").exists():\n",
    "                        counter += 1\n",
    "                    dest_file = database_folder / f\"{base_name}_{counter}{ext}\"\n",
    "\n",
    "                    log_message(f\"Copying: {src_file} -> {dest_file}\")\n",
    "                    shutil.copy2(win_long_path(src_file), win_long_path(dest_file))\n",
    "\n",
    "        log_message(f\"Deleting folder: {item_path}\")\n",
    "        shutil.rmtree(win_long_path(item_path))\n",
    "\n",
    "\n",
    "log_message(f\"\\n All .fli files collected, sent from folder {source_folder} to folder {database_folder}.\\n\")\n",
    "\n",
    "\"\"\"\n",
    "6 SEPARATION OF FLI FILES ACCORDING TO EXPERIMENTS\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Some fli files have the wrong structure (they are not polarization measurementes) and if they are polarization files they can have more than one experiment per file.\n",
    "For evey fli file we will read the contents and try to find the header (a string in an entire line). This symbolizes the beginning of an experiment\n",
    "If there are numerical values before the first header, that means that the process of saving the file occured before changing something of the experiment. These data rows will be skipped\n",
    "A correct fli file will have the following structure:\n",
    "    polariser cell info ge18004 pressure/init. polar 2.29 0.79 initial date/time 17 09 23 @ 10:39\n",
    "    37391   4.000   0.000   1.000 18/09/23 06:20:44     155.03  +z +z     0.8391    0.0156   11.4270    1.2031     120.00\n",
    "    37392   4.000   0.000   1.000 18/09/23 06:26:49     155.05  +x +x     0.8255    0.0110   10.4610    0.7211     300.00\n",
    "    ...\n",
    "\n",
    "Which corresponds to the following information:\n",
    "    String:'polariser cell info', CellID, String:'pressure/init. polar', Pressure(unknown units), InitialLabPolarization, String:'date/time', Day, Month, Year, String:'@', Hour:Minute\n",
    "    Measurement Number, First Miller Index, Second Miller Index, Third Miller Index, Date Of Measurement, Time Of Measurement, Temperature [Kelvin],\n",
    "                        Direction Of Polarization In The First Polarizer Cell (Direction of the quantum operator S_x,S_y,S_z), Direction Of Polarization In The Second Polarizer Cell,\n",
    "                        Polarization, Polarization uncertainty, Flipping Ratio, FlippingRatio Uncertainty, Duration of the measurement\n",
    "\n",
    "The direction +z is chosen to be pointing away from the ground.\n",
    "The direction +x is the direction of the flow of neutrons, i.e, the direction of Scattering.\n",
    "The direction +y is the orthogonal to both of them.\n",
    "D3 uses two polariser cells, one between the reactor and the sample and a second between the sample and the sensor. The first one guarantees that only neutrons with the correct spin direction\n",
    "interacts with the sample. The second one guarantees that only the neutrons that have unchanged spin direction after interacting with the sample are detected by the sensor. This is\n",
    "the reason why the directions (+z,+y,+x,-z,-y,-x) appear twice.\n",
    "We have considered that temperature is not a relevant factor and the flipping ratio has no new information that polarization alrady posseses.\n",
    "First, the code will first locate the first header (ignoring eveything before) and save all the data afterwards (until the next header or end of the document) in a file with the suffix Array_{i} (i is the number of headers already processed in that fli file)\n",
    "Second, it will save the header as a file with the suffix Parameters.\n",
    "Third, the header row and the columns of Measurement Number, Temperature, Flipping Ratio, FlippingRatio Uncertainty and Time Between Measurements will be erased\n",
    "Fourth, as all data measurement uses the +z,+z combination, all other combinations are erased\n",
    "Fifth, not all data from all Miller Index combinations are polarization measurements. Even some of the ones that are polarization measurements are tampered (playing with magnetic fields for example).\n",
    "This means that there needs to be a way to select the correct combination. For starters, irrational Miller indices are not used for measurements with the samples (they need to be discarded)\n",
    "The integer Miller indices combination will be put to the test by all the functions defined before.\n",
    "For evey succesful experiment we will output:\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Multiplier={Multiplier}.png\" in PlotResults. Shows the plot with the extended area with the raw data\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Multiplier={Multiplier}_Soft.png\" in PlotResults. Shows the plot with the extended area with the filtered data\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt_plot_Derivatives.png\" in PlotResults. Shows the evolution of the \"derivatives\"\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt_N_{N}_ManualInterval.png\" in PlotResults. Shows the plot with the non-exteded area\n",
    "    Txt:    \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}.txt\" in MLDataBase. It contains the four data columns (DeltaTime, PolarizationD3, SoftPolarizationD3, ErrPolarizationD3)\n",
    "    Txt:    \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\" in MLDataBase. It contains the parameters (CellID, Pressure, LabPolarization, LabTime)\n",
    "These plots are not necessary but are saved for the user to know what all the files look like.\n",
    "The files that are wrong or useless when all is done are the folowing:\n",
    "    Txt:    \"{folder_name}_Arrays_{i}.txt\" in SeparatedFolder/{folder_name}. It still has the header and useless columns. It is the fli file of evey chunk, of every recorded experiment (correct or incorrect)\n",
    "    Txt:    \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}.txt\" in SeparatedFolder/{folder_name}. It is the same as the one in MLDataBase (a duplicate)\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}.png\" in SeparatedFolder/{folder_name}. It plots (with error bars) PolarizationD3\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Combined.png\" in SeparatedFolder/{folder_name}. It plots (with error bars) both PolarizationD3 and SoftPolarizationD3\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Softened.png\" in SeparatedFolder/{folder_name}. It plots (with error bars) SoftPolarizationD3\n",
    "    Folder: \"FailuresTest\" contains all the graphs of the data sets that were considered not worthy but had more points that the ones saved. Check them if your experiment was not properly added\n",
    "    Folder: \"DataBase\" has the raw fli files. Once the code has been used they are no longer important (if you don't find the folder I may have added a line of code to erase it. Sorry in advance for any inconveniences)  \n",
    "\"\"\"\n",
    "# Path to the original folder and the final folder\n",
    "DataBase = Path('CrystallineDataBase')\n",
    "output_base = Path('CrystallineSeparatedFolder')\n",
    "\n",
    "# List all .fli files in that folder, prepare folders\n",
    "FileNameList = [f.name for f in DataBase.glob('*.fli')]\n",
    "polyorder = 2\n",
    "default_window_length = 5\n",
    "SeparatedFolder = Path(\"CrystallineSeparatedFolder\")\n",
    "BadFilesFolder = Path(\"CrystallineBadFiles\")\n",
    "MLDataBaseFolder = Path(\"CrystallineMLDataBase\")\n",
    "BadFilesFolder.mkdir(exist_ok=True, parents=True)\n",
    "MLDataBaseFolder.mkdir(exist_ok=True, parents=True)\n",
    "log_message(f\"\\n\\n Files in the data base that will be (tried) to be used\\n {FileNameList}\\n\")\n",
    "\n",
    "for FileName in FileNameList:\n",
    "    \"\"\"READ THE FILE AND SEPRATE IT INTO EACH EXPERIMENT USING THE POLARIZATION CELL\"\"\"\n",
    "    # 1- Open file\n",
    "    folder_name = FileName.replace(\".fli\", \"\")\n",
    "    output_folder = output_base / folder_name\n",
    "    file_path = DataBase / FileName\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(win_long_path(file_path), \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    \n",
    "    # 2- Locate the header with CellID, Pressure, etc. Chunks are the data rows sandwiched between two 'polariser cell info' strings\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    started = False  # flag to know when we found first header\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"polariser cell info\"):  # All before polariser cell info will be forgotten\n",
    "            if started and current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = [line]\n",
    "            started = True\n",
    "        else:\n",
    "            if started:\n",
    "                current_chunk.append(line)\n",
    "            # else: we are before the first header, so ignore these lines\n",
    "\n",
    "    if not started:\n",
    "        log_message(f\" File '{FileName}' does NOT contain any 'polariser cell info' header. Skipping.\\n\")\n",
    "        continue\n",
    "    else:\n",
    "        log_message(f\" File '{FileName}' contains at least one 'polariser cell info' header.\")\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    #3- Save .fli files for every correct chunk\n",
    "    base_name = FileName.replace(\".fli\", \"\")  # remove .fli for clean filenames\n",
    "    log_message(f\"Creating all the Array files \\n\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        log_message(chunk)\n",
    "        fli_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "        fli_path = output_folder / fli_filename\n",
    "        with open(fli_path, \"w\") as f_out:\n",
    "            f_out.writelines(chunk)  # each row is written exactly as it was\n",
    "\n",
    "    # 4- As CellID can be exchanged with real parameters, it is written in an independent file\n",
    "    cell_id_file = Path.cwd() / \"Crystalline_CellID.txt\"\n",
    "    try:\n",
    "        with open(win_long_path(cell_id_file), 'r', encoding='utf-8') as file:\n",
    "            seen_strings = set(line.strip() for line in file)\n",
    "    except FileNotFoundError:\n",
    "        seen_strings = set()\n",
    "    \n",
    "    # 5- Open each Array file and work with it (The Array file still has the header)\n",
    "    with open(win_long_path(cell_id_file), 'a', encoding='utf-8') as file:\n",
    "        for i in range(len(chunks)):\n",
    "            FLI_filename = f\"{base_name}_Arrays_{i}.fli\"  # Name of the Array file\n",
    "            FLI_path = output_folder / FLI_filename  # Full path\n",
    "            if not FLI_path.exists():\n",
    "                log_message(f\"WARNING: Array file does not exist: {FLI_path}\")\n",
    "                continue\n",
    "            df = pd.read_csv(win_long_path(FLI_path), sep=r'\\s+', header=None, on_bad_lines='skip')  # Read file\n",
    "            log_message(f\"Reading {FLI_path}, removing ***WARNING No centering scan found \")\n",
    "            warning_str = \"***WARNING No centering scan found\"\n",
    "\n",
    "            #5.1 Combine first 4 columns as strings, join them with space, and filter rows containing this phrase (it is not important for us)\n",
    "            df = df[~df.iloc[:, :5].astype(str).agg(' '.join, axis=1).str.contains('No centering scan found', regex=False)] \n",
    "            \n",
    "            #5.2 Extract useful information from the header. Hopefully, CellID, Pressure, LabPolarization, Year, Month, Day, time of lab measurement before first experiment measurement (negative time) will be stored locally\n",
    "            log_message(f\"Header Information Extraction...\")\n",
    "            CellID =          df.iloc[0].tolist()[3]\n",
    "            Pressure =        df.iloc[0].tolist()[6]\n",
    "            LabPolarization = df.iloc[0].tolist()[7]\n",
    "\n",
    "            try:\n",
    "                HM, DD, MM, YY = df.iloc[0].tolist()[14], int(df.iloc[0].tolist()[10]), int(df.iloc[0].tolist()[11]), int(df.iloc[0].tolist()[12])\n",
    "                Day_Ref = f\"{DD:02d}/{MM:02d}/{YY:02d}\"\n",
    "                dt = Time(Day_Ref, HM)\n",
    "            except Exception as e:\n",
    "                log_message(f\"Skipping file {file_path} because of invalid header data: {e}\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            #5.3 All redundant/useless information is removed\n",
    "            log_message(f\"Removing Measurement Index, Temperature, Flipping Ratio, Uncertainty of Flipping Ratio and Time between measurements,...\")\n",
    "            df = df.iloc[1:].reset_index(drop=True)\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "            df = df.drop(df.columns[5], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            log_message(f\"Saving only polarization values for the Spin Directions wanted in both Polarizer Cells, i.e. (+z,+z)\")\n",
    "\n",
    "            #5.4 Keep only rows where both are +z\n",
    "            df = df[(df[7] == '+z') & (df[8] == '+z')].copy()\n",
    "            if df.empty:\n",
    "                log_message(f\"No valid '+z' rows in file {FileName}_Arrays_{i}.fli, skipping\")\n",
    "                continue  # skip to next file in your loop\n",
    "            df = df.drop(df.columns[[5,6]], axis=1, errors='ignore')\n",
    "            #5.5 Convert Miller index columns into integers. From string or object to float and if the float is close to an integer (tolerance is 1e-8) then save as integer. Otherwise remove row\n",
    "            cols_to_convert = [1, 2, 3]\n",
    "            df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce').astype(float)            \n",
    "            mask = np.isclose(df[cols_to_convert], np.round(df[cols_to_convert]), atol=1e-8)\n",
    "            df = df[mask.all(axis=1)].copy()\n",
    "            log_message(f\"All Spin directions removed. All irrational Miller Indices removed. Adding DeltaTime\")\n",
    "            \n",
    "            #5.5 The time columns are converted into difference of time being the referenced time the first +z,+z measurement that has survived at this point\n",
    "            if df.shape[0] < 2:\n",
    "                log_message(f\"Not enough valid rows after filtering, skipping chunk\")\n",
    "                continue\n",
    "            df['DeltaTime'] = df.apply(\n",
    "                lambda row: deltatime(df[4].iloc[0], df[5].iloc[0], row[4], row[5]), axis=1 )\n",
    "            ref_dt = Time(df[4].iloc[0], df[5].iloc[0])\n",
    "            LabTime = int((dt - ref_dt).total_seconds())\n",
    "            \n",
    "            #5.5 The time columns are converted into difference of time being the referenced time the first +z,+z measurement that has survived at this point\n",
    "            df['DeltaTime'] = df.apply(\n",
    "                lambda row: deltatime(df[4].iloc[0], df[5].iloc[0], row[4], row[5]), axis=1 )\n",
    "            ref_dt = Time(df[4].iloc[0], df[5].iloc[0])\n",
    "            with open(\"PolarizationTimeReference.txt\", \"a\") as f:\n",
    "                f.write(str(ref_dt) + \"\\n\")\n",
    "            LabTime = int((dt - ref_dt).total_seconds())\n",
    "\n",
    "            #5.6 Rename the columns PolarizationD3, ErrPolarizationD3 (the polarization column and its uncertainty). The other one with name is DeltaTime. The rest are numbers (will be erased).\n",
    "            #Also we remove the time strings (with DeltaTime they have no new information)\n",
    "            log_message(f\"Renaming PolarizationD3 and ErrPolarizationD3\")\n",
    "            df.rename(columns={\n",
    "                df.columns[5]: 'PolarizationD3',\n",
    "                df.columns[6]: 'ErrPolarizationD3'\n",
    "            }, inplace=True)\n",
    "            df.drop(columns=[df.columns[3], df.columns[4]], inplace=True)\n",
    "            log_message(f\"Dropped Time Strings\")\n",
    "\n",
    "            \n",
    "            #5.7 Begin filtering and softening with previous functions\n",
    "            log_message(f\"Begin removal of Bad files and softening with Savitzky-Golay filter\")\n",
    "            filtered_df, PrettyCombination = filter_best_combination(i,\n",
    "                df,\n",
    "                filter_func=savgol_filter,\n",
    "                filter_column_idx=df.columns.get_loc('PolarizationD3'),\n",
    "                new_column_name='SoftPolarizationD3',\n",
    "                filter_params_func=savgol_params_func,\n",
    "                min_points_required=3,\n",
    "                tolerance=1e-8,\n",
    "                time_column_idx=df.columns.get_loc('DeltaTime'),\n",
    "                error_column_idx=df.columns.get_loc('ErrPolarizationD3')\n",
    "                )\n",
    "            #If nothing survived the filters/purge then use'continue' and go for the next experiment\n",
    "            if filtered_df is None and PrettyCombination is None:\n",
    "                log_message(f\"Chunk {i}: No suitable combination found. Skipping to next chunk or file.\")\n",
    "                log_message(f\"_______________________________________________________________\\n\")\n",
    "                continue  # skip to next chunk\n",
    "            \n",
    "            #5.8 Removal of Miller indices (we have all the information they could give us)\n",
    "            log_message(f\"Removing Miller Indices columns\")\n",
    "            #log_message(filtered_df)\n",
    "            filtered_df = filtered_df.iloc[:, 3:]\n",
    "            desired_order = [\"DeltaTime\", \"PolarizationD3\", \"SoftPolarizationD3\", \"ErrPolarizationD3\"]\n",
    "\n",
    "            \n",
    "            # 5.9 Remove the points that won't be useful for the ML algorithm\n",
    "            columns_to_save = [col for col in desired_order if col in filtered_df.columns]  # Keep only the columns that exist\n",
    "            df_SEMIFINAL = filtered_df[columns_to_save].copy()\n",
    "            df_FINAL = filtered_df = df_SEMIFINAL\n",
    "            \n",
    "            \n",
    "\n",
    "            # 5.11 Save the files\n",
    "            log_message(f\"Finally we save the chunk\")\n",
    "            csv_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}.txt\"\n",
    "            Parameter_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\"\n",
    "            csv_path = output_folder / csv_filename\n",
    "            csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                \n",
    "            # Save CSV\n",
    "\n",
    "            df_FINAL.to_csv(win_long_path(csv_path), index=False, sep=',')\n",
    "            \n",
    "            # Save a copy to ML database\n",
    "            ml_txt_path = MLDataBaseFolder / csv_filename\n",
    "            ml_txt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            df_FINAL.to_csv(win_long_path(ml_txt_path), index=False, sep=',')\n",
    "            log_message(f\"Saved: {csv_filename}\")\n",
    "            \n",
    "            # Parameter file\n",
    "            log_message(f\"Processing Parameter file...\")\n",
    "            ml_param_path = MLDataBaseFolder / Parameter_filename\n",
    "            ml_txt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            with open(win_long_path(ml_param_path), 'w', encoding='utf-8') as f:\n",
    "                f.write(\"CellID,Pressure,LabPolarization,LabTime\\n\")\n",
    "                f.write(f\"{CellID},{Pressure},{LabPolarization},{LabTime}\")\n",
    "            log_message(f\"Saved: {Parameter_filename}\")\n",
    "            log_message(f\"Parameter and Array files saved to ML database: {MLDataBaseFolder}\\n_______________________________________________________________\\n\\n\")\n",
    "        else:\n",
    "            log_message(f\"{FileName}_Arrays_{i}.fli is empty, skipping this file.\\n\")\n",
    "            continue  \n",
    "\n",
    "    #5.12 Remove unwanted folders and files\n",
    "    for i in range(len(chunks)):\n",
    "        temp_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "        temp_path = output_folder / temp_filename\n",
    "        try:\n",
    "            temp_path.unlink()\n",
    "        except FileNotFoundError:\n",
    "            pass        \n",
    "    log_message(f\"Created and saved {len(chunks)} CSV files from file called {FileName}.\")\n",
    "    if output_folder.exists() and not any(output_folder.iterdir()):\n",
    "        output_folder.rmdir()\n",
    "        log_message(f\"Removed empty folder: {output_folder}\")\n",
    "    log_message('\\n\\n')\n",
    "\n",
    "import glob\n",
    "         \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "7. CELLID FILE PROCESSING\n",
    "\"\"\"\n",
    "ml_database_folder = Path(\"CrystallineMLDataBase\")\n",
    "\n",
    "# Find all txt files whose names end with Parameters.txt (case insensitive)\n",
    "parameter_files = list(ml_database_folder.glob('*Parameters.txt'))\n",
    "\n",
    "log_message(f\"Found {len(parameter_files)} parameter files.\")\n",
    "\n",
    "unique_cell_ids = []\n",
    "seen = set()\n",
    "\n",
    "for filepath in parameter_files:\n",
    "    try:\n",
    "        with open(win_long_path(filepath), 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 2:\n",
    "                second_row = lines[1].strip()\n",
    "                parts = second_row.split(',')\n",
    "                if parts:\n",
    "                    cell_id = parts[0]\n",
    "                    if cell_id not in seen:\n",
    "                        seen.add(cell_id)\n",
    "                        unique_cell_ids.append(cell_id)\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to read {filepath}: {e}\")\n",
    "\n",
    "# Write to Crystalline_CellID.txt\n",
    "cellid_file = Path.cwd() / \"Crystalline_CellID.txt\"\n",
    "with open(win_long_path(cellid_file), \"w\", encoding='utf-8') as f:\n",
    "    for cell_id in unique_cell_ids:\n",
    "        f.write(f\"{cell_id}\\n\")\n",
    "\n",
    "log_message(f\"Saved {len(unique_cell_ids)} unique cell IDs to {cellid_file.name}.\")\n",
    "\n",
    "# Remove the separated folder\n",
    "folder_to_delete = Path.cwd() / \"CrystallineSeparatedFolder\"\n",
    "if folder_to_delete.exists():\n",
    "    shutil.rmtree(win_long_path(folder_to_delete))\n",
    "    log_message(f\"Folder '{folder_to_delete}' has been deleted.\")\n",
    "else:\n",
    "    log_message(f\"Folder '{folder_to_delete}' does not exist.\")\n",
    "\n",
    "\"\"\"\n",
    "Removal of Duplicates\n",
    "\"\"\"\n",
    "hash_map = defaultdict(list)\n",
    "\n",
    "def file_sha256(filepath, block_size=65536):\n",
    "    \"\"\"Compute SHA256 hash of a file (safe for large files).\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(win_long_path(filepath), \"rb\") as f:\n",
    "        while chunk := f.read(block_size):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "# Scan all .txt files (only base files without '_Parameters')\n",
    "for root, _, files in os.walk(win_long_path(ml_database_folder)):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".txt\") and \"_parameters\" not in file.lower():\n",
    "            path = Path(root) / file\n",
    "            file_hash = file_sha256(path)\n",
    "            hash_map[file_hash].append(path)\n",
    "\n",
    "# Report & delete duplicates\n",
    "duplicates_found = False\n",
    "for file_hash, paths in hash_map.items():\n",
    "    if len(paths) > 1:\n",
    "        duplicates_found = True\n",
    "        log_message(f\"\\nDuplicate group (hash={file_hash}):\")\n",
    "        log_message(f\"   Keeping: {paths[0]}\")\n",
    "\n",
    "        # All but the first are duplicates\n",
    "        for p in paths[1:]:\n",
    "            base_name, ext = os.path.splitext(p)\n",
    "            param_file = Path(f\"{base_name}_Parameters{ext}\")\n",
    "\n",
    "            try:\n",
    "                os.remove(win_long_path(p))\n",
    "                log_message(f\"   Deleted duplicate base file: {p}\")\n",
    "            except Exception as e:\n",
    "                log_message(f\"   Could not delete base file {p}: {e}\")\n",
    "\n",
    "            # Also try deleting the corresponding parameter file\n",
    "            if param_file.exists():\n",
    "                try:\n",
    "                    os.remove(win_long_path(param_file))\n",
    "                    log_message(f\"   Deleted parameter file: {param_file}\")\n",
    "                except Exception as e:\n",
    "                    log_message(f\"   Could not delete parameter file {param_file}: {e}\")\n",
    "\n",
    "if not duplicates_found:\n",
    "    log_message(\"No duplicates found in MLDataBase!\")\n",
    "else:\n",
    "    log_message(\"\\n Duplicate cleanup complete!\")\n",
    "    \n",
    "    \n",
    "# Define paths relative to the notebook location\n",
    "current_dir = Path().resolve()  # directory of FileLecture.ipynb\n",
    "ml_database = current_dir / \"CrystallineMLDataBase\"\n",
    "predict_files = current_dir.parent / \"ML\" / \"CrystallineToPredictFiles\"\n",
    "\n",
    "# Make sure the destination exists\n",
    "predict_files.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Step 1: Clear the destination folder ---\n",
    "for item in predict_files.iterdir():\n",
    "    if item.is_file():\n",
    "        os.remove(item)\n",
    "        log_message(f\"Deleted existing file: {item}\")\n",
    "    elif item.is_dir():\n",
    "        shutil.rmtree(item)\n",
    "        log_message(f\"Deleted existing folder: {item}\")\n",
    "\n",
    "# --- Step 2: Move all files from CrystallineMLDataBase -> CrystallineToPredictFiles ---\n",
    "for item in ml_database.iterdir():\n",
    "    dest = predict_files / item.name\n",
    "    shutil.move(win_long_path(item), win_long_path(dest))\n",
    "    log_message(f\"Moved {item.name} -> {predict_files}\")\n",
    "\n",
    "# --- Step 3: Remove the now-empty CrystallineMLDataBase ---\n",
    "try:\n",
    "    ml_database.rmdir()\n",
    "    log_message(f\"Removed empty folder: {ml_database}\")\n",
    "except OSError:\n",
    "    log_message(f\"Could not remove {ml_database}, not empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e20208-60ea-488c-b02a-45d73f9303b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
