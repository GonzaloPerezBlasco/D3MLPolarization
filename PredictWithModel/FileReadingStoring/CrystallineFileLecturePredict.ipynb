{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd7452b-da74-4a36-9663-8758d350adf7",
   "metadata": {},
   "source": [
    "<h1>CrystalineFileLecturePredict.ipynb</h1>\n",
    "\n",
    "Reads from D3Files all the files it is going to process\n",
    "\n",
    "# WE REQUEST THE USER TO GIVE THE FILE WITH ONLY TWO ROWS PER POLARISER CELL USED\n",
    "\n",
    "We should expect something like this:\n",
    "\n",
    "\n",
    "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
    "|--|--|--|--|--|--|--|--|--|--|--|--|--|--|\n",
    "polariser cell info |ge18004 |pressure/init. |polar |2.29 |0.79 |initial |date/time |17 |09 |23 |@ |10:39\n",
    "|37391 |4.000 |0.000 |1.000 |18/09/23 |06:20:44 |155.03 |+z |+z |0.8391 |0.0156 |11.4270 |1.2031 |120.00\n",
    "|37417 |4.000 |0.000 |1.000 |18/09/23 |10:31:59 |155.29 |+z |+z |0.8120 |0.0187 |9.6406 |1.0613 |120.00\n",
    "polariser cell info |ge18012 |pressure/init. |polar |2.27 |0.79 |initial |date/time |18 |09 |23 |@ |10:33\n",
    "|37418 |4.000 |0.000 |1.000 |18/09/23 |10:37:52 |155.29 |+z |+z |0.9101 |0.0107 |21.2483 |2.6375 |120.00\n",
    "|37434 |4.000 |0.000 |1.000 |18/09/23 |14:16:07 |155.33 |+z |+z |0.8784 |0.0129 |15.4409 |1.7409 |120.00\n",
    "polariser cell info |ge18004 |pressure/init. |polar |2.28 |0.79 |initial |date/time |22 |09 |23 |@ |09:45\n",
    "|37462 |4.000 |0.000 |1.000 |22/09/23 |10:06:36 |0.00 |+z |+z |0.8670 |0.0427 |14.0333 |4.8278 |10.00\n",
    "|37521 |4.000 |0.000 |1.000 |23/09/23 |09:51:06 |0.00 |+z |+z |0.7598 |0.0211 |7.3276 |0.7333 |120.00\n",
    "|  ...  |       |       |       |          |          |      |        |        |        |        |        | | |\n",
    "\n",
    "You can have as many sets of three lines as you desire, but they have to be a header and two regular rows per experiment. \n",
    "### How to prepare the files\n",
    "1. Take the raw .fli file and open it in a text reader app (The Note Bloc in Windows opens them like .txt files so it works). \n",
    "2. Choose a Miller index combination and find the first row that has as polarization directions (+z,+z)\n",
    "3. Find the last appearance of that Miller index combination with polarization direction (+z,+z)\n",
    "4. Erase all but those two lines and keep the header too.\n",
    "5. Repeat for every cell used (for every 'polariser cell info' row)\n",
    "\n",
    "The reason why this process has not been automated is to allow the user more freedom when predicting. Perhaps an automatic routine will not take into account what Miller index combination is adequate. Also, the user might have files that have already the correct structure and they don't need the whole pipeline. Either way, if the ILL staff wants this automated routine (using the same logic as the other Reading.ipynb files) please contact Gonzalo and he will happily help you.\n",
    "\n",
    "__________________________________________________________________________________________\n",
    "\n",
    "Outputs the following folders and files:\n",
    "\n",
    "1. **CrystallineLog_Predicting_Creation.txt**\n",
    "Logs all the prints and every step the code does. If you trust the code, it is irrelevant. If you don't trust it or want to change it then this txt file will tell you how each experiment file has been processed and where there might have been issues.\n",
    "\n",
    "\n",
    "2. **Crystalline_CellID**\n",
    "Contains the Cell IDs found on all the files. Needed for the ML code.\n",
    "\n",
    "\n",
    "3. **CrystallineSeparatedFolder**\n",
    "It will create a folder for each subfolder where .fli files, that could be used, were found. Only the numerical files are saved, a.k.a, *{base_name}.txt*. They contain DeltaTime (the time of the measurement measured from the first VALID polarization measurement), PolarizationD3, SoftPolarizationD3 (the polarization after using a Savitzky-Golay filter) and ErrPolarizationD3 (the uncertainty). The parameter files are directly sent to the next folder. Despite having duplicates of the information, this folder has been saved as it also has any intermediate file that has not been fully processed. If one of the .fli files has any issues, the file is saved, as is, before running to the issue. \n",
    "\n",
    "4. **ML/CrystallinePredictFiles**\n",
    "This folder is not inside the folder you are currently working on (FileReadingStoring) but on another folder at the same level as FileReadingStoring called ML. It contains the files needed for the ML predictions.\n",
    "\n",
    "    4.1 **{base\\_name}.txt**\n",
    "Contains DeltaTime (the time of the measurement measured from the first VALID polarization measurement), PolarizationD3, SoftPolarizationD3 (the polarization after using a Savitzky-Golay filter) and ErrPolarizationD3 (the uncertainty).\n",
    "\n",
    "    4.2 **{base\\_name}\\_Parameters.txt**\n",
    "Contains the CellID, Pressure, LabPolarization (the polarization measured at the lab) and LabTimeCellID (the time when it was measured)\n",
    "\n",
    "\n",
    "5. **CrystallineDataBase**\n",
    "Contains all the .fli files that were attempted to be read before manipulating them\n",
    "\n",
    "\n",
    "6. **CrystallineBadFiles**\n",
    "Contains all the .fli separated in experiment sets folders that were rejected (not enough points, negative polarizations, etc.)\n",
    "\n",
    "7. **PolarizationTimeReference**\n",
    "The models where trained using relative time only. This means that the first valid polarization measurement has been considered as time zero for each experiment and the rest of measurements have their associated time values as a variation of time since that reference. This way all experiments have the same structure. However this reference time is not absolute and the measurements of the diffractograms may have an absolute time reference different from the ones used in the models. By saving the string \"Year-Month-Day Hour:Minute:Second\" we can safely change the time reference\n",
    "\n",
    "\n",
    "_________________________________________________________________________________________\n",
    "\n",
    "Some parts of the code might use data from different sessions. It is safer to erase them and create all files from scratch everytime. This is not a big deal because this code file should only be run once unless the data base changes. It also clears all previous ML predictions to avoid mixing up information between experiments\n",
    "\n",
    "The code will take all zipped folders from the folder _D3Files_ and prepare them to get their .fli files extracted.\n",
    "\n",
    "First, it will check if there are duplicate zip folders. To check it, it will compare the folder name and the hash sha256. Duplicate folders will be erased. For more information about hash sha256 check for example:\n",
    ">Wikipedia contributors. (2026, January 2). SHA-2. In Wikipedia, The Free Encyclopedia. Retrieved 10:49, January 17, 2026, from https://en.wikipedia.org/w/index.php?title=SHA-2&oldid=1330753870\n",
    "\n",
    "Second, it will copy the contents of the zipped folders and create a new folder with the name of the experiment inside _D3Files_. No more zipped folders are erased and information gets duplicated.\n",
    "\n",
    "Third, it will try to find all .fli files inside all the unzipped folders whether if they come from a zipped folder or not. It will then send them to the newly created folder _CrystalineDataBase_. If, for each experiment proposal there are more than one .fli files, they get a numeric suffix (\\_1, \\_2,...) to distinguish them. Afterwards, all unzipped folders get erased leaving behind only the non-duplicated zipped folders.\n",
    "\n",
    "Note: All unzipped folders in D3Files will be explored, however they will get erased at the end of the pipeline. If you want them to persist for future runs of the code, they should be zipped first. For ILL users, when navigating the ILL Cloud, the easiest way to prepare the zip files is to download the _processed_ folder for each experiment proposal. The code is \"smart\" enough to only process .fli files with polarization information. Therefore, there is no need to manually prepare anything.\n",
    "To be precise, this cell of code will take all the zip files, extract them nd remove duplicates using the file name And the hash sha256. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "An explanation of the information that all the contents in the .fli files give can be found here: \n",
    "1. 'polariser cell info' (str): Log of the installation of the polariser cells\n",
    "2. 'PolariserID' (str): A string with the type of cell used\n",
    "3. 'pressure/init. polar' (str): A string to introduce the $^\\mathrm{3}$He gas pressure and the polarization measured at the creation lab.\n",
    "4. 'PolariserPressure' (float): $^\\mathrm{3}$He gas pressure in some units\n",
    "5. 'InitialLabPolarization' (float): Polarization measured at the creation lab\n",
    "6. 'initial date/time' (str): A string that introduces the day, month and year and the hour and minutes.\n",
    "7. 'Date' (str): A string with the information DD/MM/YY\n",
    "8. '@' (str): A string to separate date and time\n",
    "9. 'time' (str): A string with the information HH:MM\n",
    "\n",
    "And for the rest of the rows:\n",
    "1. 'Measurement number' (int): The number index of the measurement.\n",
    "2. 'First_Miller_Index' (float): The first Miller index of the crystal. Polarization is measured using a known Si Bragg crystal. For the source of the origin of the Si crystal see:\n",
    ">Stunault, Anne & Vial, S & Pusztai, Laszlo & Cuello, Gabriel & Temleitner, László. (2016). Structure of hydrogenous liquids: separation of coherent and incoherent cross sections using polarised neutrons. Journal of Physics: Conference Series. 711. 012003. 10.1088/1742-6596/711/1/012003. \n",
    "3. 'Second_Miller_Index' (float)\n",
    "4. 'Third_Miller_Index' (float):\n",
    "5. 'Date' (str): A string with the information DD/MM/YY of that measurement\n",
    "6. 'time' (str): A string with the information HH:MM:SS of that measurement\n",
    "7. Temperature \n",
    "8. 'Direction_1' (str): It is the direction of the polarization after the monochromator.The direction +z corresponds to the orthogonal with respect to the floor pointing away from it. +x is the direction of the beam (variable) and +y is the orthogonal (positive orthonormal basis) direction to +z and +x. \n",
    "8. 'Direction_2' (str): The direction of the polarization at the sensors. True polarization measurements are done **only** on the (+z,+z) direction.\n",
    "\n",
    "8. 'D3Polarization' (float): A float with the polarization measurement\n",
    "9. 'ErrD3Polarization' (float): A float with the uncertainty of that polarization measurement\n",
    "10. 'FlippingRatio' (float): The flipping ratio. Given either the flipping ratio or the polarization value, the other one is fully determined. Therefore, only one is needed and that is why we don´t work with the flipping ratio\n",
    "11. 'ErrFlippingRatio' (float): The uncertainty of the flipping ratio\n",
    "12. 'Elapsed time' (float): It is the time used to obtain the measurement (integration of the beam over that number of seconds)\n",
    "\n",
    "Temperature did not seem to have an effect on the decay. Therefore, it has been eliminated in this code cell. Here is a summary of what the code does:\n",
    "\n",
    "1. The code will go through the .fli files and find all rows with 'polariser cell info'. A cell change is considered once a new 'polariser cell info'. At the moment it ignores the experiments that use the 'magical box' as we are not sure if they are experiments compatible with the ones studied here.\n",
    "2. For evey cell change, a new .fli file is created storing all the information including the header row and the measured data rows (in this case, just two). Also, all cell IDs are recorded\n",
    "3. For all .fli files the code now will:   \n",
    "\n",
    "    3.1 Remove unwanted rows (hopefully none)\n",
    "    \n",
    "    3.2 It removes any rows that don´t have polarization directions (+z,+z). (Should remove none if done correctly).\n",
    "    \n",
    "    3.3 Extract data form the header row.\n",
    "    \n",
    "    3.4 Remove unwanted columns (temperature, flipping ratio, counts, elapsed time,...).\n",
    "    \n",
    "    3.5 Set a time reference with the first measurement row. All other time values get referenced with respect to this moment in time and converted into seconds.\n",
    "    \n",
    "    3.6 Ignore all Miller index combinations that are not integers (hopefully no issues here).\n",
    "    \n",
    "    3.7 Save two files for each experiment. One with the header rows and another one with just the numeric rows (with a new header that explains what each column has).\n",
    "\n",
    "For every succesful experiment we will output:\n",
    "1. Image:  **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Multiplier={Multiplier}.png\"** in PlotResults. Shows the plot with the extended area with the raw data\n",
    "2. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Multiplier={Multiplier}\\_Soft.png\"** in PlotResults. Shows the plot with the extended area with the filtered data\n",
    "3. Image: **\"PolarizationD3\\_{folder_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Filtered.txt\\_plot\\_Derivatives.png\"** in PlotResults. Shows the evolution of the \"derivatives\". I apologize for the hideous names. Unless it results in a fatal error, I am scared to change the code.\n",
    "4. Image: **\"PolarizationD3\\_{folder_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex_{PrettyCombination}\\_Filtered.txt\\_N\\_{N}\\_ManualInterval.png\"** in PlotResults. Shows the plot with the non-exteded area\n",
    "5. Txt: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}.txt\"** in MLDataBase. It contains the four data columns (DeltaTime, PolarizationD3, SoftPolarizationD3, ErrPolarizationD3)\n",
    "6. Txt: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Parameters.txt\"** in MLDataBase. It contains the parameters (CellID, Pressure, LabPolarization, LabTime)\n",
    "\n",
    "\n",
    "The plots are not necessary but are saved for the user to know what all the files look like. The txt files are fundamental for the rest of the pipeline. \n",
    "The files that are wrong or useless when all is done are the folowing. They are kept for  debug purposes (to see files with differents structures, why they fail,etc).\n",
    "\n",
    "1. Txt: **\"{folder\\_name}\\_Arrays\\_{i}.txt\"** in SeparatedFolder/{folder_name}. It still has the header and useless columns. It is the fli file of evey chunk, of every recorded experiment (correct or incorrect)\n",
    "2. Txt: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}.txt\"** in SeparatedFolder/{folder_name}. It is the same as the one in MLDataBase (a duplicate)\n",
    "3. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}.png\"** in SeparatedFolder/{folder_name}. It plots (with error bars) PolarizationD3\n",
    "4. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Combined.png\"** in SeparatedFolder/{folder_name}. It plots (with error bars) both PolarizationD3 and SoftPolarizationD3\n",
    "5. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Softened.png\"** in SeparatedFolder/{folder_name}. It plots (with error bars) SoftPolarizationD3\n",
    "6. Folder: **\"FailuresTest\"** contains all the graphs of the data sets that were considered not worthy but had more points that the ones saved. Check them if your experiment was not properly added\n",
    "7. Folder: **\"DataBase\"** has the raw fli files. Once the code has been used they are no longer important (if you don't find the folder I may have added a line of code to erase it. Sorry in advance for any inconveniences)  \n",
    "\n",
    "\n",
    "It erases all intermediate files and prepares the remaining ones for the ML pipeline\n",
    "\n",
    "1. Removes all .fli files that have been created.\n",
    "2. Removes empty folders\n",
    "3. Collects all unique polariser–analyser ID pairs\n",
    " \n",
    "As a result, the only useful files are _Crystalline_CellID.txt_ and the folder _ML/CrystallineToPredictFiles_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d17d7-3285-4923-9afc-26893461c188",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c110f3-e80f-4632-a363-56a257dd73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import curve_fit\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "import hashlib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3c756a-a03d-410a-9e37-001bbe23eab6",
   "metadata": {},
   "source": [
    "## 2. Auxiliary Functions and log file creation\n",
    "\n",
    "1. _PrintDebug_ is a flag that allows the code to output on screen all the steps. If it is set to false, it won´t show anything. However, all information will be properly logged whether this flag is set to true or false. The name of the log is determined by the variable *log_file_path*. The code runs faster if it is set to False.\n",
    "\n",
    "2. _ShowPlot_ is a similar flag that allows the code to show on screen all plots that are being produced. They are all stored independently of whether this flag is True or False. The code runs faster if it is set to False.\n",
    "\n",
    "3. **log_message** is a function used for writting on the log file\n",
    "\n",
    "4. **long_path** is a function that \"fixes\" directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4524a2-b3d6-4375-9f48-555c7c367519",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintDebug = True \n",
    "ShowPlot = False \n",
    "log_file_path = os.path.join(\".\", \"CrystallineLog_Reading_Creation.txt\")\n",
    "with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(\"=== Log started ===\\n\")\n",
    "\n",
    "def log_message(message):\n",
    "    if PrintDebug:\n",
    "        print(message)\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "        log_file.write(str(message) + \"\\n\")\n",
    "\n",
    "\n",
    "def long_path(path):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        path (path): The path that needs to be converted\n",
    "    \n",
    "    Returns:\n",
    "        The updated path string or path depending on the platform used\n",
    "        \n",
    "    Notes:\n",
    "        To avoid Windows 260 character limit for Windows paths, a special \"prefix\" is added.\n",
    "        It also unifies how directories are managed.\n",
    "        Also works with Linux and Mac\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert to Path and resolve to absolute\n",
    "    path = Path(path).resolve()\n",
    "    \n",
    "    #Windows only:  \n",
    "    if os.name == \"nt\":\n",
    "        path_str = str(path)\n",
    "        if not path_str.startswith(\"\\\\\\\\?\\\\\"):\n",
    "            # UNC paths need special handling\n",
    "            if path_str.startswith(\"\\\\\\\\\"):\n",
    "                path_str = \"\\\\\\\\?\\\\UNC\\\\\" + path_str[2:]\n",
    "            else:\n",
    "                path_str = \"\\\\\\\\?\\\\\" + path_str\n",
    "            return path_str\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0bfadb-400e-4325-b65e-caf82dab7597",
   "metadata": {},
   "source": [
    "## 3. Functions\n",
    "\n",
    "1. **Time** is a function that converts time to a universal format\n",
    "\n",
    "2. **deltatime** is a funtions that computes the difference in time between two sets of time, in seconds \n",
    "\n",
    "3. **format_combination** is just an aesthetic change in the Miller index combination variable\n",
    "\n",
    "4. **sanitize** is a funtion that fixes any directory path with \"illegal\" variables\n",
    "\n",
    "5. **savgol_params_func** is a function that ensures that the window length is odd and large enough for the polynomial order.\n",
    "\n",
    "6. Formating functions: Just optimizations for the names of the files being stored. Nothing of interest.\n",
    "\n",
    "7. **filter_best_combination**  is a function that discards all problematic data sets (negative polarization, small sets of data and also uses **Overall_Decrease**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d26ab-3747-44d0-8191-5903c57a60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Time(Day_Ref, Hour_Ref):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        Day_Ref (str): 'DD/MM/YY' a.k.a Day/Month/Year\n",
    "        Hour_Ref (str): = 'HH:MM' or 'HH:MM:SS' a.k.a Hour:Month:Second\n",
    "        \n",
    "    Returns:\n",
    "        A 'datetime' object with format (year, month, day, hour, minute, second).\n",
    "        \n",
    "    Notes:\n",
    "        If there is no information about the seconds, they will be considered 0\n",
    "    \"\"\"\n",
    "    match = re.match(r\"(\\d+)/(\\d+)/(\\d+)\", Day_Ref)\n",
    "    if match:\n",
    "        DD = int(match.group(1))\n",
    "        MM = int(match.group(2))\n",
    "        YY = int(match.group(3))\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid date format: {Day_Ref}\")\n",
    "\n",
    "    match = re.match(r\"(\\d+):(\\d+):(\\d+)\", Hour_Ref)\n",
    "    if match:\n",
    "        Hour = int(match.group(1))\n",
    "        Minute = int(match.group(2))\n",
    "        Second = int(match.group(3))\n",
    "    else:\n",
    "        # If seconds are missing, try HH:MM\n",
    "        match = re.match(r\"(\\d+):(\\d+)\", Hour_Ref)\n",
    "        if match:\n",
    "            Hour = int(match.group(1))\n",
    "            Minute = int(match.group(2))\n",
    "            Second = 0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time format: {Hour_Ref}\")\n",
    "\n",
    "    return datetime(YY + 2000 if YY < 100 else YY, MM, DD, Hour, Minute, Second)\n",
    "\n",
    "################################################################\n",
    "\n",
    "\n",
    "def deltatime(AIni,BIni, AFin,BFin):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        AIni (str): 'DD/MM/YY' a.k.a Day/Month/Year for the initial time\n",
    "        BIni (str): = 'HH:MM' or 'HH:MM:SS' a.k.a Hour:Month:Second for the initial time\n",
    "        AFin (str): 'DD/MM/YY' a.k.a Day/Month/Year for the final time\n",
    "        BFin (str): = 'HH:MM' or 'HH:MM:SS' a.k.a Hour:Month:Second for the final time\n",
    "    Returns:\n",
    "        The time variation in seconds\n",
    "        \n",
    "    Notes:\n",
    "        Requires the function \"Time\"\n",
    "    \"\"\"\n",
    "    time1 = Time(AIni, BIni)\n",
    "    time2 = Time(AFin, BFin)\n",
    "    return( int((time2 - time1).total_seconds()))\n",
    "\n",
    "#################################################################\n",
    "\n",
    "def format_combination(comb):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        comb (float, float, float): A set of three floats characterizing the Miller indices.\n",
    "\n",
    "    Returns:\n",
    "        An object (int,int,int): With the floor integer of those float variables. (3.0 -> 3)\n",
    "    \"\"\"\n",
    "    if comb is None:\n",
    "        return \"(None)\"\n",
    "    ints = tuple(int(float(x)) for x in comb)\n",
    "    return f\"({','.join(map(str, ints))})\"\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def sanitize(name):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        name (str): Directory string \n",
    "    Returns:\n",
    "        The same string but with symbols [,<,>,:,\",/,\\\\,|,?,*,] converted to _\n",
    "    \"\"\"\n",
    "    \n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', name)\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def savgol_params_func(n_points):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        n_points (int): Number of points where the filtered will be used\n",
    "    Returns:\n",
    "        A dictionary containing valid parameters for a Savitzky–Golay filter.\n",
    "    Notes:    \n",
    "        It ensures that the window length is odd and large enough for the polynomial order.\n",
    "    \"\"\"\n",
    "    window_length = min(default_window_length, n_points)\n",
    "    if window_length % 2 == 0:\n",
    "        window_length -= 1\n",
    "    if window_length < polyorder + 2:\n",
    "        window_length = polyorder + 2\n",
    "        if window_length % 2 == 0:\n",
    "            window_length += 1\n",
    "    return {'window_length': window_length, 'polyorder': polyorder}\n",
    "\n",
    "###################################################\n",
    "\n",
    "\n",
    "def make_clean_name(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Turn e.g.\n",
    "      PolarizationD3_CaFeAl_13_7_6_24_2_MillerIndex_(0,0,2)_Filtered.txt\n",
    "    into:\n",
    "      CaFeAl_13_7_6_24_2_(0,0,2)\n",
    "    and handle cases where filename contains '/' or '\\\\' (dates like DD/MM/YY).\n",
    "    \"\"\"\n",
    "    s = str(filename).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")  # prevent path splitting\n",
    "    base = Path(s).stem  # remove extension if present\n",
    "    if base.startswith(\"PolarizationD3_\"):\n",
    "        base = base[len(\"PolarizationD3_\"):]\n",
    "    if base.endswith(\"_Filtered\"):\n",
    "        base = base[:-len(\"_Filtered\")]\n",
    "    base = base.replace(\"MillerIndex_\", \"\")\n",
    "    return base\n",
    "\n",
    "\n",
    "def clean_plot_filename(filename: str, needed_N: Optional[float], plot_folder: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Build a clean filename for linear fit plots.\n",
    "    Example:\n",
    "    155K_2_18_9_23_1_(4,0,1)_N_4.30e-03.png\n",
    "    or\n",
    "    155K_2_18_9_23_1_(4,0,1)_NoNFound.png\n",
    "    \"\"\"\n",
    "    base = make_clean_name(filename)\n",
    "    suffix = f\"_N_{needed_N:.2e}\" if needed_N else \"_NoNFound\"\n",
    "    return plot_folder / f\"{base}{suffix}.png\"\n",
    "\n",
    "def derivative_plot_filename(filename: str, plot_folder: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Build a clean filename for derivative plots.\n",
    "    Example:\n",
    "    CaFeAl_13_7_6_24_2_(0,0,2)_Derivatives.png\n",
    "    \"\"\"\n",
    "    base = make_clean_name(filename)\n",
    "    return plot_folder / f\"{base}_Derivatives.png\"\n",
    "\n",
    "def extended_area_plot_filename(filename: str) -> str:\n",
    "    \"\"\"EuAgAs_5_31_10_23_0_(3,0,0)_ExtendedArea.png\"\"\"\n",
    "    return f\"{make_clean_name(filename)}_ExtendedArea.png\"\n",
    "    \n",
    "\n",
    "#####################################################\n",
    "    \n",
    "def filter_best_combination(i, df):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. i (int): The chunk number a.k.a the (ordinal) number of the Miller index combination\n",
    "        2. df (pandas object): It is something like this: (NaNs are intended)\n",
    "                  1    2    3  PolarizationD3 ErrPolarizationD3  12   13   14  DeltaTime\n",
    "            0   3.0  3.0  3.0          0.5383            0.0021 NaN  NaN  NaN          0\n",
    "            1   3.0  3.0  3.0          0.5379            0.0021 NaN  NaN  NaN        147\n",
    "            2   3.0  3.0  3.0          0.5315            0.0022 NaN  NaN  NaN       3919\n",
    "            ...\n",
    "\n",
    "    Returns:\n",
    "        1. A filtered df object like this one:\n",
    "                  1    2    3  PolarizationD3 ErrPolarizationD3 DeltaTime\n",
    "            0   3.0  3.0  3.0          0.5383            0.0021         0\n",
    "            1   3.0  3.0  3.0          0.5379            0.0021       147\n",
    "            2   3.0  3.0  3.0          0.5315            0.0022      3919\n",
    "            ...\n",
    "        2. An object (int,int,int) with the adequate Miller index combination.\n",
    "        \n",
    "    Notes:    \n",
    "        First, it extracts the Miller index combination and converts it into a set of three integers (format_combination)\n",
    "        Then it tries a couple of tests to see if the data associated to them is valid\n",
    "            1. Check there is data\n",
    "            2. Check if the time array is present and convert all values to either floats or integers\n",
    "            3. Check if all polarization values are positive. If they are not, skip that Miller index combination\n",
    "            4. Check if there are more than three rows of data. If there are not, skip that Miller index combination\n",
    "            5. Check if the filtered df object passes the Overall_Decrease\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    filter_func=savgol_filter #Only tested for 'savgol_filter'\n",
    "    filter_params_func=savgol_params_func #Only tested for the previously defined function 'savgol_params_func'\n",
    "    min_points_required=3  #Minimum points needed for the filter to work (3 for Savitzky-Golay)\n",
    "    tolerance=1e-8 #Tolerance to decide if the filtered value is worth keeping\n",
    "    filter_column_idx=df.columns.get_loc('PolarizationD3')\n",
    "    time_column_idx=df.columns.get_loc('DeltaTime')\n",
    "    error_column_idx=df.columns.get_loc('ErrPolarizationD3')\n",
    "    new_column_name='SoftPolarizationD3'\n",
    "    folder_name = FileName.replace(\".fli\", \"\")   \n",
    "    \n",
    "    # Group by first three columns (Miller indices)\n",
    "    combination_counts = (\n",
    "        df.groupby([df.columns[0], df.columns[1], df.columns[2]])\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    log_message(f\"Analyzing combinations in file: {folder_name}_Array_{i}.fli\")\n",
    "    #Read the three numbers from the .fli file\n",
    "    for comb, count in combination_counts.items(): #This becomes a loop of just one combination\n",
    "        log_message(f\"Combination {comb} occurs {count} times in file {folder_name}.fli. Trying this combination\")\n",
    "        mask = (\n",
    "            (df.iloc[:,0] == comb[0]) &\n",
    "            (df.iloc[:,1] == comb[1]) &\n",
    "            (df.iloc[:,2] == comb[2]))\n",
    "        \n",
    "        PrettyCombination = format_combination(comb)\n",
    "        filtered_df = df.loc[mask].copy()\n",
    "        \n",
    "        # Requisites for the Combination to be valid:\n",
    "        # Requisite 1: Have data in the data\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      {PrettyCombination} has no data\")\n",
    "            continue\n",
    "        \n",
    "        # Requisite 2: Check if data column exists\n",
    "        if filtered_df.shape[1] <= filter_column_idx:\n",
    "            log_message(f\"      Expected column index {filter_column_idx} not found. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert to numeric all columns (all columns are considered as object type)\n",
    "        filtered_df = filtered_df.apply(pd.to_numeric, errors='coerce')\n",
    "        filtered_df = filtered_df.dropna()  # drops any rows with NaNs introduced by coercion in the last line\n",
    "        \n",
    "        # Check dtypes\n",
    "        all_numeric = all(dtype.kind in ('f', 'i') for dtype in filtered_df.dtypes)\n",
    "        \n",
    "        if all_numeric:\n",
    "            log_message(f\"      All columns have been successfully converted to numbers.\")\n",
    "        else:\n",
    "            log_message(f\"      Not all columns are numbers. Current dtypes:\")\n",
    "            log_message(f\"      {filtered_df.dtypes}\")\n",
    "            log_message(f\"      Expect Error Message from Python. Perhaps removing this file might be wise unless all files have the same issue\")\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      All rows dropped after conversion to numeric. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Requisite 3: Polarization is ALWAYS positive. If any is negative, that is not a polarization. Immediately sent to the Bad Files Folder\n",
    "        if (filtered_df.iloc[:, filter_column_idx] < 0).any():\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "            badfile_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "            filtered_df.to_csv(long_path(badfiles_txt_path), index=False, sep='\\t')\n",
    "            log_message(f\"      {PrettyCombination} has negative polarization values. Sent to BadFiles with name {filename}. Skipping to next Combination\")\n",
    "            continue\n",
    "\n",
    "        # Requisite 4: Have at least three rows (otherwise we can't teach the ML algorithm anything).\n",
    "        if len(filtered_df) < 2:\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "            badfile_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "            filtered_df.to_csv(long_path(badfiles_txt_path), index=False, sep='\\t')\n",
    "            log_message(\n",
    "                f\"      {PrettyCombination} has only {len(filtered_df)} rows (< {min_points_required}). \"\n",
    "                f\"Sent to BadFiles with name {filename}. Skipping to next Combination\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "\n",
    "        # \"Requisite 5\": Be worthy of having the filter used.\n",
    "        y = filtered_df.iloc[:, filter_column_idx].values\n",
    "        filter_params = filter_params_func(len(y))\n",
    "        try:\n",
    "            y_filtered = filter_func(y, **filter_params)\n",
    "            diff = np.abs(y - y_filtered)\n",
    "            changed_count = np.sum(diff > tolerance)\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "            if changed_count > 0:\n",
    "                log_message(f\"      Filter changed {changed_count}/{len(y)} points. Adding column '{new_column_name}'.\")\n",
    "            else:\n",
    "                log_message(f\"      Filter applied but data unchanged. Adding '{new_column_name}' as duplicated values.\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error applying filter to combination {comb}: {e}\")\n",
    "            log_message(f\"      Adding '{new_column_name}' as duplicated values to proceed anyway.\")\n",
    "            # Just duplicate the original column\n",
    "            y_filtered = y.copy()\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "        # Requisite 5: Add the Derivative and BandWidth filtering logic\n",
    "        return filtered_df, PrettyCombination\n",
    "\n",
    "    return None, None #As a Failsafe just in case someone uses two Miller index combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f5fa2-8276-4a06-a424-7c20aa20664f",
   "metadata": {},
   "source": [
    "## 4. Clean old files and force certain experiments\n",
    "\n",
    "Some parts of the code might use data from different sessions. It is safer to erase them and create all files from scratch everytime. This is not a big deal because this code file should only be run once unless the data base changes. It also clears all previous ML predictions to avoid mixing up information between experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fea540-2500-4f6f-b044-cdebdd94f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_erase = [\n",
    "    \"CrystallineLog_Predicting_Creation.txt\",\n",
    "    \"CrystallinePolariserAndAnalyser_IDs.txt\",\n",
    "    \"CrystallineSeparatedFolder\",\n",
    "    \"CrystallinePlotResults\",\n",
    "    \"CrystallineMLDataBase\",\n",
    "    \"PolarizationTimeReference.txt\",\n",
    "    \"CrystallineDataBase\",\n",
    "    \"CrystallineBadFiles\", \n",
    "    \"CrystallineFailuresTest\", \n",
    "    \"CrystallineCell_ID.txt\"\n",
    "]\n",
    "for item in to_erase:\n",
    "    path = os.path.abspath(item)  \n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                log_message(f\"Deleted file: {path}\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                log_message(f\"Deleted folder: {path}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\" Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"Not found (skipped): {path}\")\n",
    "        \n",
    "predict_path = os.path.abspath(os.path.join(\"..\", \"ML\", \"CrystallinePredictFiles\"))\n",
    "\n",
    "if os.path.exists(predict_path):\n",
    "    try:\n",
    "        if os.path.isdir(predict_path):\n",
    "            shutil.rmtree(predict_path)\n",
    "            log_message(f\"Deleted folder: {predict_path}\")\n",
    "        else:\n",
    "            os.remove(predict_path)\n",
    "            log_message(f\"Deleted file (unexpected, was not a folder): {predict_path}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Could not delete {predict_path}: {e}\")\n",
    "else:\n",
    "    log_message(f\"Not found (skipped): {predict_path}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc5535-d20f-4223-8a8d-fa4061250532",
   "metadata": {},
   "source": [
    "## 5. ZIP Folder Treatment and .fli data extraction\n",
    "\n",
    "The code will take all zipped folders from the folder _D3Files_ and prepare them to get their .fli files extracted.\n",
    "\n",
    "First, it will check if there are duplicate zip folders. To check it, it will compare the folder name and the hash sha256. Duplicate folders will be erased. For more information about hash sha256 check for example:\n",
    ">Wikipedia contributors. (2026, January 2). SHA-2. In Wikipedia, The Free Encyclopedia. Retrieved 10:49, January 17, 2026, from https://en.wikipedia.org/w/index.php?title=SHA-2&oldid=1330753870\n",
    "\n",
    "Second, it will copy the contents of the zipped folders and create a new folder with the name of the experiment inside _D3Files_. No more zipped folders are erased and information gets duplicated.\n",
    "\n",
    "Third, it will try to find all .fli files inside all the unzipped folders whether if they come from a zipped file or not. It will then send them to the newly created folder _CrystalineDataBase_. If, for each experiment proposal there are more than one .fli files, they get a numeric suffix ('_1', '_2',...) to distinguish them. Afterwards, all unzipped folders get erased leaving behind only the non-duplicated zipped folders.\n",
    "\n",
    "Note: All unzipped folders in D3Files will be explored, however they will get erased at the end of the pipeline. If you want them to persist for future runs of the code, they should be zipped first. For ILL users, when navigating the ILL Cloud, the easiest way to prepare the zip files is to download the _processed_ folder for each experiment proposal. The code is \"smart\" enough to only process .fli files with polarization information. Therefore, there is no need to manually prepare anything.\n",
    "To be precise, this cell of code will take all the zip files, extract them nd remove duplicates using the file name And the hash sha256. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198379e4-301e-4544-8f8f-0c701bc437d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_hash(filepath, algo=\"sha256\", block_size=65536):\n",
    "    \"\"\"Compute hash of a file (default SHA256).\"\"\"\n",
    "    h = hashlib.new(algo)\n",
    "    with open(long_path(filepath), \"rb\") as f:\n",
    "        for block in iter(lambda: f.read(block_size), b\"\"):\n",
    "            h.update(block)\n",
    "    return h.hexdigest()\n",
    "\n",
    "folder = Path(\"D3Files\")  \n",
    "zip_files = [f.name for f in folder.glob(\"*.zip\")] \n",
    "\n",
    "log_message(f\"Reading ZIP files. Checking for true duplicates by content...\")\n",
    "base_names = set()\n",
    "seen_hashes = {}\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    zip_path = folder / zip_file\n",
    "    name = Path(zip_file).stem\n",
    "    ext = Path(zip_file).suffix\n",
    "    filehash = file_hash(zip_path)\n",
    "\n",
    "    if filehash in seen_hashes:\n",
    "        log_message(f\"Duplicate confirmed by hash! Removing: {zip_file} (same as {seen_hashes[filehash]})\")\n",
    "    else:\n",
    "        seen_hashes[filehash] = zip_file\n",
    "        base_names.add(name)\n",
    "\n",
    "log_message(f\"\\n All duplicates (by content) removed. Begin unzipping...\\n\")\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\"\"\" UNZIPPING \"\"\"\n",
    "\n",
    "log_message(f\"Begin unzipping...\\n\")\n",
    "\n",
    "# Refresh zip_files list after removals\n",
    "zip_files = [f.name for f in folder.glob(\"*.zip\")]\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    zip_path = folder / zip_file\n",
    "    if zipfile.is_zipfile(long_path(zip_path)):\n",
    "        folder_name = sanitize(zip_file.stem if isinstance(zip_file, Path) else os.path.splitext(zip_file)[0])\n",
    "        extract_dir = folder / folder_name\n",
    "        log_message(f\"   Unzipping: {zip_file} -> {extract_dir}\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(long_path(zip_path), 'r') as zip_ref:\n",
    "                zip_ref.extractall(long_path(extract_dir))\n",
    "        except Exception as e:\n",
    "            log_message(f\"   WARNING: Error extracting {zip_file}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"   WARNING: Skipping invalid zip file: {zip_file}\")\n",
    "\n",
    "log_message(f\"\\nFinished Unzipping. Experiments stored in individual folders substituting the zip files\\n\")\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\"\"\" .fli FILE EXTRACTION \"\"\"\n",
    "\n",
    "source_folder = Path(\"D3Files\")\n",
    "database_folder = (Path.cwd() / \"CrystallineDataBase\").resolve()\n",
    "database_folder.parent.mkdir(parents=True, exist_ok=True)\n",
    "os.makedirs(long_path(database_folder), exist_ok=True)\n",
    "\n",
    "log_message(f\"\\nScanning all folders for .fli files...\\n \")\n",
    "for item in os.listdir(long_path(source_folder)):\n",
    "    item_path = source_folder / item  # Path object\n",
    "    if item_path.is_dir():\n",
    "        log_message(f\"   Processing folder: {item_path.name}\")\n",
    "        for root, dirs, files in os.walk(long_path(item_path)):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(\".fli\"):\n",
    "                    src_file = Path(root) / file\n",
    "                    dest_file = database_folder / file\n",
    "\n",
    "                    # Handle duplicate names\n",
    "                    counter = 1\n",
    "                    base_name = Path(file).stem\n",
    "                    ext = Path(file).suffix\n",
    "                    while (database_folder / f\"{base_name}_{counter}{ext}\").exists():\n",
    "                        counter += 1\n",
    "                    dest_file = database_folder / f\"{base_name}_{counter}{ext}\"\n",
    "\n",
    "                    log_message(f\"   Copying: {src_file} -> {dest_file}\")\n",
    "                    shutil.copy2(src_file, dest_file)\n",
    "\n",
    "\n",
    "        log_message(f\"   Deleting folder: {item_path}\")\n",
    "        shutil.rmtree(long_path(item_path))\n",
    "\n",
    "\n",
    "log_message(f\"\\nAll .fli files collected, sent from folder {source_folder} to folder {database_folder} . \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f15121-5aa8-4ed6-995b-f19ea01f7bee",
   "metadata": {},
   "source": [
    "## 6. Separation of .fli files according to experiments\n",
    "\n",
    "This code cell is different from all the other ones because **we request the user to give the file with only two rows per polariser cell used**\n",
    "\n",
    "We should expect something like this:\n",
    "\n",
    "\n",
    "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
    "|--|--|--|--|--|--|--|--|--|--|--|--|--|--|\n",
    "polariser cell info |ge18004 |pressure/init. |polar |2.29 |0.79 |initial |date/time |17 |09 |23 |@ |10:39\n",
    "|37391 |4.000 |0.000 |1.000 |18/09/23 |06:20:44 |155.03 |+z |+z |0.8391 |0.0156 |11.4270 |1.2031 |120.00\n",
    "|37417 |4.000 |0.000 |1.000 |18/09/23 |10:31:59 |155.29 |+z |+z |0.8120 |0.0187 |9.6406 |1.0613 |120.00\n",
    "polariser cell info |ge18012 |pressure/init. |polar |2.27 |0.79 |initial |date/time |18 |09 |23 |@ |10:33\n",
    "|37418 |4.000 |0.000 |1.000 |18/09/23 |10:37:52 |155.29 |+z |+z |0.9101 |0.0107 |21.2483 |2.6375 |120.00\n",
    "|37434 |4.000 |0.000 |1.000 |18/09/23 |14:16:07 |155.33 |+z |+z |0.8784 |0.0129 |15.4409 |1.7409 |120.00\n",
    "polariser cell info |ge18004 |pressure/init. |polar |2.28 |0.79 |initial |date/time |22 |09 |23 |@ |09:45\n",
    "|37462 |4.000 |0.000 |1.000 |22/09/23 |10:06:36 |0.00 |+z |+z |0.8670 |0.0427 |14.0333 |4.8278 |10.00\n",
    "|37521 |4.000 |0.000 |1.000 |23/09/23 |09:51:06 |0.00 |+z |+z |0.7598 |0.0211 |7.3276 |0.7333 |120.00\n",
    "|  ...  |       |       |       |          |          |      |        |        |        |        |        | | |\n",
    "\n",
    "You can have as many sets of three lines as you desire, but they have to be a header and two regular rows per experiment. \n",
    "### How to prepare the files\n",
    "1. Take the raw .fli file and open it in a text reader app (The Note Bloc in Windows opens them like .txt files so it works). \n",
    "2. Choose a Miller index combination and find the first row that has as polarization directions (+z,+z)\n",
    "3. Find the last appearance of that Miller index combination with polarization direction (+z,+z)\n",
    "4. Erase all but those two lines and keep the header too.\n",
    "5. Repeat for every cell used (for every 'polariser cell info' row)\n",
    "\n",
    "The reason why this process has not been automated is to allow the user more freedom when predicting. Perhaps an automatic routine will not take into account what Miller index combination is adequate. Also, the user might have files that have already the correct structure and don't need the whole pipeline. Either way, if the ILL staff wants this automated routine (using the same logic as the other Reading.ipynb files) please contact Gonzalo and he will happily help you.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "An explanation of the information that all the contents in the .fli files give can be found here: \n",
    "1. 'polariser cell info' (str): Log of the installation of the polariser cells\n",
    "2. 'PolariserID' (str): A string with the type of cell used\n",
    "3. 'pressure/init. polar' (str): A string to introduce the $^\\mathrm{3}$He gas pressure and the polarization measured at the creation lab.\n",
    "4. 'PolariserPressure' (float): $^\\mathrm{3}$He gas pressure in some units\n",
    "5. 'InitialLabPolarization' (float): Polarization measured at the creation lab\n",
    "6. 'initial date/time' (str): A string that introduces the day, month and year and the hour and minutes.\n",
    "7. 'Date' (str): A string with the information DD/MM/YY\n",
    "8. '@' (str): A string to separate date and time\n",
    "9. 'time' (str): A string with the information HH:MM\n",
    "\n",
    "And for the rest of the rows:\n",
    "1. 'Measurement number' (int): The number index of the measurement.\n",
    "2. 'First_Miller_Index' (float): The first Miller index of the crystal. Polarization is measured using a known Si Bragg crystal. For the source of the origin of the Si crystal see:\n",
    ">Stunault, Anne & Vial, S & Pusztai, Laszlo & Cuello, Gabriel & Temleitner, László. (2016). Structure of hydrogenous liquids: separation of coherent and incoherent cross sections using polarised neutrons. Journal of Physics: Conference Series. 711. 012003. 10.1088/1742-6596/711/1/012003. \n",
    "3. 'Second_Miller_Index' (float)\n",
    "4. 'Third_Miller_Index' (float):\n",
    "5. 'Date' (str): A string with the information DD/MM/YY of that measurement\n",
    "6. 'time' (str): A string with the information HH:MM:SS of that measurement\n",
    "7. Temperature \n",
    "8. 'Direction_1' (str): It is the direction of the polarization after the monochromator.The direction +z corresponds to the orthogonal with respect to the floor pointing away from it. +x is the direction of the beam (variable) and +y is the orthogonal (positive orthonormal basis) direction to +z and +x. \n",
    "8. 'Direction_2' (str): The direction of the polarization at the sensors. True polarization measurements are done **only** on the (+z,+z) direction.\n",
    "\n",
    "8. 'D3Polarization' (float): A float with the polarization measurement\n",
    "9. 'ErrD3Polarization' (float): A float with the uncertainty of that polarization measurement\n",
    "10. 'FlippingRatio' (float): The flipping ratio. Given either the flipping ratio or the polarization value, the other one is fully determined. Therefore, only one is needed and that is why we don´t work with the flipping ratio\n",
    "11. 'ErrFlippingRatio' (float): The uncertainty of the flipping ratio\n",
    "12. 'Elapsed time' (float): It is the time used to obtain the measurement (integration of the beam over that number of seconds)\n",
    "\n",
    "Temperature did not seem to have an effect on the decay. Therefore, it has been eliminated in this code cell. Here is a summary of what the code does:\n",
    "\n",
    "1. The code will go through the .fli files and find all rows with 'polariser cell info'. A cell change is considered once a new 'polariser cell info'. At the moment it ignores the experiments that use the 'magical box' as we are not sure if they are experiments compatible with the ones studied here.\n",
    "2. For evey cell change, a new .fli file is created storing all the information including the header row and the measured data rows (in this case, just two). Also, all cell IDs are recorded\n",
    "3. For all .fli files the code now will:   \n",
    "\n",
    "    3.1 Remove unwanted rows (hopefully none)\n",
    "    \n",
    "    3.2 It removes any rows that don´t have polarization directions (+z,+z). (Should remove none if done correctly).\n",
    "    \n",
    "    3.3 Extract data form the header row.\n",
    "    \n",
    "    3.4 Remove unwanted columns (temperature, flipping ratio, counts, elapsed time,...).\n",
    "    \n",
    "    3.5 Set a time reference with the first measurement row. All other time values get referenced with respect to this moment in time and converted into seconds.\n",
    "    \n",
    "    3.6 Ignore all Miller index combinations that are not integers (hopefully no issues here).\n",
    "    \n",
    "    3.7 Save two files for each experiment. One with the header rows and another one with just the numeric rows (with a new header that explains what each column has).\n",
    "\n",
    "For every succesful experiment we will output:\n",
    "1. Image:  **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Multiplier={Multiplier}.png\"** in PlotResults. Shows the plot with the extended area with the raw data\n",
    "2. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Multiplier={Multiplier}\\_Soft.png\"** in PlotResults. Shows the plot with the extended area with the filtered data\n",
    "3. Image: **\"PolarizationD3\\_{folder_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Filtered.txt\\_plot\\_Derivatives.png\"** in PlotResults. Shows the evolution of the \"derivatives\". I apologize for the hideous names. Unless it results in a fatal error, I am scared to change the code.\n",
    "4. Image: **\"PolarizationD3\\_{folder_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex_{PrettyCombination}\\_Filtered.txt\\_N\\_{N}\\_ManualInterval.png\"** in PlotResults. Shows the plot with the non-exteded area\n",
    "5. Txt: **\"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}.txt\"** in MLDataBase. It contains the four data columns (DeltaTime, PolarizationD3, SoftPolarizationD3, ErrPolarizationD3)\n",
    "6. Txt: **\"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\"** in MLDataBase. It contains the parameters (CellID, Pressure, LabPolarization, LabTime)\n",
    "\n",
    "\n",
    "The plots are not necessary but are saved for the user to know what all the files look like. The txt files are fundamental for the rest of the pipeline. \n",
    "The files that are wrong or useless when all is done are the folowing. They are kept for  debug purposes (to see files with differents structures, why they fail,etc).\n",
    "\n",
    "1. Txt: **\"{folder\\_name}\\_Arrays\\_{i}.txt\"** in SeparatedFolder/{folder_name}. It still has the header and useless columns. It is the fli file of evey chunk, of every recorded experiment (correct or incorrect)\n",
    "2. Txt: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}.txt\"** in SeparatedFolder/{folder_name}. It is the same as the one in MLDataBase (a duplicate)\n",
    "3. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}.png\"** in SeparatedFolder/{folder_name}. It plots (with error bars) PolarizationD3\n",
    "4. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Combined.png\"** in SeparatedFolder/{folder_name}. It plots (with error bars) both PolarizationD3 and SoftPolarizationD3\n",
    "5. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Softened.png\"** in SeparatedFolder/{folder_name}. It plots (with error bars) SoftPolarizationD3\n",
    "6. Folder: **\"FailuresTest\"** contains all the graphs of the data sets that were considered not worthy but had more points that the ones saved. Check them if your experiment was not properly added\n",
    "7. Folder: **\"DataBase\"** has the raw fli files. Once the code has been used they are no longer important (if you don't find the folder I may have added a line of code to erase it. Sorry in advance for any inconveniences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1d542-7fb7-4257-8b39-7c1138548741",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataBase = Path('CrystallineDataBase')\n",
    "output_base = Path('CrystallineSeparatedFolder')\n",
    "\n",
    "# List all .fli files in that folder, prepare folders\n",
    "FileNameList = [f.name for f in DataBase.glob('*.fli')]\n",
    "polyorder = 2\n",
    "default_window_length = 5\n",
    "SeparatedFolder = Path(\"CrystallineSeparatedFolder\")\n",
    "BadFilesFolder = Path(\"CrystallineBadFiles\")\n",
    "MLDataBaseFolder = Path(\"CrystallineMLDataBase\")\n",
    "BadFilesFolder.mkdir(exist_ok=True, parents=True)\n",
    "MLDataBaseFolder.mkdir(exist_ok=True, parents=True)\n",
    "log_message(f\"\\n\\nFiles in the data base that will be (tried) to be used\\n {FileNameList}\\n\")\n",
    "\n",
    "for FileName in FileNameList:\n",
    "    \"\"\"READ THE FILE AND SEPRATE IT INTO EACH EXPERIMENT USING THE POLARIZATION CELL\"\"\"\n",
    "    # 1- Open file\n",
    "    folder_name = FileName.replace(\".fli\", \"\")\n",
    "    output_folder = output_base / folder_name\n",
    "    file_path = DataBase / FileName\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(long_path(file_path), \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    \n",
    "    # 2- Locate the header with CellID, Pressure, etc. Chunks are the data rows sandwiched between two 'polariser cell info' strings\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    started = False  \n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"polariser cell info\"):  # All before polariser cell info will be forgotten\n",
    "            if started and current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = [line]\n",
    "            started = True\n",
    "        else:\n",
    "            if started:\n",
    "                current_chunk.append(line)\n",
    "\n",
    "    if not started:\n",
    "        log_message(f\" File '{FileName}' does NOT contain any 'polariser cell info' header. Skipping.\\n\")\n",
    "        continue\n",
    "    else:\n",
    "        log_message(f\" File '{FileName}' contains at least one 'polariser cell info' header.\")\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    #3- Save .fli files for every correct chunk\n",
    "    base_name = FileName.replace(\".fli\", \"\")  # remove .fli for clean filenames\n",
    "    log_message(f\"\\n\\nCreating all the Array files \\n\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        log_message(chunk)\n",
    "        fli_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "        fli_path = output_folder / fli_filename\n",
    "        with open(fli_path, \"w\") as f_out:\n",
    "            f_out.writelines(chunk)  \n",
    "\n",
    "    # 4- As CellID can be exchanged with real parameters, it is written in an independent file\n",
    "    cell_id_file = Path.cwd() / \"Crystalline_CellID.txt\"\n",
    "    try:\n",
    "        with open(long_path(cell_id_file), 'r', encoding='utf-8') as file:\n",
    "            seen_strings = set(line.strip() for line in file)\n",
    "    except FileNotFoundError:\n",
    "        seen_strings = set()\n",
    "    \n",
    "    # 5- Open each Array file and work with it (The Array file still has the header)\n",
    "    with open(long_path(cell_id_file), 'a', encoding='utf-8') as file:\n",
    "        for i in range(len(chunks)):\n",
    "            FLI_filename = f\"{base_name}_Arrays_{i}.fli\"  \n",
    "            FLI_path = output_folder / FLI_filename  \n",
    "            if not FLI_path.exists():\n",
    "                log_message(f\"WARNING: Array file does not exist: {FLI_path}\")\n",
    "                continue\n",
    "            df = pd.read_csv(long_path(FLI_path), sep=r'\\s+', header=None, on_bad_lines='skip')  # Read file\n",
    "            log_message(f\"Reading {FLI_path}, removing ***WARNING No centering scan found \")\n",
    "            warning_str = \"***WARNING No centering scan found\"\n",
    "\n",
    "            #5.1 Combine first 4 columns as strings, join them with space, and filter rows containing this phrase (it is not important for us)\n",
    "            df = df[~df.iloc[:, :5].astype(str).agg(' '.join, axis=1).str.contains('No centering scan found', regex=False)] \n",
    "            \n",
    "            #5.2 Extract useful information from the header. Hopefully, CellID, Pressure, LabPolarization, Year, Month, Day, time of lab measurement before first experiment measurement (negative time) will be stored locally\n",
    "            log_message(f\"Header Information Extraction...\")\n",
    "            CellID =          df.iloc[0].tolist()[3]\n",
    "            Pressure =        df.iloc[0].tolist()[6]\n",
    "            LabPolarization = df.iloc[0].tolist()[7]\n",
    "\n",
    "            try:\n",
    "                HM, DD, MM, YY = df.iloc[0].tolist()[14], int(df.iloc[0].tolist()[10]), int(df.iloc[0].tolist()[11]), int(df.iloc[0].tolist()[12])\n",
    "                Day_Ref = f\"{DD:02d}/{MM:02d}/{YY:02d}\"\n",
    "                dt = Time(Day_Ref, HM)\n",
    "            except Exception as e:\n",
    "                log_message(f\"Skipping file {file_path} because of invalid header data: {e}\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            #5.3 All redundant/useless information is removed\n",
    "            log_message(f\"Removing Measurement Index, Temperature, Flipping Ratio, Uncertainty of Flipping Ratio and Time between measurements,...\")\n",
    "            df = df.iloc[1:].reset_index(drop=True)\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "            df = df.drop(df.columns[5], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            log_message(f\"Saving only polarization values for the Spin Directions wanted in both Polarizer Cells, i.e. (+z,+z)\")\n",
    "\n",
    "            #5.4 Keep only rows where both are +z\n",
    "            df = df[(df[7] == '+z') & (df[8] == '+z')].copy()\n",
    "            if df.empty:\n",
    "                log_message(f\"No valid '+z' rows in file {FileName}_Arrays_{i}.fli, skipping\")\n",
    "                continue  \n",
    "            df = df.drop(df.columns[[5,6]], axis=1, errors='ignore')\n",
    "            #5.5 Convert Miller index columns into integers. From string or object to float and if the float is close to an integer (tolerance is 1e-8) then save as integer. Otherwise remove row\n",
    "            cols_to_convert = [1, 2, 3]\n",
    "            df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce').astype(float)            \n",
    "            mask = np.isclose(df[cols_to_convert], np.round(df[cols_to_convert]), atol=1e-8)\n",
    "            df = df[mask.all(axis=1)].copy()\n",
    "            log_message(f\"All Spin directions removed. All irrational Miller Indices removed. Adding DeltaTime\")\n",
    "            \n",
    "            #5.6 The time columns are converted into difference of time being the referenced time the first +z,+z measurement that has survived at this point\n",
    "            if df.shape[0] < 2:\n",
    "                log_message(f\"Not enough valid rows after filtering, skipping chunk\")\n",
    "                continue\n",
    "            df['DeltaTime'] = df.apply(\n",
    "                lambda row: deltatime(df[4].iloc[0], df[5].iloc[0], row[4], row[5]), axis=1 )\n",
    "            ref_dt = Time(df[4].iloc[0], df[5].iloc[0])\n",
    "            LabTime = int((dt - ref_dt).total_seconds())\n",
    "            with open(\"PolarizationTimeReference.txt\", \"a\") as f:\n",
    "                f.write(str(ref_dt) + \"\\n\")\n",
    "\n",
    "            #5.6 Rename the columns PolarizationD3, ErrPolarizationD3 (the polarization column and its uncertainty). The other one with name is DeltaTime. The rest are numbers (will be erased).\n",
    "            #Also we remove the time strings (with DeltaTime they have no new information)\n",
    "            log_message(f\"Renaming PolarizationD3 and ErrPolarizationD3\")\n",
    "            df.rename(columns={\n",
    "                df.columns[5]: 'PolarizationD3',\n",
    "                df.columns[6]: 'ErrPolarizationD3'\n",
    "            }, inplace=True)\n",
    "            df.drop(columns=[df.columns[3], df.columns[4]], inplace=True)\n",
    "            log_message(f\"Dropped Time Strings\")\n",
    "\n",
    "            \n",
    "            #5.7 Begin filtering and softening with previous functions\n",
    "            log_message(f\"Begin removal of Bad files and softening with Savitzky-Golay filter\")\n",
    "            filtered_df, PrettyCombination = filter_best_combination(i,df)\n",
    "            #If nothing survived the filters/purge then use 'continue' and go for the next experiment\n",
    "            if filtered_df is None and PrettyCombination is None:\n",
    "                log_message(f\"Chunk {i}: No suitable combination found. Perhaps, check the file again to see what  could have gone wrong or check the log to see why it has been discarded. Skipping to next chunk or file.\")\n",
    "                log_message(f\"_______________________________________________________________\\n\")\n",
    "                continue  # skip to next chunk\n",
    "            \n",
    "            #5.8 Removal of Miller indices (we have all the information they could give us)\n",
    "            log_message(f\"Removing Miller Indices columns\")\n",
    "            filtered_df = filtered_df.iloc[:, 3:]\n",
    "            desired_order = [\"DeltaTime\", \"PolarizationD3\", \"SoftPolarizationD3\", \"ErrPolarizationD3\"]\n",
    "\n",
    "            \n",
    "            # 5.9 Remove the points that won't be useful for the ML algorithm\n",
    "            columns_to_save = [col for col in desired_order if col in filtered_df.columns]  # Keep only the columns that exist\n",
    "            df_SEMIFINAL = filtered_df[columns_to_save].copy()\n",
    "            df_FINAL = filtered_df = df_SEMIFINAL\n",
    "            \n",
    "            \n",
    "            # 5.10 Save the files\n",
    "            log_message(f\"Finally we save the chunk\")\n",
    "            csv_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}.txt\"\n",
    "            Parameter_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\"\n",
    "            csv_path = output_folder / csv_filename\n",
    "            csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            df_FINAL.to_csv(long_path(csv_path), index=False, sep=',')\n",
    "            \n",
    "            ml_txt_path = MLDataBaseFolder / csv_filename\n",
    "            ml_txt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            df_FINAL.to_csv(long_path(ml_txt_path), index=False, sep=',')\n",
    "            log_message(f\"Saved: {csv_filename}\")\n",
    "            log_message(f\"Processing Parameter file...\")\n",
    "            ml_param_path = MLDataBaseFolder / Parameter_filename\n",
    "            ml_txt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            with open(long_path(ml_param_path), 'w', encoding='utf-8') as f:\n",
    "                f.write(\"CellID,Pressure,LabPolarization,LabTime\\n\")\n",
    "                f.write(f\"{CellID},{Pressure},{LabPolarization},{LabTime}\")\n",
    "            log_message(f\"Saved: {Parameter_filename}\")\n",
    "            log_message(f\"Parameter and Array files saved to ML database: {MLDataBaseFolder}\\n_______________________________________________________________\\n\\n\")\n",
    "        else:\n",
    "            log_message(f\"{FileName}_Arrays_{i}.fli is empty, skipping this file.\\n\")\n",
    "            continue  \n",
    "            \n",
    "    #5.12 Remove unwanted folders and files\n",
    "    for i in range(len(chunks)):\n",
    "        temp_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "        temp_path = output_folder / temp_filename\n",
    "        try:\n",
    "            temp_path.unlink()\n",
    "        except FileNotFoundError:\n",
    "            pass        \n",
    "    log_message(f\"Created and saved {len(chunks)} CSV files from file called {FileName}.\")\n",
    "    if output_folder.exists() and not any(output_folder.iterdir()):\n",
    "        output_folder.rmdir()\n",
    "        log_message(f\"Removed empty folder: {output_folder}\")\n",
    "    log_message('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cabc9e-ce16-436a-a99a-e028d60078b1",
   "metadata": {},
   "source": [
    "## 7. CellID File processing\n",
    "\n",
    "We need to save the CellID types without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f7936-1d25-4259-95a2-67447178f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_database_folder = Path(\"CrystallineMLDataBase\")\n",
    "parameter_files = list(ml_database_folder.glob('*Parameters.txt'))\n",
    "log_message(f\"Found {len(parameter_files)} parameter files.\")\n",
    "unique_cell_ids = []\n",
    "seen = set()\n",
    "\n",
    "for filepath in parameter_files:\n",
    "    try:\n",
    "        with open(long_path(filepath), 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 2:\n",
    "                second_row = lines[1].strip()\n",
    "                parts = second_row.split(',')\n",
    "                if parts:\n",
    "                    cell_id = parts[0]\n",
    "                    if cell_id not in seen:\n",
    "                        seen.add(cell_id)\n",
    "                        unique_cell_ids.append(cell_id)\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to read {filepath}: {e}\")\n",
    "\n",
    "# Write to Crystalline_CellID.txt\n",
    "cellid_file = Path.cwd() / \"Crystalline_CellID.txt\"\n",
    "with open(long_path(cellid_file), \"w\", encoding='utf-8') as f:\n",
    "    for cell_id in unique_cell_ids:\n",
    "        f.write(f\"{cell_id}\\n\")\n",
    "\n",
    "log_message(f\"Saved {len(unique_cell_ids)} unique cell IDs to {cellid_file.name}.\")\n",
    "\n",
    "# Remove the separated folder\n",
    "folder_to_delete = Path.cwd() / \"CrystallineSeparatedFolder\"\n",
    "if folder_to_delete.exists():\n",
    "    shutil.rmtree(long_path(folder_to_delete))\n",
    "    log_message(f\"Folder '{folder_to_delete}' has been deleted.\")\n",
    "else:\n",
    "    log_message(f\"Folder '{folder_to_delete}' does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc861e-b89b-4076-b656-0cb8d5378a06",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "It erases all intermediate files and prepares the remaining ones for the ML pipeline\n",
    "\n",
    "1. Removes all .fli files that have been created.\n",
    "2. Removes empty folders\n",
    "3. Collects all unique polariser–analyser ID pairs\n",
    " \n",
    "As a result, the only useful files are _Crystalline_CellID.txt_ and the folder _CrystallineMLDataBase_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c7bd5-be82-47cf-8d94-ee6e9b535ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hash_map = defaultdict(list)\n",
    "\n",
    "def file_sha256(filepath, block_size=65536):\n",
    "    \"\"\"Compute SHA256 hash of a file (safe for large files).\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(long_path(filepath), \"rb\") as f:\n",
    "        while chunk := f.read(block_size):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "# Scan all .txt files (only base files without '_Parameters')\n",
    "for root, _, files in os.walk(long_path(ml_database_folder)):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".txt\") and \"_parameters\" not in file.lower():\n",
    "            path = Path(root) / file\n",
    "            file_hash = file_sha256(path)\n",
    "            hash_map[file_hash].append(path)\n",
    "\n",
    "# Report & delete duplicates\n",
    "duplicates_found = False\n",
    "for file_hash, paths in hash_map.items():\n",
    "    if len(paths) > 1:\n",
    "        duplicates_found = True\n",
    "        log_message(f\"\\nDuplicate group (hash={file_hash}):\")\n",
    "        log_message(f\"   Keeping: {paths[0]}\")\n",
    "\n",
    "        # All but the first are duplicates\n",
    "        for p in paths[1:]:\n",
    "            base_name, ext = os.path.splitext(p)\n",
    "            param_file = Path(f\"{base_name}_Parameters{ext}\")\n",
    "\n",
    "            try:\n",
    "                os.remove(long_path(p))\n",
    "                log_message(f\"   Deleted duplicate base file: {p}\")\n",
    "            except Exception as e:\n",
    "                log_message(f\"   Could not delete base file {p}: {e}\")\n",
    "            if param_file.exists():\n",
    "                try:\n",
    "                    os.remove(long_path(param_file))\n",
    "                    log_message(f\"   Deleted parameter file: {param_file}\")\n",
    "                except Exception as e:\n",
    "                    log_message(f\"   Could not delete parameter file {param_file}: {e}\")\n",
    "\n",
    "if not duplicates_found:\n",
    "    log_message(\"No duplicates found in MLDataBase!\")\n",
    "else:\n",
    "    log_message(\"\\n Duplicate cleanup complete!\")\n",
    "    \n",
    "    \n",
    "# Define paths relative to the notebook location\n",
    "current_dir = Path().resolve()  \n",
    "ml_database = current_dir / \"CrystallineMLDataBase\"\n",
    "predict_files = current_dir.parent / \"ML\" / \"CrystallineToPredictFiles\"\n",
    "\n",
    "# Make sure the destination exists\n",
    "predict_files.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for item in predict_files.iterdir():\n",
    "    if item.is_file():\n",
    "        os.remove(item)\n",
    "        log_message(f\"Deleted existing file: {item}\")\n",
    "    elif item.is_dir():\n",
    "        shutil.rmtree(item)\n",
    "        log_message(f\"Deleted existing folder: {item}\")\n",
    "\n",
    "for item in ml_database.iterdir():\n",
    "    dest = predict_files / item.name\n",
    "    shutil.move(long_path(item), long_path(dest))\n",
    "    log_message(f\"Moved {item.name} -> {predict_files}\")\n",
    "\n",
    "try:\n",
    "    ml_database.rmdir()\n",
    "    log_message(f\"Removed empty folder: {ml_database}\")\n",
    "except OSError:\n",
    "    log_message(f\"Could not remove {ml_database}, not empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342611a-3cde-49e2-805f-39c1b012c624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
