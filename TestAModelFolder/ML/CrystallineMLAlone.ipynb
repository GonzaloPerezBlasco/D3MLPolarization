{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c15272a9-ef9c-4e9e-b7ac-9d93ad331454",
   "metadata": {},
   "source": [
    "<H1> CrystallineMLAlone.ipynb </H1>\n",
    "\n",
    "\n",
    "\n",
    "**Warning. When loading a single experiment and Hot-encoding CellID the program won't know it it should encode one or three values. (brings up an error of shape mismatch)\n",
    "WARNING VERY VERY IMPORTANT, IF A NEW CELL IS ADDED, TWO EXPERIMENTS WITH THAT CELL ARE REQUIRED OR HOT ENCODING WILL BREAK (AND THE PROGRAM WON'T PREDICT WITH ZERO EXAMPLES OF THAT CELL)**\n",
    "\n",
    "This code requires that CrystallineFileLectureTests.ipynb has been run. As the data base is extremely small, the effects of overfitting and underfitting are very prone to happening. To test if a model performs as expected it should try to predict files that have not been used in the process of training and validation. As we can't extract too many files form the data base, the solution is to extract only one experiment, train a model with a given structure with all the other experiments, try to predict this isolated experiment (it is a blind prediction where we know what the output looks like) and repeat this entire process for all experiments. To compare the results of different models please use the code files inside LTestVisualCheck\n",
    "\n",
    "__________________________________________________________________________________________\n",
    "\n",
    "OUTPUTS OF THE CODE: \n",
    "\n",
    "1. **CrystallineLog\\_Testing\\_ML.txt**\n",
    "A log file with every step that the algorithm has followed\n",
    "\n",
    "\n",
    "2. **CrystallineExecution\\_times.txt**\n",
    "It times how long the code took to loop for each model to loop over all the isolation \n",
    "\n",
    "\n",
    "3. **CrystallineAllTestsFolder\\_{Complexity}\\_{num_augmentations}** \n",
    "For each Complexity and each num_augmentations (a.k.a, for each model structure and data organization) a folder is created. Inside there are folders for each isolated experiment iteration\n",
    "\n",
    "    3.1 Missing{base\\_name}\n",
    "For each experiment in the data base (CrystallineMLDataBase) a folder is created and inside there are all the files that the code has created when training and predicting\n",
    " \n",
    "        3.1.1 Difference\\_{base\\_name}.jpg\n",
    "For each experimental point, we obtain the prediction and we obtain the difference between prediction and experimental values. It may be helpful to detect an overall deficiency when predicting\n",
    "\n",
    "        3.1.2 Difference\\_{base\\_name}.txt\n",
    "Has the same information as the last .jpg but written on a .txt file\n",
    "\n",
    "        3.1.3 Missing\\_{base\\_name}.jpg\n",
    "This is the important graph. It shows in black the experimental values with their uncertainty, in red the pure ML predictions (without the correction) with the uncertainty bands in a transparent red, in green the final predictions (ML + linear correction) with uncertainty as the green transparent band and in blue the corrected predictions for the time points where experimental values are known\n",
    "\n",
    "        3.1.4 PredictedData\\_PolarizationD3\\_Missing{base\\_name}.txt\n",
    "The green part of the graph but in a .txt file\n",
    "\n",
    "        3.1.5 PredictedPoints\\_PolarizationD3\\_Missing{base\\_name}.txt\n",
    "The blue part of the graph but in a .txt file\n",
    "\n",
    "        3.1.6 RawData\\_PolarizationD3\\_Missing{base\\_name}.txt\n",
    "The black part of the graph but in a .txt file. It is supposed to be a duplicate of the file in CrystallineMLDataBase but it is stored to make sure that the scaling process is being done accurately (if it doesn't fit, be very very careful)\n",
    "\n",
    "        3.1.7 model_PolarizationD3.keras\n",
    "The model used in this isolated-experiment iteration\n",
    "\n",
    "        3.1.8 scaler_static_PolarizationD3.pkl\n",
    "The scaler for the static parameters (initial and final polarizations included) used in this isolated-experiment iteration\n",
    "\n",
    "        3.1.9 scaler_time_PolarizationD3.pkl\n",
    "The scaler for the time evolution used in this isolated-experiment iteration\n",
    "\n",
    "        3.1.10 scaler_y_PolarizationD3.pkl\n",
    "The scaler for the polarization values used in this isolated-experiment iteration\n",
    "\n",
    "\n",
    "Here are all funtions and a overall guide of what the code does:\n",
    "\n",
    "    1. Define the model and decide the number of augmentations\n",
    "    1. Creates the folder where the isolated files will temporally reside\n",
    "    2. Loops for all pairs of array and parameter files\n",
    "    3. Moves both, parameter and array, files to the isolated directory\n",
    "    4. A subfolder on the main folder gets created with the name of the isolated experiment. All results will be stored here for this isolation iteration.\n",
    "    5. Loop for all polarization modes to be processed. It can be SoftPolarizationD3, PolarizationD3 or both\n",
    "    6. Load the rest of the files and augment them\n",
    "    7. Obtain the time and polarization scalers. They are obtained using all time values and all polarization values including the soon-to-become static features\n",
    "    8. Obtain the static features scaler.\n",
    "    9. Save scalers locally\n",
    "    10. Train the model. The data gets scaled during this step.\n",
    "    11. Load the isolated experiment and align both sets of data (use align_static_vectors)\n",
    "    12. Build manually the dataset for the isolated experiment\n",
    "    13. Extract the initial and final points, add them as parameters and scale the entire static vector with the general scaler.\n",
    "    14. Scale time and polarization arrays (only intermediate points). This is compatible with the scaling done to the rest of the experiments.\n",
    "    15. Combine all structures so that the shape is correct for prediction.\n",
    "    16. Check for shapes and create the final NumPy arrays of the experiment\n",
    "    17. Extract the initial and final polarizations and keep a scaled and unscaled version of all. Then predict at those time points. \n",
    "    18. Prepare the linear correction and predict in the time points where we have data. The unscaled (physical units) outputs are pred_polar and pred_sigma.\n",
    "    19. Plot the raw values (black), the predictions for those time points (blue) and predictions every 1000 seconds (red if there is no correction, green if there is correction)\n",
    "    20. Save data\n",
    "    21. Move all experiments back to the original folder\n",
    "\n",
    "\n",
    "1. **load\\_experiments**. It uses the directory where the array (numerical values) and parameter files resides, picks one polarization column and prepares a list of experiments to feed the ML algorythm. The output is as follows:\n",
    "    \n",
    "    *encoded\\_experiments=[(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 1}$,(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 2}$,...]* \n",
    "    Note that the Cell Id is Hot Encoded. The type of cell did not affect greatly the predictions. However in the future, it may be useful to give more information to Cell_ID. As of 2025, these strings are Hot Encoded which means that the code finds all the different types, creates columns for each type and writes 0 or 1 (bool) depending of whether the cell was from one type or the other. This is the standard procedure to feed categorical variables to ML.\n",
    "    The size of the output list depends on the number of pairs of .txt files present in the folder. This means that it also works for isolated experiments\n",
    "    \n",
    "2. **augment_experiments** is a function that takes the original data list (as load_experiments returns it) and augments them _num\\_augmentations_ times. The final legth of the list will be (num_augmentations +1) times the original length. Due to the reduced size of the data base (Not more than 30 experiments before 2026), techniques like augmentation are required. Due to the stochastic nature of neutron detection, most measurements, if repeated under the same conditions, will yield different results. Of course, all measurements converge (with uncertainty) to what we can say is \"the true value\". Therefore, we can duplicate the experiments adding noise to the measurements to obtain \"hypothetical\" measurements that expand the data base. It was decided that the uncertainty won't be modified. To decide what type of noise could be applied two possibilities were considered. The first one was to suppose that sensor detection follows a Poisson distribution (law of rare events). The second one was to suppose that it follows a Normal Distribution of mean the measured value and width the uncertainty. As a poissonian distribution converges stochastically (in probability and distribution but not almost sure nor in L^p) to a normal distribution, it is safe to assume that they converge to the same result so the gaussian approach was selected. Also, the measurements are not raw counts but a function of them.\n",
    "$P=\\frac{n^+-n^-}{n^++n^-}$\n",
    "We have no direct data of the number of counts (the parameter of a Posissonian distribution) but a rough estimate points to them being (worst case scenario) within the order of the millions (After 100 counts, poisson distributions vary very little from normal distributions). Threfore, count detection can be approximated to a normal distribution and a linear combination of random variables distributed as normal distributions is also a random variable with a normal distribution. Therefore, it is safe to assume that sensor measurements can be modelled after a gaussian distribution.\n",
    "\n",
    "\n",
    "3. **build_dataset** is a function that prepares the data base to be fed directly into a ML model for training and validation. There are a few import decisions taken here. The way this function is set up, it removes 2 rows of data per polariser cell studied. The reason why it was done is because we want the ML model to be able to predict polarization decay when given the specs of the cell (the static parameters) and the initial and final polarization values (with their associated time values). The reason why those two values are considered \"known values\" for each cell is beacuse they can be easily measured experimentally and give as a good estimate about the overall behaviour. In some experiments, the environment of the studied sample is too fragile to move and place the Si crystal for polarization measurements. Therefore, a working ML algorithm whose inputs are the specs of the cell and the initial and final polarizations (measured before and after the sample is in place) would enable experiments that could not be done before without proper polarization efficiency corrections. \n",
    "Also, we remove those two values per polariser cell from the training arrays (Xt, y and err). The reason why it is done is to avoid data leaks in the model. If we give those values as training data and also give them as parameters, the ML algorithm can run into the risk of memorizing those pairs (over-fitting) and worsen any new predicitions. Uncertainties for the first and last polarization measurements are not added to the static features. This was a decision taken to avoid giving too much weight to two variables that don't have value on their own (they compliment the polarization values but if the ML architecture is as shallow as\n",
    "the one used here, they can be considered independent variables and reduce the weight and importance of the other variables). \n",
    "Another consideration taken here was that the static features get duplicated in Xs a lot of times. One could think that using a similar method that the one used for augmentations of the data sets could also diversify the data base. However, we wanted all measurements of a same session and cell to be coherent \n",
    "and it wouldn't make sense to have different static features. Therefore, the safest approach was to only duplicate these values. For the augmented experiments the only parameters that are changed are the initial and final times and polarizations. There is no incompatibility here to what we have just said as these work as \"hypothetical independent experiments\". This is why augmentation is done before this function gets used.\n",
    "         \n",
    "\n",
    "4. **nll_loss** ML algorythms require a way to tell the algorythm if it is learning or not. The most standard practice is with a Loss function. If the loss value goes down that means that the algorythm is learning and, if a step increases the loss, then it is punished and tries other approaches. When using uncertainties when teaching the model, the most common loss function is the NLL or Negative log-likelihood of a normal distribution NLL$=\\frac{1}{2}$log$(σ^2)+\\frac{(y−μ)^2}{2σ^2}$. Instead of predicting $σ^2$ we obtain its logarythm to have a more stable process (and avoid accounting precision as $\\sigma^{-2}$ which is numerically unstable when uncertainties are low).\n",
    "However we want to avoid uncertainties to drift too far from the overall model predictions. To achieve that, we can get a rough estimate on what the uncertainty of a set predictions look like:\n",
    "Let $\\vec{\\mu}$ be the vector of $N$ predicted values. It can be considered as a random vector and its variance can be obtained\n",
    "$Var(\\vec{mu})=\\frac{1}/{N}\\sum_{i=1}^N\\left(\\mu_i-E(\\vec{\\mu})\\right)$\n",
    "where $E({\\mu})$ is the mean of the predicted values. As the square root of the variance is tha uncertainty, we have a statistic of the data related to the data. To avoid numerical instability, as we are comparing logarithms of variances, we add a small constant of 10^{-6}\n",
    "Then, an easy way to model how far the real log of the variance predictions stray from this overall behaviour, is with a quadratic expression.\n",
    "$StrayPenalty = B \\cdot \\left(\\log\\left(\\sigma^2\\right)-\\log\\left(Var(\\vec{\\mu}\\right)\\right)^2$\n",
    "(analogous to the norm $\\mid\\mid\\cdot\\mid\\mid_2$ in $\\mathcal{L}^2$ so this \"distance\" is reasonable (given that we are working with discrete functions or successions). Also, we also want to enphasize to the model that overestimation of the uncertainties is worst than underestimations. The reason why is because observed models that overinflated uncertainties showed constant polarization predictions. The slope correction done further on the pipeline can \"fix\" this issue but what the model returns then is closer to a poorly calculated linear fit\n",
    "Therefore, an addition penalty was added.\n",
    "$OverestimatePenalty= C \\cdot \\max\\left(0, \\left(\\log\\left(\\sigma^2\\right)-\\log\\left(Var(\\vec{mu})\\right)\\right)\\right)$\n",
    "(analogous to the norm $\\mid\\mid\\cdot-0\\mid\\mid_\\infty$ in $\\mathcal{L}^\\infty$ so this \"distance\" is reasonable (given that we are working with discrete functions or successions))\n",
    "\n",
    "5. **Define\\_Complexity**. Given the name of a model it defines the model function of the ML algorithm. To be precise, it defines a function called _build_model_ every time _Define\\_Complexity_ runs. If _Define\\_Complexity_ gets run another time, it will define (possibly) another _build_model_ if the _Complexity_ variable changes. To find the best model, first two models were obtaines, one that overfits and another one that underfits. Then a series of intermediate models are built and tested. As ML model training has an important random aspect, there is no continuous transition from models with similar characteristics. Therefore, 185 models were tested whose names represent the main feature that defined them (NaifTwice1D have two Conv1D layers, SimpleNoDroput was the Simple model without the Dropout layer, etc). The numbers represent the size of the layers or a property of those layers (num_kernel for example).\n",
    "                Complex, Simple, Naif, Naif834, Naif838, Naif8316, Naif8332, Naif854, Naif858, Naif8516,\n",
    "                Naif8532, Naif8104, Naif8108, Naif81016, Naif81032, Naif1634, Naif1638, Naif16316, Naif16332,\n",
    "                Naif1654, Naif1658, Naif16516, Naif16532, Naif16104, Naif16108, Naif161016, Naif161032, Naif2434,\n",
    "                Naif2438, Naif24316, Naif24332, Naif2454, Naif2458, Naif24516, Naif24532, Naif24104, Naif24108,\n",
    "                Naif241016, Naif241032, NaifTwice1D883316, NaifTwice1D883332, NaifTwice1D883516, NaifTwice1D883532,\n",
    "                NaifTwice1D885316, NaifTwice1D885332, NaifTwice1D885516, NaifTwice1D885532, NaifTwice1D843316,\n",
    "                NaifTwice1D843332, NaifTwice1D843516, NaifTwice1D843532, NaifTwice1D845316, NaifTwice1D845332\n",
    "                NaifTwice1D845516, NaifTwice1D845532, NaifTwice1D483316, NaifTwice1D483332, NaifTwice1D483516,\n",
    "                NaifTwice1D483532, NaifTwice1D485316, NaifTwice1D485332, NaifTwice1D485516, NaifTwice1D485532,\n",
    "                NaifTwice1D443316, NaifTwice1D443332, NaifTwice1D443516, NaifTwice1D443532, NaifTwice1D445316,\n",
    "                NaifTwice1D445332, NaifTwice1D445516, NaifTwice1D445532, NaifTwiceDense414, NaifTwiceDense418\n",
    "                NaifTwiceDense4116, NaifTwiceDense4124, NaifTwiceDense814, NaifTwiceDense818, NaifTwiceDense8116,\n",
    "                NaifTwiceDense8124, NaifTwiceDense1614, NaifTwiceDense1618, NaifTwiceDense16116, NaifTwiceDense16124,\n",
    "                NaifTwiceDense2414, NaifTwiceDense2418, NaifTwiceDense24116, NaifTwiceDense24124, NaifTwiceDense424,\n",
    "                NaifTwiceDense428, NaifTwiceDense4216, NaifTwiceDense4224, NaifTwiceDense824, NaifTwiceDense828,\n",
    "                NaifTwiceDense8216, NaifTwiceDense8224, NaifTwiceDense1624, NaifTwiceDense1628, NaifTwiceDense16216,\n",
    "                NaifTwiceDense16224, NaifTwiceDense2424, NaifTwiceDense2428, NaifTwiceDense24216, NaifTwiceDense24224,\n",
    "                SimpleNoDropout444, SimpleNoDropout448, SimpleNoDropout484, SimpleNoDropout488, SimpleNoDropout4164,\n",
    "                SimpleNoDropout4168, SimpleNoDropout844, SimpleNoDropout848, SimpleNoDropout884, SimpleNoDropout888,\n",
    "                SimpleNoDropout8164, SimpleNoDropout8168, SimpleNoDropout1644, SimpleNoDropout1648, SimpleNoDropout1684,\n",
    "                SimpleNoDropout1688, SimpleNoDropout16164, SimpleNoDropout16168, Simple414414, Simple414418,\n",
    "                Simple414424, Simple414428, Simple414814, Simple414818, Simple414824, Simple414828, Simple418414,\n",
    "                Simple418418, Simple418424, Simple418428, Simple418814, Simple418818, Simple418824, Simple418828,\n",
    "                Simple424414, Simple424418, Simple424424, Simple424428, Simple424814, Simple424818, Simple424824,\n",
    "                Simple424828, Simple428414, Simple428418, Simple428424, Simple428428, Simple428814, Simple428818,\n",
    "                Simple428824, Simple428828, Simple814414, Simple814418, Simple814424, Simple814428, Simple814814,\n",
    "                Simple814818, Simple814824, Simple814828, Simple818414, Simple818418, Simple818424, Simple818428,\n",
    "                Simple818814, Simple818818, Simple818824, Simple818828, Simple824414, Simple824418, Simple824424,\n",
    "                Simple824428, Simple824814, Simple824818, Simple824824, Simple824828, Simple828414, Simple828418,\n",
    "                Simple828424, Simple828428, Simple828814, Simple828818, Simple828824, Simple828828\n",
    "\n",
    "\n",
    "\n",
    "6. **model\\_fitting**. It is a function that logs and runs model.fit() on a two-input Keras model and returns the training history. It needs **static, time and polarization variables scaled**. Training is not done using the uncertainties of the data as it was decided that uncertainty information is encoded in the augmentations. Note: No validation is done anywhere in the code. Here are some of the reasons:\n",
    "    \n",
    "    6.1. The data base is very small. The amorphous data base contains only 199 points while the crystalline one contains 251. Removing a small percentage of those points for validation might leave the data base too small and underfitting might worsen the result more than fine tuning parametrs with validation. However this has not been tested. A set of models should be ran with and without validation to check if it affects positively, negatively or if it doesn't matter. **Possible optimization of the algorithm**\n",
    "    \n",
    "    6.2. A randomized validation split may be physically wrong. Therefore it should be chronological, not shuffled. However, in crystalline experiments, there are decay experiments that have four or five intermediate points. Even removing one point for validation is a massive hit on the experiment. Therefore, it is risky to add validation\n",
    "    \n",
    "    6.3. To find good models, a Leave-one-out approach was used. For a certain model structure, an experiment gets removed and the model trains on all the remaining experiments. Then, the model tries to predict this isolated experiment. Afterwards, the experiment is returned and a new one becomes isolated. This process loops for all experiments and an overall score of the model is computed. This process was done for 498 models for amorphous materials. This is a stronger (and more expensive) method than validation as it is not dependent on the validation splits and avoids possible information leaks.\n",
    "\n",
    "7. **model\\_prediction**. It is a funtion that predicts with a given model. It needs **static and time variables scaled**. This scaling must be coherent to the one done in the rest of the funtions.\n",
    "\n",
    "8. **train**. This function is the one responsible of scaling the inputs and training the model (it uses **model\\_fitting**)\n",
    "\n",
    "    8.1. It creates the independent arrays with all the encoded experiments (augmented or not) using build_dataset\n",
    "    \n",
    "    8.2. Then it scales the data. ML algorithms work better when the inputs and outputs are normalized. The reason why we don't normalize inside the function is to have those scaler defined globally and not locally\n",
    "    \n",
    "    8.3. It builds the model depending on the use_uncertainty bool. (It changes the loss function and the output).\n",
    "    \n",
    "    8.4. It trains the model and returns its history (the trained model)\n",
    "    \n",
    "9. **align_static_vectors**. It converts the columns not present on an isolated experiment to zeros.\n",
    "    \n",
    "10. **model_predict_sloped** It substracts a linear function to the predicted values. If done correctly this makes it so that the polarization predictions at the time points stored as static features are the same as the measured polarization values at those times. This fixes a vertical shift and also an overall slope. As it is a correction done with experimental values, the algorithm is still universal. However we can't fully say that it is a pure ML algorithm. The 'correctness' of this method is subjective. It is a warning in the ML front that there is an issue with the data base but it is a valid fix for experimentalists.\n",
    "\n",
    "11. **isolated_experiments**. It does the entire Leave-One-Out logic and uses all the previous functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9d4c2-acc8-421e-996c-5619a998948a",
   "metadata": {},
   "source": [
    "## 1- Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb769d6-3f94-4c3f-974d-4736639e3eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import shutil\n",
    "import gc\n",
    "import joblib\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras import Model, Input, regularizers, backend as K\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Concatenate, Conv1D, GlobalAveragePooling1D)\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint)\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee301f8-4bf4-431a-8fcc-7daa882afecf",
   "metadata": {},
   "source": [
    "## 2. Auxiliary Functions and log file creation\n",
    "\n",
    "1. _PrintDebug_ is a flag that allows the code to output on screen all the steps. If it is set to false, it won´t show anything. However, all information will be properly logged whether this flag is set to true or false. The name of the log is determined by the variable *log_file_path*. The code runs faster if it is set to False.\n",
    "\n",
    "2. _ShowPlot_ is a similar flag that allows the code to show on screen all plots that are being produced. They are all stored independently of whether this flag is True or False. The code runs faster if it is set to False.\n",
    "\n",
    "3. _LogNoise_ is a flag that can toggle on or off if numerical values get written in the log. These values are not very useful but, if there is ever a need to check what is going on numerically, this flag can help.\n",
    "\n",
    "3. **log_message** is a function used for writting on the log file. If you run this cell ti will first erase the previous log. If you need to keep information of what happened please save it manually. It also erases the elapsed time that each model has taken.\n",
    "\n",
    "4. **long_path** is a function that \"fixes\" directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55438ae3-4a02-44f9-8523-5064742fc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintDebug = False \n",
    "ShowPlot = False \n",
    "LogNoise = False \n",
    "\n",
    "\n",
    "log_file_path = \"CrystallineLog_Testing_ML.txt\"\n",
    "def log_message(message):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        message (string): The text that will be logged\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    Notes:\n",
    "        It will write the string \"message\" in the log file.\n",
    "        If PrintDebug==True then it will also print the string\n",
    "    \"\"\"\n",
    "    message = str(message)\n",
    "    if PrintDebug:\n",
    "        print(message)\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "        log_file.write(str(message) + \"\\n\")\n",
    "\n",
    "################################################################\n",
    "\n",
    "def long_path(path):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        path (path): The path that needs to be converted\n",
    "    \n",
    "    Returns:\n",
    "        The updated path string or path depending on the platform used\n",
    "        \n",
    "    Notes:\n",
    "        To avoid Windows 260 character limit for Windows paths, a special \"prefix\" is added.\n",
    "        It also unifies how directories are managed.\n",
    "        Also works with Linux and Mac\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert to Path and resolve to absolute\n",
    "    path = Path(path).resolve()\n",
    "    \n",
    "    #Windows only:  \n",
    "    if os.name == \"nt\":\n",
    "        path_str = str(path)\n",
    "        if not path_str.startswith(\"\\\\\\\\?\\\\\"):\n",
    "            # UNC paths need special handling\n",
    "            if path_str.startswith(\"\\\\\\\\\"):\n",
    "                path_str = \"\\\\\\\\?\\\\UNC\\\\\" + path_str[2:]\n",
    "            else:\n",
    "                path_str = \"\\\\\\\\?\\\\\" + path_str\n",
    "            return path_str\n",
    "    \n",
    "    return path\n",
    "\n",
    "to_erase = [\"CrystallineExecution_times.txt\", \"CrystallineLog_Testing_ML.txt\"]\n",
    "for item in to_erase:\n",
    "    path = os.path.abspath(item)  # full path\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                log_message(f\"Deleted file: {path}\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                log_message(f\"Deleted folder: {path}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\" Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"Not found (skipped): {path}\")\n",
    "        \n",
    "with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(\"=== Log started ===\\n\")\n",
    "    log_file.write(\"All outputs from functions have a string at the beginning to show the origin:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df3b6a4-0743-4b6e-a027-81193d92cc01",
   "metadata": {},
   "source": [
    "## 3. Functions\n",
    "\n",
    "1. **load\\_experiments**. It uses the directory where the array (numerical values) and parameter files resides, picks one polarization column and prepares a list of experiments to feed the ML algorythm. The output is as follows:\n",
    "    \n",
    "    *encoded\\_experiments=[(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 1}$,(static\\_values, Deltatime, PolarizationD3, ErrPolarizationD3)$_{experiment 2}$,...]* \n",
    "    Note that the Cell Id is Hot Encoded. The type of cell did not affect greatly the predictions. However in the future, it may be useful to give more information to Cell_ID. As of 2025, these strings are Hot Encoded which means that the code finds all the different types, creates columns for each type and writes 0 or 1 (bool) depending of whether the cell was from one type or the other. This is the standard procedure to feed categorical variables to ML.\n",
    "    The size of the output list depends on the number of pairs of .txt files present in the folder. This means that it also works for isolated experiments\n",
    "    \n",
    "2. **augment_experiments** is a function that takes the original data list (as load_experiments returns it) and augments them _num\\_augmentations_ times. The final legth of the list will be (num_augmentations +1) times the original length. Due to the reduced size of the data base (Not more than 30 experiments before 2026), techniques like augmentation are required. Due to the stochastic nature of neutron detection, most measurements, if repeated under the same conditions, will yield different results. Of course, all measurements converge (with uncertainty) to what we can say is \"the true value\". Therefore, we can duplicate the experiments adding noise to the measurements to obtain \"hypothetical\" measurements that expand the data base. It was decided that the uncertainty won't be modified. To decide what type of noise could be applied two possibilities were considered. The first one was to suppose that sensor detection follows a Poisson distribution (law of rare events). The second one was to suppose that it follows a Normal Distribution of mean the measured value and width the uncertainty. As a poissonian distribution converges stochastically (in probability and distribution but not almost sure nor in $L^p$) to a normal distribution, it is safe to assume that they converge to the same result so the gaussian approach was selected. Also, the measurements are not raw counts but a function of them.\n",
    "$P=\\frac{n^+-n^-}{n^++n^-}$\n",
    "We have no direct data of the number of counts (the parameter of a Posissonian distribution) but a rough estimate points to them being (worst case scenario) within the order of the millions (After 100 counts, poisson distributions vary very little from normal distributions). Threfore, count detection can be approximated to a normal distribution and a linear combination of random variables distributed as normal distributions is also a random variable with a normal distribution. Therefore, it is safe to assume that sensor measurements can be modelled after a gaussian distribution.\n",
    "\n",
    "\n",
    "3. **build_dataset** is a function that prepares the data base to be fed directly into a ML model for training and validation. There are a few import decisions taken here. The way this function is set up, it removes 2 rows of data per polariser cell studied. The reason why it was done is because we want the ML model to be able to predict polarization decay when given the specs of the cell (the static parameters) and the initial and final polarization values (with their associated time values). The reason why those two values are considered \"known values\" for each cell is beacuse they can be easily measured experimentally and give as a good estimate about the overall behaviour. In some experiments, the environment of the studied sample is too fragile to move and place the Si crystal for polarization measurements. Therefore, a working ML algorithm whose inputs are the specs of the cell and the initial and final polarizations (measured before and after the sample is in place) would enable experiments that could not be done before without proper polarization efficiency corrections. \n",
    "Also, we remove those two values per polariser cell from the training arrays (Xt, y and err). The reason why it is done is to avoid data leaks in the model. If we give those values as training data and also give them as parameters, the ML algorithm can run into the risk of memorizing those pairs (over-fitting) and worsen any new predicitions. Uncertainties for the first and last polarization measurements are not added to the static features. This was a decision taken to avoid giving too much weight to two variables that don't have value on their own (they compliment the polarization values but if the ML architecture is as shallow as\n",
    "the one used here, they can be considered independent variables and reduce the weight and importance of the other variables). \n",
    "Another consideration taken here was that the static features get duplicated in Xs a lot of times. One could think that using a similar method that the one used for augmentations of the data sets could also diversify the data base. However, we wanted all measurements of a same session and cell to be coherent \n",
    "and it wouldn't make sense to have different static features. Therefore, the safest approach was to only duplicate these values. For the augmented experiments the only parameters that are changed are the initial and final times and polarizations. There is no incompatibility here to what we have just said as these work as \"hypothetical independent experiments\". This is why augmentation is done before this function gets used.\n",
    "         \n",
    "\n",
    "4. **nll_loss** ML algorythms require a way to tell the algorythm if it is learning or not. The most standard practice is with a Loss function. If the loss value goes down that means that the algorythm is learning and, if a step increases the loss, then it is punished and tries other approaches. When using uncertainties when teaching the model, the most common loss function is the NLL or Negative log-likelihood of a normal distribution \n",
    "NLL$=\\frac{1}{2}$log$(σ^2)+\\frac{(y−μ)^2}{2σ^2}$\n",
    "where $\\sigma$ is the uncertainty in the predictions, $y$ is the measured value and $\\mu$ the predicted value. Instead of predicting $σ^2$ directly, we obtain its logarithm to have a more stable process (and avoid accounting precision as $\\sigma^{-2}$ which is numerically unstable when uncertainties are low).\n",
    "\n",
    "However we want to avoid uncertainties that drift too far from the overall model predictions. To achieve that, we can get a rough estimate on what the uncertainty of a set predictions looks like.\n",
    "Let $\\vec{\\mu}=\\left(\\mu_1,\\ldots,\\mu_N\\right)^T$ be the vector of $N$ predicted values. It can be considered as a random vector of variance:\n",
    "$Var(\\vec{\\mu})=\\frac{1}{N}\\sum_{i=1}^N\\left(\\mu_i-E(\\vec{\\mu})\\right)$\n",
    "where $E(\\vec{\\mu})$ is the mean of the predicted values. We then have two different variances, one obtained as the sparseness of the predictions, (denoted as $Var\\left(\\vec{\\mu}\\right)$, and one obtained as a result of the internal ML calculations (denoted as $\\sigma^2$). A penalty can be added to the loss functions to force the model to try to reduce this differences. An easy way to model it is to obtain the difference of those variances and square the result (taking the absolute value was also a good estimate, but using squared values punished big discrepancies in a stronger way).\n",
    "\n",
    "$StrayPenalty = B \\cdot \\left[\\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}\\left(\\vec{\\mu}\\right)\\right)\\right]^2$\n",
    "where $B$ is a constant used to control the weight of this penalty. The reason why $Var\\left(\\vec{\\mu}\\right)$ was used and not $Var\\left(\\vec{y}\\right)$ (with $\\vec{y}$ the vector of measured values) was to avoid noise in the original data to tamper with the loss function. It would be physically clearer to use measured values sparseness as a way to guide the model but some experimental uncertainties are clearly underestimated and that would cause this penalty to dominate the loss and obscure the main loss protocol, the NLL.\n",
    "\n",
    "Also, we also want to punish the model if it tries overestimating $\\sigma$. If the model is unable to minimize $y-\\mu$, in order to lower NLL, it increases $\\sigma$. If no precautions are taken, this \"escape solution\" achieves bad predictions with inflated uncertainties that simulate a low loss value. A new penalty was added that punishes overestimation of the uncertainties more than underestimation (which never happened). The slope correction done further on the pipeline can \"fix\" this issue but what the model returns then is closer to a poorly calculated linear fit\n",
    "Therefore, an addition penalty was added.\n",
    "\n",
    "$OverestimatePenalty= C \\cdot \\max\\left(0, \\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}(\\vec{\\mu})\\right)\\right)$\n",
    "where $C$ is a constant used to control the weight of this penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b69f948-7e6f-4c3a-a54b-d073247ad0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiments(data_dir, polarization_column):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data_dir (str): The direction to the folder where all files to be loaded will be found\n",
    "        polarization_column: It is the name in the header (in the .txt files) of the column that will be read.\n",
    "                             It can be 'SoftPolarizationD3' or 'PolarizationD3'. All results were obtained with 'PolarizationD3'\n",
    "    Output: \n",
    "        A list, for all polarization decays like:\n",
    "        encoded_experiments = [\n",
    "            (static_values,   # list of static parameters (per experiment)\n",
    "            Deltatime,       # 1D numpy array of time values\n",
    "            polarization,    # 1D numpy array of polarization values\n",
    "            Uncertainty      # 1D numpy array of uncertainty values), ...]\n",
    "        A pd object with the Hot encoded Ids and the rest of the parameters per experiment (each experiment in a different row)\n",
    "    \n",
    "    Notes:\n",
    "    The steps the code does are the following:\n",
    "    1. Finds all array files (the ones with the numerical values of the decay) and loops over all of them\n",
    "    2. For each array files it reconstructs the name of the parameter file. It concatenates all parameter files into one pd structure.\n",
    "    3. Finally it loops over all array files appending the parameters, time, polarization and uncertainty of every experiment to a common list\n",
    "    \"\"\"\n",
    "    \n",
    "    log_message(f\"    load_experiments: Finding all Array Files...\")\n",
    "    # Step 1:\n",
    "\n",
    "    arrays_files = sorted(\n",
    "        glob.glob(os.path.join(data_dir, \"*.txt\"))) #Find all files that are .txt\n",
    "    arrays_files = [f for f in arrays_files if not f.endswith(\"_Parameters.txt\")] #Keep only the Arrays (not the parameters)\n",
    "\n",
    "    encoded_experiments = []\n",
    "    all_static_df = []\n",
    "\n",
    "    static_columns = ['CellID', 'Pressure', 'LabPolarization', 'LabTime'] #Parameter header\n",
    "\n",
    "    for arrays_path in arrays_files:\n",
    "        # Step 2:\n",
    "        base = os.path.basename(arrays_path)\n",
    "        # Build parameters filename by adding _Parameters before .txt\n",
    "        name_without_ext = os.path.splitext(base)[0]\n",
    "        parameters_filename = f\"{name_without_ext}_Parameters.txt\"\n",
    "        parameters_path = os.path.join(data_dir, parameters_filename)\n",
    "\n",
    "        # Read parameters file\n",
    "        try:\n",
    "            parameters_df = pd.read_csv(parameters_path) #Import the parameter file\n",
    "            if LogNoise:\n",
    "                log_message(f\"    load_experiments: Reading parameters file: {parameters_filename}\") #Clutter logging\n",
    "\n",
    "            #Get the first row as static data\n",
    "            static_row = parameters_df.iloc[0][static_columns] #Get the parameter numerical values\n",
    "            all_static_df.append(static_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            log_message(f\"    ****load_experiments: Failed to read parameters file: {parameters_filename}, error: {e}\")\n",
    "            continue\n",
    "\n",
    "    log_message(f\"    load_experiments: Create combined DataFrame for static parameters...\")\n",
    "    static_df = pd.DataFrame(all_static_df) #Combine all static rows into a Dataframe\n",
    "\n",
    "    log_message(f\"    load_experiments: Collected static data:\")\n",
    "    \"\"\"\n",
    "    The type of cell did not affect greatly the predictions. However in the future, it may be useful to give more information to the Cell_IDs.\n",
    "    As of 2025, these strings are Hot Encoded which means that the code finds all the different types, creates columns for each type and writes\n",
    "    0 or 1 (bool) depending of whether the cell was from one type or the other. This is the standard procedure to feed categorical varaibles to ML.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_message(f\"    load_experiments: Static dataframe columns:, {static_df.columns.tolist()}\")\n",
    "    log_message(f\"    load_experiments: Static dataframe shape:, {static_df.shape}\")\n",
    "\n",
    "    log_message(f\"    load_experiments: Hot encoding CellID.\") \n",
    "    categorical_cols = ['CellID']\n",
    "    static_df = pd.get_dummies(static_df, columns=categorical_cols) #Does the hot encoding\n",
    "    \n",
    "    # Now second pass: read arrays and create encoded_experiments with encoded static params\n",
    "    for i, arrays_path in enumerate(arrays_files):\n",
    "        base = os.path.basename(arrays_path)\n",
    "        name_without_ext = os.path.splitext(base)[0]\n",
    "        parameters_filename = f\"{name_without_ext}_Parameters.txt\"\n",
    "        parameters_path = os.path.join(data_dir, parameters_filename)\n",
    "        # log_message(f\"Reading arrays file: {base}\") #Clutter log\n",
    "        arrays_df = pd.read_csv(arrays_path) # Reads the time series data \n",
    "    \n",
    "        static_values = static_df.iloc[i].to_list() #Fetches the static parameters corresponding to this experiment\n",
    "    \n",
    "        Deltatime = arrays_df[\"DeltaTime\"].values\n",
    "        polarization = arrays_df[polarization_column].values #Extracts the time array and the selected polarization column.\n",
    "        #Save the uncertainty even if it is not used afterwards\n",
    "        Uncertainty = arrays_df[\"ErrPolarizationD3\"].values\n",
    "        #if len(Deltatime) > 2:\n",
    "        encoded_experiments.append((static_values, Deltatime, polarization, Uncertainty))\n",
    "        # log_message(f\"Creating Encoded Experiments (appending parameters, time array, polarization array and uncertainty array)\") #Clutter log\n",
    "    log_message(f\"    load_experiments: Loaded {len(encoded_experiments)} experiments.\")\n",
    "    return encoded_experiments, static_df.columns.tolist()\n",
    "    \n",
    "#################################################################################################\n",
    "\n",
    "def augment_experiments(original_experiments, num_augmentations=5, base_seed=42):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        original_experiments (list): A list, for all polarization decays like encoded_experiments in last function:\n",
    "            original_experiments = [\n",
    "                (static_values,   # list of static parameters (per experiment)\n",
    "                Deltatime,       # 1D numpy array of time values\n",
    "                polarization,    # 1D numpy array of polarization values\n",
    "                Uncertainty      # 1D numpy array of uncertainty values), ...]\n",
    "        num_augmentations (int): The number of augmentations done per experiment.\n",
    "        base_seed (int): A seed that guarantees reproducibility\n",
    "    \n",
    "    Outputs:\n",
    "        A list that concatenates the original experiments with the augmented ones.\n",
    "        Instead of being a list of len(original_experiments) it becomes a list of\n",
    "        len(original_experiments) * (1+ num_augmentations)\n",
    "    \n",
    "    Notes: Due to the stochastic nature of neutron detection, most measurements, if\n",
    "    repeated under the same conditions, will yield different results. Of course, all\n",
    "    measurements converge (with uncertainty) to what we can say is \"the true value\".\n",
    "    Therefore, we can duplicate the experiments adding noise to the measurements to obtain\n",
    "    \"hypothetical\" measurements that expand the data base. It was decided that the\n",
    "    uncertainty won't be modified. To decide what type of noise could be applied two\n",
    "    possibilities were considered. The first one was to suppose that sensor detection\n",
    "    follows a Poisson distribution (law of rare events). The second one was to suppose\n",
    "    that it follows a Normal Distribution of mean the measured value and width the uncertainty.\n",
    "    As a poissonian distribution converges stochastically (in probability and distribution\n",
    "    but not almost sure nor in L^p) to a normal distribution, it is safe to assume that they\n",
    "    converge to the same result so the gaussian approach was selected. Also, the measurements\n",
    "    are not raw counts but a function of them. P=(n^+ - n^-)/(n^+ + n^-)\n",
    "    We have no direct data of the number of counts (the parameter of a Posissonian distribution)\n",
    "    but a rough estimate points to them being (worst case scenario) within the order of the millions\n",
    "    (After 100 counts, poisson distributions vary very little from normal distributions).\n",
    "    Threfore, count detection can be approximated to a normal distribution and a linear\n",
    "    combination of random variables distributed as normal distributions is also a random variable\n",
    "    with a normal distribution. Therefore, it is safe to assume that sensor measurements can be\n",
    "    modelled after a gaussian distribution.  \n",
    "    \"\"\"\n",
    "    \n",
    "    log_message(f\"    augment_experiments: Augmenting {len(original_experiments)} a number of {num_augmentations} times\")\n",
    "    augmented_experiments = []\n",
    "    for idx, (static, time, polar, uncertainty) in enumerate(original_experiments):\n",
    "        for n in range(num_augmentations):\n",
    "            seed = hash((idx, n, base_seed)) % 2**32\n",
    "            rng = np.random.default_rng(seed)\n",
    "            noise = rng.normal(loc=0.0, scale=uncertainty)\n",
    "            new_polar = polar + noise\n",
    "            if LogNoise:\n",
    "                log_message(f\"    augment_experiments:       Augmented experiment {idx} #{n} with seed {seed}\")\n",
    "            augmented_experiments.append((static, time, new_polar, uncertainty))\n",
    "    log_message(f\"    augment_experiments: Augmented to {len(original_experiments + augmented_experiments)} experiments\")\n",
    "    return original_experiments + augmented_experiments\n",
    "\n",
    "\n",
    "#######################################################################################3\n",
    "\n",
    "def build_dataset(experiments, mode=\"PolarizationD3\"):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        experiments (list): It is a list like the output of load_experiments or augment_experiments\n",
    "            experiments = [\n",
    "                (static_values,   # list of static parameters (per experiment)\n",
    "                Deltatime,       # 1D numpy array of time values\n",
    "                polarization,    # 1D numpy array of polarization values\n",
    "                Uncertainty      # 1D numpy array of uncertainty values), ...]\n",
    "        mode (str): The column of polarization that will be used. It can be either 'SoftPolarizationD3' or 'PolarizationD3'.\n",
    "    \n",
    "    Output:\n",
    "        1. Xs. An array of lists. Shape (number_of_samples, number_static_features) Each list contains all the original static parameters (hot encoded) plus the first and last polarization values with uncertainty.\n",
    "           To be precise, they get added in this order: static parameters + initial time + initial polarization + final time + final polarization.\n",
    "           For all samples, the static features get added to this list. This means that, if an experiment has M measurements, two get added to the parameter\n",
    "           list and then those parameter features get written M-2 times in this array\n",
    "        2. Xt. An array of time. Shape (number_of_samples, 1). All time values that are not used as parameters get added to this array. However they are not\n",
    "           saved directly. If they are for example 0,120,250 then they get saved as [0],[120],[250]. The reason is compatibility with Keras/TensorFlow.\n",
    "        3. y. An array of polarization. It is the same as Xt but with polarization values (the type of polarization is determined by _mode_)\n",
    "        4. err. An array of polarization uncertainties. It is the same as Xt and y but with polarization uncertainties.\n",
    "    Notes:\n",
    "        If there are only two rows then the file gets skipped. It shouldn't happen but there is logic for it. The reason why it gets skipped is because\n",
    "        there would not be any values left to use for training or validation    \n",
    "    \"\"\"\n",
    "\n",
    "    Xs, Xt, y, u = [], [], [], []\n",
    "    log_message(f\"    build_dataset: Starting build_dataset for column {mode} \")\n",
    "    log_message(f\"    build_dataset: Number of experiments to process: {len(experiments)} (should be (num_augmentations+1)*(number_of_experiments-1)\")\n",
    "    \n",
    "    for exp_idx, (static_params, delta_time, polarization, uncertainty) in enumerate(experiments):\n",
    "        if len(delta_time) < 2:\n",
    "            log_message(f\"    ****build_dataset:       Skipping experiment {exp_idx}: too few data points (len={len(delta_time)})\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        if LogNoise:\n",
    "            log_message(f\"    build_dataset: Adding First and Last Polarization (with time) values as static parameters\") #Clutter log\n",
    "        init_idx = 0\n",
    "        final_idx = -1\n",
    "        initial_dt, initial_p = delta_time[init_idx], polarization[init_idx]\n",
    "        final_dt, final_p = delta_time[final_idx], polarization[final_idx]\n",
    "\n",
    "        static_vector = static_params + [\n",
    "            initial_dt, initial_p,\n",
    "            final_dt, final_p\n",
    "        ]\n",
    "        if LogNoise:\n",
    "            log_message(f\"    build_dataset: Experiment {exp_idx}: static_vector length={len(static_vector)} (should be 10 (three parameters, CellID hot encoded creates three posibilities, four for the initial and final polarization) \") #Clutter log\n",
    "        if LogNoise:\n",
    "            log_message(f\"    build_dataset: Building samples Static+time+polarization\") #Clutter log\n",
    "        \n",
    "        for t, p, err in zip(delta_time[1:-1], polarization[1:-1], uncertainty[1:-1]): \n",
    "            Xs.append(static_vector)\n",
    "            Xt.append([t])\n",
    "            y.append(p)\n",
    "            u.append(err) #We will ignore always uncertainty in parameters and even if they are not used, we will keep uncertainties in the data sets(same dimensions everywhere)\n",
    "\n",
    "    log_message(f\"    build_dataset: Number of experiments processed for mode '{mode}': {len(experiments)}\")\n",
    "    log_message(f\"    build_dataset: Final dataset shapes: Xs: {np.array(Xs).shape}, Xt: {np.array(Xt).shape}, y: {np.array(y).reshape(-1, 1).shape}\")\n",
    "    return np.array(Xs), np.array(Xt), np.array(y).reshape(-1, 1), np.array(u).reshape(-1, 1)\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def split_experiments(experiments, val_fraction=0.2, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_exp = len(experiments)\n",
    "    indices = rng.permutation(n_exp)\n",
    "\n",
    "    n_val = int(val_fraction * n_exp)\n",
    "    val_idx = indices[:n_val]\n",
    "    train_idx = indices[n_val:]\n",
    "\n",
    "    train_experiments = [experiments[i] for i in train_idx]\n",
    "    val_experiments   = [experiments[i] for i in val_idx]\n",
    "\n",
    "    log_message(f\"Split experiments: {len(train_experiments)} train / {len(val_experiments)} val\")\n",
    "    return train_experiments, val_experiments\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def nll_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        y_true (array): An array of the real values.\n",
    "        y_pred (array): An array of the predicted values\n",
    "    Output:\n",
    "     A scalar tensorflow tensor ( () ) with the value of the loss as the the mean Gaussian negative log-likelihood over the batch\n",
    "    Notes:\n",
    "        This loss function is not just a pure Negative Loss Likelyhood (NLL).\n",
    "        ML algorythms require a way to tell the algorythm if it is learning or not. The most standard practice is with a Loss function.\n",
    "        If the loss value goes down that means that the algorythm is learning and, if a step increases the loss, then it is punished and\n",
    "        tries other approaches. When using uncertainties when teaching the model, the most common loss function is the NLL or Negative\n",
    "        log-likelihood of a normal distribution \n",
    "        \n",
    "        NLL$=\\frac{1}{2}$log$(σ^2)+\\frac{(y−μ)^2}{2σ^2}$\n",
    "        \n",
    "        where $\\sigma$ is the uncertainty in the predictions, $y$ is the measured value and $\\mu$ the predicted value.\n",
    "        Instead of predicting $σ^2$ directly, we obtain its logarithm to have a more stable process (and avoid accounting precision as\n",
    "        $\\sigma^{-2}$ which is numerically unstable when uncertainties are low).\n",
    "\n",
    "        However we want to avoid uncertainties that drift too far from the overall model predictions. To achieve that, we can get a rough\n",
    "        estimate on what the uncertainty of a set predictions looks like. Let $\\vec{\\mu}=\\left(\\mu_1,\\ldots,\\mu_N\\right)^T$ be the vector\n",
    "        of $N$ predicted values. It can be considered as a random vector of variance:\n",
    "        \n",
    "        $Var(\\vec{\\mu})=\\frac{1}{N}\\sum_{i=1}^N\\left(\\mu_i-E(\\vec{\\mu})\\right)$\n",
    "        \n",
    "        where $E(\\vec{\\mu})$ is the mean of the predicted values. We then have two different variances, one obtained as the sparseness of\n",
    "        the predictions, (denoted as $Var\\left(\\vec{\\mu}\\right)$, and one obtained as a result of the internal ML calculations (denoted as\n",
    "        $\\sigma^2$). A penalty can be added to the loss functions to force the model to try to reduce this differences. An easy way to model\n",
    "        it is to obtain the difference of those variances and square the result (taking the absolute value was also a good estimate, but\n",
    "        using squared values punished big discrepancies in a stronger way).\n",
    "\n",
    "        $StrayPenalty = B \\cdot \\left[\\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}\\left(\\vec{\\mu}\\right)\\right)\\right]^2$\n",
    "        \n",
    "        where $B$ is a constant used to control the weight of this penalty. The reason why $Var\\left(\\vec{\\mu}\\right)$ was used and not\n",
    "        $Var\\left(\\vec{y}\\right)$ (with $\\vec{y}$ the vector of measured values) was to avoid noise in the original data to tamper with the\n",
    "        loss function. It would be physically clearer to use measured values sparseness as a way to guide the model but some experimental\n",
    "        uncertainties are clearly underestimated and that would cause this penalty to dominate the loss and obscure the main loss protocol, the NLL.\n",
    "\n",
    "        Also, we also want to punish the model if it tries overestimating $\\sigma$. If the model is unable to minimize $y-\\mu$, in order to\n",
    "        lower NLL, it increases $\\sigma$. If no precautions are taken, this \"escape solution\" achieves bad predictions with inflated\n",
    "        uncertainties that simulate a low loss value. A new penalty was added that punishes overestimation of the uncertainties more than\n",
    "        underestimation (which never happened). The slope correction done further on the pipeline can \"fix\" this issue but what the model\n",
    "        returns then is closer to a poorly calculated linear fit. Therefore, an addition penalty was added.\n",
    "\n",
    "        $OverestimatePenalty= C \\cdot \\max\\left(0, \\log\\left(\\sigma^2\\right)-\\log\\left(\\mathrm{Var}(\\vec{\\mu})\\right)\\right)$\n",
    "        \n",
    "        where $C$ is a constant used to control the weight of this penalty\n",
    "    \"\"\"\n",
    "\n",
    "    mu = y_pred[:, 0:1]\n",
    "    log_var = y_pred[:, 1:2]\n",
    "\n",
    "    # Base Gaussian NLL\n",
    "    precision = tf.exp(-log_var)\n",
    "    nll = tf.reduce_mean(0.5 * (log_var + tf.square(y_true - mu) * precision))\n",
    "\n",
    "\n",
    "    mu_centered = mu - tf.reduce_mean(mu)\n",
    "    sigma_ref = tf.sqrt(tf.reduce_mean(tf.square(mu_centered)) + 1e-6)\n",
    "    Stray_penalizer = 1e-2 #Parameter used to limit how far the uncertainty predictions stray from the experimental data\n",
    "    OverUncert_penalizer  = 5e-3 #Parameter used for punishing overestimated uncertainties\n",
    "    log_var_prior = tf.math.log(sigma_ref ** 2) #Order of magnitude of what uncertainties should look like\n",
    "\n",
    "    # Prior penalty. It penalizes if uncertainty strays too much from the experimental value\n",
    "    #It avoids underestimation of uncertainty and overestimation\n",
    "    penalty_stray = Stray_penalizer * tf.reduce_mean(tf.square(log_var - log_var_prior))\n",
    "\n",
    "    # Asymmetric penalty: punish overestimation more than underestimation\n",
    "    penalty_overestimate = OverUncert_penalizer * tf.reduce_mean(tf.nn.relu(log_var - log_var_prior))\n",
    "\n",
    "    return nll + penalty_stray + penalty_overestimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584ce5c-da06-4218-98e2-cdd5f1bbb78b",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "It consists of a single function called _Define\\_Complexity_. Given the name of a model it defines the model function of the ML algorithm. To be precise, it defines a function called _build_model_ every time _Define\\_Complexity_ runs. If _Define\\_Complexity_ gets run another time, it will define (possibly) another _build_model_ if the _Complexity_ variable changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb1acb8-f674-4ffd-83f9-a69b6b566656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Define_Complexity(Complexity):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        Complexity (str): It is the name of the model\n",
    "    Output: \n",
    "        None, but it defines the model architecture. It can be accessed outside of this function by calling build_model (once Definde_Complexity has ran)\n",
    "    \n",
    "    Characteristics of build_model:\n",
    "        Arguments:\n",
    "            input_dim_static (int): It is the number of static input features (the static parameters + 4)\n",
    "            use_uncertainty (bool): It changes the output of the model to use (or not) uncertainty.\n",
    "                The model will either train with mse and output just the mean or train wil NLL and output the mean and the log of the variance (σ^2)\n",
    "        Output:\n",
    "            A keras model.\n",
    "        Notes: Here are the models used:\n",
    "                Complex, Simple, Naif, Naif834, Naif838, Naif8316, Naif8332, Naif854, Naif858, Naif8516,\n",
    "                Naif8532, Naif8104, Naif8108, Naif81016, Naif81032, Naif1634, Naif1638, Naif16316, Naif16332,\n",
    "                Naif1654, Naif1658, Naif16516, Naif16532, Naif16104, Naif16108, Naif161016, Naif161032, Naif2434,\n",
    "                Naif2438, Naif24316, Naif24332, Naif2454, Naif2458, Naif24516, Naif24532, Naif24104, Naif24108,\n",
    "                Naif241016, Naif241032, NaifTwice1D883316, NaifTwice1D883332, NaifTwice1D883516, NaifTwice1D883532,\n",
    "                NaifTwice1D885316, NaifTwice1D885332, NaifTwice1D885516, NaifTwice1D885532, NaifTwice1D843316,\n",
    "                NaifTwice1D843332, NaifTwice1D843516, NaifTwice1D843532, NaifTwice1D845316, NaifTwice1D845332\n",
    "                NaifTwice1D845516, NaifTwice1D845532, NaifTwice1D483316, NaifTwice1D483332, NaifTwice1D483516,\n",
    "                NaifTwice1D483532, NaifTwice1D485316, NaifTwice1D485332, NaifTwice1D485516, NaifTwice1D485532,\n",
    "                NaifTwice1D443316, NaifTwice1D443332, NaifTwice1D443516, NaifTwice1D443532, NaifTwice1D445316,\n",
    "                NaifTwice1D445332, NaifTwice1D445516, NaifTwice1D445532, NaifTwiceDense414, NaifTwiceDense418\n",
    "                NaifTwiceDense4116, NaifTwiceDense4124, NaifTwiceDense814, NaifTwiceDense818, NaifTwiceDense8116,\n",
    "                NaifTwiceDense8124, NaifTwiceDense1614, NaifTwiceDense1618, NaifTwiceDense16116, NaifTwiceDense16124,\n",
    "                NaifTwiceDense2414, NaifTwiceDense2418, NaifTwiceDense24116, NaifTwiceDense24124, NaifTwiceDense424,\n",
    "                NaifTwiceDense428, NaifTwiceDense4216, NaifTwiceDense4224, NaifTwiceDense824, NaifTwiceDense828,\n",
    "                NaifTwiceDense8216, NaifTwiceDense8224, NaifTwiceDense1624, NaifTwiceDense1628, NaifTwiceDense16216,\n",
    "                NaifTwiceDense16224, NaifTwiceDense2424, NaifTwiceDense2428, NaifTwiceDense24216, NaifTwiceDense24224,\n",
    "                SimpleNoDropout444, SimpleNoDropout448, SimpleNoDropout484, SimpleNoDropout488, SimpleNoDropout4164,\n",
    "                SimpleNoDropout4168, SimpleNoDropout844, SimpleNoDropout848, SimpleNoDropout884, SimpleNoDropout888,\n",
    "                SimpleNoDropout8164, SimpleNoDropout8168, SimpleNoDropout1644, SimpleNoDropout1648, SimpleNoDropout1684,\n",
    "                SimpleNoDropout1688, SimpleNoDropout16164, SimpleNoDropout16168, Simple414414, Simple414418,\n",
    "                Simple414424, Simple414428, Simple414814, Simple414818, Simple414824, Simple414828, Simple418414,\n",
    "                Simple418418, Simple418424, Simple418428, Simple418814, Simple418818, Simple418824, Simple418828,\n",
    "                Simple424414, Simple424418, Simple424424, Simple424428, Simple424814, Simple424818, Simple424824,\n",
    "                Simple424828, Simple428414, Simple428418, Simple428424, Simple428428, Simple428814, Simple428818,\n",
    "                Simple428824, Simple428828, Simple814414, Simple814418, Simple814424, Simple814428, Simple814814,\n",
    "                Simple814818, Simple814824, Simple814828, Simple818414, Simple818418, Simple818424, Simple818428,\n",
    "                Simple818814, Simple818818, Simple818824, Simple818828, Simple824414, Simple824418, Simple824424,\n",
    "                Simple824428, Simple824814, Simple824818, Simple824824, Simple824828, Simple828414, Simple828418,\n",
    "                Simple828424, Simple828428, Simple828814, Simple828818, Simple828824, Simple828828\n",
    "    \"\"\"\n",
    "    log_message(f\"Define_Complexity: Defining model {Complexity}\")\n",
    "    if Complexity == \"Average\":       \n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "\n",
    "    elif Complexity == \"Complex\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.3)(x_time)\n",
    "            x_time = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = Dropout(0.3)(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.3)(x)\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(16, activation='tanh')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "\n",
    "    elif Complexity == \"Simple\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.3)(x_time)\n",
    "            x_time = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-2))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "\n",
    "    elif Complexity == \"Naif\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif834\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif838\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif8316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif8332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif854\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif858\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif8516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif8532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif8104\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif8108\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif81016\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif81032\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    elif Complexity == \"Naif1634\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif1638\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif16316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif16332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif1654\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif1658\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif16516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif16532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif16104\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif16108\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif161016\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif161032\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif Complexity == \"Naif2434\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif2438\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif24316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif24332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif2454\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif2458\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif24516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif24532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif24104\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif24108\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "    elif Complexity == \"Naif241016\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model        \n",
    "    elif Complexity == \"Naif241032\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=24, kernel_size=10, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='relu')(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            if use_uncertainty:\n",
    "                log_var = Dense(1)(x)\n",
    "                output = Concatenate()([mu, log_var])\n",
    "            else:\n",
    "                output = mu\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"NaifTwice1D883316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D883332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D883516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D883532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D885316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D885332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D885516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D885532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D843316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D843332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D843516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D843532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D845316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D845332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D845516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D845532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"NaifTwice1D483316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D483332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D483516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D483532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D485316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D485332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D485516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D485532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D443316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D443332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D443516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D443532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwice1D445316\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D445332\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"NaifTwice1D445516\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"NaifTwice1D445532\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=5, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"NaifTwiceDense414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                    \n",
    "    elif Complexity == \"NaifTwiceDense4116\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense4124\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense8116\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense8124\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense1614\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense1618\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense16116\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense16124\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense2414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense2418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense24116\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense24124\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"NaifTwiceDense424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                    \n",
    "    elif Complexity == \"NaifTwiceDense4216\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense4224\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense8216\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense8224\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense1624\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model    \n",
    "    elif Complexity == \"NaifTwiceDense1628\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense16216\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense16224\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense2424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense2428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense24216\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(16, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"NaifTwiceDense24224\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(24, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)   # moved here\n",
    "            x = Dense(24, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"SimpleNoDropout444\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"SimpleNoDropout448\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "        \n",
    "    elif Complexity == \"SimpleNoDropout484\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout488\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout4164\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout4168\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "\n",
    "    elif Complexity == \"SimpleNoDropout844\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"SimpleNoDropout848\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"SimpleNoDropout884\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout888\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout8164\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout8168\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model         \n",
    "    elif Complexity == \"SimpleNoDropout1644\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model            \n",
    "    elif Complexity == \"SimpleNoDropout1648\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model  \n",
    "    elif Complexity == \"SimpleNoDropout1684\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout1688\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout16164\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model          \n",
    "    elif Complexity == \"SimpleNoDropout16168\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model         \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"Simple414414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple414418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple414424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple414428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple414814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple414818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple414824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple414828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                 \n",
    "    elif Complexity == \"Simple418414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple418418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple418424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple418428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple418814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple418818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple418824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple418828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model   \n",
    "    elif Complexity == \"Simple424414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple424418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple424424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple424428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple424814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple424818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple424824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple424828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                 \n",
    "    elif Complexity == \"Simple428414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple428418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple428424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple428428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple428814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple428818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple428824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple428828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model   \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif Complexity == \"Simple814414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple814418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple814424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple814428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple814814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple814818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple814824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple814828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                 \n",
    "    elif Complexity == \"Simple818414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple818418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple818424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple818428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple818814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple818818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple818824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple818828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.1)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model   \n",
    "    elif Complexity == \"Simple824414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple824418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple824424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple824428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple824814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple824818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple824824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple824828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model                 \n",
    "    elif Complexity == \"Simple828414\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple828418\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple828424\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple828428\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "        \n",
    "    elif Complexity == \"Simple828814\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple828818\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model \n",
    "    elif Complexity == \"Simple828824\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(4, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model             \n",
    "    elif Complexity == \"Simple828828\":\n",
    "        def build_model(input_dim_static, use_uncertainty=False):\n",
    "            static_input = Input(shape=(input_dim_static,), name=\"static_input\")\n",
    "            time_input = Input(shape=(None, 1), name=\"time_input\")\n",
    "\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(time_input)\n",
    "            x_time = Dropout(0.2)(x_time)\n",
    "            x_time = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(x_time)\n",
    "            x_time = GlobalAveragePooling1D()(x_time)\n",
    "\n",
    "            x = Concatenate()([static_input, x_time])\n",
    "            x = Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "            x = Dropout(0.2)(x_time)\n",
    "            x = Dense(8, activation='tanh')(x)  # second dense layer\n",
    "\n",
    "            mu = Dense(1)(x)\n",
    "            log_var = Dense(1)(x)\n",
    "            output = Concatenate()([mu, log_var])\n",
    "\n",
    "            model = Model(inputs=[static_input, time_input], outputs=output)\n",
    "            model.compile(optimizer=Adam(), loss=nll_loss if use_uncertainty else 'mse')\n",
    "            return model         \n",
    "        \n",
    "    else:\n",
    "        log_message(f\"****Define_Complexity: UNKNOWN MODEL TYPE. FAIL INCOMING! Please check that the model name actually exists.\")\n",
    "    log_message(f\"Define_Complexity: Model built\\n\")\n",
    "    return build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1c644-8809-409f-af71-e4e798a38f6c",
   "metadata": {},
   "source": [
    "## 5. Model specific functions\n",
    "\n",
    "Here we have functions that train, validate and fit the models. Some models require the variables to be scaled or will scale them. Extra precautions need to be taken into account\n",
    "\n",
    "1. **model\\_fitting**. It is a function that logs and runs model.fit() on a two-input Keras model and returns the training history. It needs **static, time and polarization variables scaled**. Training is not done using the uncertainties of the data as it was decided that uncertainty information is encoded in the augmentations. Note: No validation is done anywhere in the code. Here are some of the reasons:\n",
    "    \n",
    "    1.1. The data base is very small. The amorphous data base contains only 199 points while the crystalline one contains 251. Removing a small percentage of those points for validation might leave the data base too small and underfitting might worsen the result more than fine tuning parametrs with validation.\n",
    "    \n",
    "    1.2. A randomized validation split may be physically wrong. Therefore it should be chronological, not shuffled. However, in crystalline experiments, there are decay experiments that have only four or five intermediate points. Even removing one point for validation is a massive hit on the experiment. Therefore, it is risky to add validation\n",
    "    \n",
    "    1.3. To find good models, a Leave-one-out approach was used. For a certain model structure, an experiment gets removed and the model and it trains on all the remaining experiments. Then, the model tries to predict this isolated experiment. Afterwards, the experiment is returned and a new one becomes isolated. This process loops for all experiments and an overall score of the model is computed. This process was done for 498 models for crystalline materials. This is a stronger (and more expensive) method than validation as it is not dependent on the validation splits and avoids possible information leaks.\n",
    "\n",
    "Also, five eight randomly picked models were tested with and without validation and with and without an asymetric uncertainty-overestimated penalizing loss. The result showed that the Loss update was an improvement and validation did not increase performance (without validation, the results were slightly better).\n",
    "\n",
    "2. **model\\_prediction**. It is a funtion that predicts with a given model. It needs **static and time variables scaled**. This scaling must be coherent to the one done in the rest of the funtions.\n",
    "\n",
    "3. **train**. This function is the one responsible of scaling the inputs and training the model (it uses **model\\_fitting**)\n",
    "\n",
    "    3.1. It creates the independent arrays with all the encoded experiments (augmented or not) using build_dataset\n",
    "    \n",
    "    3.2. Then it scales the data. ML algorithms work better when the inputs and outputs are normalized. The reason why we don't normalize inside the function is to have those scaler defined globally and not locally\n",
    "    \n",
    "    3.3. It builds the model depending on the use\\_uncertainty bool. (It changes the loss function and the output).\n",
    "    \n",
    "    3.4. It trains the model and returns its history (the trained model)\n",
    "    \n",
    "4. **align_static_vectors**. It converts the columns not present on an isolated experiment to zeros.\n",
    "    \n",
    "5. **model_predict_sloped** It substracts a linear function to the predicted values. If done correctly this makes it so that the polarization predictions at the initial and final time points are the same as the measured polarization values at those times. This fixes a vertical shift and also an overall slope. As it is a correction done with experimental values, the algorithm is still \"universal\". However we can't fully say that it is a pure ML algorithm. The \"correctness\" of this method is subjective. It is a warning in the ML front that there is an issue with the data base but it is a valid fix for experimentalists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d37740-43d6-4ff3-a3b1-98c3b42b38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fitting(model, X_static_scaled, X_time_scaled, y_scaled, epochs, batch_size, verbose, callbacks=None):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        model (keras model): This is the object that the build_model function returns.\n",
    "        X_static_scaled (array): It is an array os size (total number of intermediate measured points (let's call them 'samples'), number of static features). \n",
    "                                 Using the variables form build_dataset, the number of static features is 4+len(static_parameters(hot encoded)).\n",
    "                                 To be precise, it is just Xs scaled (an array of lists with the static features) but reshpaed into a 2d array\n",
    "        X_time_scaled (array): A 2d array of shape that reshapes Xt scaled from an array of lists (of size 1 like np.array([1],[13],[16],...)) to a 2D array\n",
    "        y_scaled (array): The same as X_time_scaled but with polarization values\n",
    "        epochs (int): Number of training epochs (times the model tries to validate the data and recalculates its parameters)\n",
    "        batch_size (int): The number of samples for every gradient update\n",
    "        verbose (int): It limits how much training output is printed\n",
    "        callbacks (str): It allows certain Keras callbacks (special code properties of keras). EarlyStopping, ReduceLROnPlateau or ModelCheckpoint are examples\n",
    "    Outputs:\n",
    "       A keras.callbacks.History object\n",
    "    Note:\n",
    "        It logs and runs model.fit() on a two-input Keras model and returns the training history.\n",
    "        IT REQUIRES THE INPUTS (static, time and polarization) TO BE NORMALIZED/SCALED\n",
    "        Training is not done using the uncertainties of the data.  \n",
    "    \"\"\"\n",
    "    log_message(f\"    Model_fitting: Training the model.\")\n",
    "    return model.fit(\n",
    "        [X_static_scaled, X_time_scaled],\n",
    "        y_scaled,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "\n",
    "def model_prediction(model, X_static_scaled, X_time_scaled):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X_static_scaled (array): A 2d array of Xs (samples, number of static features)\n",
    "        X_time_scaled (array): A 2d array of shape that reshapes Xt scaled from an array of lists (of size 1 like np.array([1],[13],[16],...)) to a 2D array\n",
    "        y_scaled (array): The same as X_time_scaled but with polarization values. It doesn't have to be the same as the one used in training\n",
    "    Output:\n",
    "        A 2d array with the predictions for those time values. It contains a column of the polarization predictions and another with the log of the variance\n",
    "    Notes:\n",
    "        IT REQUIRES THE INPUTS (static and time) TO BE NORMALIZED/SCALED\n",
    "    \"\"\"\n",
    "    log_message(f\"    model_prediction: Predicting {len(X_time_scaled)} time points\")\n",
    "    return model.predict([X_static_scaled, X_time_scaled], verbose=0)\n",
    "\n",
    "def train(encoded_experiments, scaler_static, scaler_time, scaler_y, use_uncertainty):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        encoded_experiments (list): A list like this:\n",
    "            encoded_experiments = [\n",
    "                (static_values,   # list of static parameters (per experiment)\n",
    "                Deltatime,       # 1D numpy array of time values\n",
    "                polarization,    # 1D numpy array of polarization values\n",
    "                Uncertainty      # 1D numpy array of uncertainty values), ...] \n",
    "        scaler_static: It is a scikit-learn scaler for the static features. Only MinMaxScaler (normalizes uniformly using the maximum and the minimum)\n",
    "        scaler_time: Analogously to scaler_static\n",
    "        scaler_y: Analogously to scaler_static\n",
    "        use_uncertainty (bool): It toggles whether it uses uncertainty for predictions or not. This changes the output of the model (mean and log_var or just the mean)\n",
    "    Output:\n",
    "        A fully trained model that can be used by model_predictions\n",
    "    Notes:\n",
    "        This function is the one responsible of scaling the inputs and training the model\n",
    "        1. It creates the independent arrays with all the encoded experiments (augmented or not) using build_dataset\n",
    "        2. Then it scales the data. ML algorithms work better when the inputs and outputs are normalized.\n",
    "           The reason why we don't normalize inside the function is to have those scaler defined globally and not locally\n",
    "        3. It builds the model depending on the use_uncertainty bool. (It changes the loss function and the output).\n",
    "        4. It trains the model and returns its history (the trained model)\n",
    "    \n",
    "    \"\"\"\n",
    "    log_message(f\"    train: Begin training and scaling with {len(encoded_experiments)} experiments.\")\n",
    "    X_static_all, X_time_all, y_all, u_all = build_dataset(encoded_experiments)\n",
    "    log_message(f\"Scaling all data\")\n",
    "    X_static_scaled = scaler_static.transform(X_static_all) #It only fits to the scaler. IT DOESN'T OVERWRITE THEM. WHAT A WASTE OF 20 DAYS OF MY LIFE >:(\n",
    "    X_time_scaled = scaler_time.transform(X_time_all)\n",
    "    y_scaled = scaler_y.transform(y_all)\n",
    "    \n",
    "    model = build_model(X_static_all.shape[1], use_uncertainty=use_uncertainty)\n",
    "    epochs = 300\n",
    "    batch_size = 32\n",
    "    log_message(f\"    train: Training final model with epochs={epochs}, batch_size={batch_size} and use_uncertainty = {use_uncertainty}\")\n",
    "    model_fitting(model, X_static_scaled, X_time_scaled, y_scaled, epochs, batch_size, verbose=0)\n",
    "    return model\n",
    "\n",
    "############################################################\n",
    "\n",
    "def align_static_vectors(experiments, static_columns_training, static_columns_isolated):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        experiments (list): A list like this:\n",
    "            experiments = [\n",
    "                (static_values,   # list of static parameters (per experiment)\n",
    "                Deltatime,       # 1D numpy array of time values\n",
    "                polarization,    # 1D numpy array of polarization values\n",
    "                Uncertainty      # 1D numpy array of uncertainty values), ...] \n",
    "        static_columns_training (list): A list of strings of all the static feature names used for training\n",
    "        static_columns_isolated (list): A list of strings of all the static feature names of the isolated experiment\n",
    "    Output:\n",
    "        aligned_experiments is a list of experiments with static vectors aligned to the training feature order.\n",
    "        To be precise (aligned_static, delta_time, polarization, uncertainty) where on the parameters where static_columns_isolated\n",
    "        had no values get turned into zeros. So, if static_columns_isolated doesn´t have a parameter 'Parameter 6', then, in the\n",
    "        original experiments lists, all numbers associated to 'Parameter 6' get turned to zero\n",
    "\n",
    "    Notes:\n",
    "        It helps the ML algorithm to focus on the important parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    aligned_experiments = []\n",
    "    for static, delta_time, polarization, uncertainty in experiments:\n",
    "        static_dict = dict(zip(static_columns_isolated, static))\n",
    "        aligned_static = [static_dict.get(col, 0.0) for col in static_columns_training]\n",
    "        aligned_experiments.append((aligned_static, delta_time, polarization, uncertainty))\n",
    "    log_message(f\"    align_static_vectors: Vectors aligned\")\n",
    "    return aligned_experiments\n",
    "\n",
    "def model_predict_sloped(model, X_static_scaled, X_time_scaled, m, n, scaler_y, scaler_time):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: A trained model\n",
    "        X_static_scaled (array). A 2d array of Xs (samples, number of static features)\n",
    "        X_time_scaled (array). A 2d array of shape that reshapes Xt scaled from an array of lists (of size 1 like np.array([1],[13],[16],...)) to a 2D array\n",
    "        m (float): The slope used for correction\n",
    "        n (float): The polarization shift for correction\n",
    "        scaler_time: It is a scikit-learn scaler for the time arrays. Only MinMaxScaler (normalizes uniformly using the maximum and the minimum)\n",
    "        scaler_y: Analogously to scaler_static\n",
    "    Outputs\n",
    "        y_pred_corrected_scaled is a 2D array where each column is the corrected predicted values (scaled) and the second one is the log of the variance (unchanged, a.k.a scaled)\n",
    "    Notes:\n",
    "        1. It predicts the polarization values\n",
    "        2. It inverse transforms the predicted values and the time points (not the uncertainty)\n",
    "        3. It calculates the correction curve in real units and corrects the predicted values.\n",
    "        4. Finally, it scales back the corrected polarization values and joins them with the unchanged logarithm of the variance\n",
    "        In summary, it substracts a linear function to the predicted values. If done correctly this makes it so that the polarization\n",
    "        predictions at the time points stored as static features are the same as the measured polarization values at those times.\n",
    "        This fixes a vertical shift and also an overall slope. As it is a correction done with experimental values, the algorithm is still\n",
    "        universal. However we can't fully say that it is a pure ML algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_message(f\"    model_predict_sloped: Correcting {len(X_time_scaled)} points by subtracting P(t) = {float(np.squeeze(m)):.3e} * t + {float(np.squeeze(n)):.3e}\")    # Step 1: Get scaled predictions from model\n",
    "    y_pred_scaled = model_prediction(model, X_static_scaled, X_time_scaled)\n",
    "    mean_scaled = y_pred_scaled[:, 0:1]  # shape (N,1)\n",
    "    mean_real = scaler_y.inverse_transform(mean_scaled)  # shape (N,1)\n",
    "    time_real = scaler_time.inverse_transform(X_time_scaled)  # shape (N,1)\n",
    "    \n",
    "    correction = m * time_real + n  # shape (N,1)\n",
    "    mean_corrected_real = mean_real - correction  # shape (N,1)\n",
    "    \n",
    "    mean_corrected_scaled = scaler_y.transform(mean_corrected_real)  # shape (N,1)\n",
    "    log_var_scaled = y_pred_scaled[:, 1:2]  # shape (N,1)\n",
    "    y_pred_corrected_scaled = np.hstack([mean_corrected_scaled, log_var_scaled])  # shape (N,2)\n",
    "    \n",
    "    return y_pred_corrected_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3bf80-b1e6-4719-bbd4-7d3461d8f4a5",
   "metadata": {},
   "source": [
    "## 6. Isolation logic\n",
    "To avoid leaks in the ML code a special pipeline was designed. First, an experiment gets removed form the training data base and a model is trained on the remaining experiments. Then, predicitions for the isolated experiment can be produced and their accuracy computed afterwards. The isolated experiment returns to the data base and a new one gets removed. If this process is looped for all experiments, we have a \"Leave-One-Out\" process to quantify how good the model predicts unseen experiments without sacrificing too much the size of the data base. This code is responsible for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8424fb87-aa6e-4ac8-9f1a-52e6be90ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_experiments(data_dir, isolated_dir, output_folder, use_uncertainty, num_augmentations, Correction, build_model):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data_dir (str): The directory folder that contains all the files for the data base\n",
    "        isolated_dir (str): The directory folder where the isolated experiment will be sent\n",
    "        use_uncertainty (bool): A bool variable that can toggle whether the model uses or not uncertainties in their output (not during training)\n",
    "        num_augmentations (int): The number of augmentations used in the model.\n",
    "        Correction (bool): A bool that toggles between predictions with the slope correction or raw predictions\n",
    "        build_model (function): It uses the model defined from Define_Complexity. That function gets called outside this function but the build_model \n",
    "        function needs to be given as an argument for isolate_experiments\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    Notes:\n",
    "        1. It creates the folder where the isolated files will temporally reside\n",
    "        2. It loops for all pairs of array and parameter files\n",
    "        3. It then moves both, parameter and array, files to the isolated directory\n",
    "        4. A subfolder on the main folder gets created with the name of the isolated experiment. All results will\n",
    "           be stored here for this isolation iteration.\n",
    "        5. Loop for all polarization modes to be processed. It can be SoftPolarizationD3, PolarizationD3 or both\n",
    "        6. Load the rest of the files and augment them\n",
    "        7. Obtain the time and polarization scalers. They are obtained using all time values and all polarization\n",
    "           values including the soon-to-become static features\n",
    "        8. Obtain the static features scaler.\n",
    "        9. Save scalers locally\n",
    "        10. Train the model. The data gets scaled during this step.\n",
    "        11. Load the isolated experiment and align both sets of data (use align_static_vectors)\n",
    "        12. Build manually the dataset for the isolated experiment\n",
    "        13. Extract the initial and final points, add them as parameters and scale the entire static vector with the general scaler.\n",
    "        14. Scale time and polarization arrays (only intermediate points). This is compatible with the scaling done to the rest of the experiments.\n",
    "        15. Combine all structures so that the shape is correct for prediction.\n",
    "        16. Check for shapes and create the final NumPy arrays of the experiment\n",
    "        17. Extract the initial and final polarizations and keep a scaled and unscaled version of all. Then predict at those time points. \n",
    "        18. Prepare the linear correction and predict in the time points where we have data.\n",
    "        The unscaled (physical units) outputs are pred_polar and pred_sigma.\n",
    "        19. Plot the raw values (black), the predictions for those time points (blue) and predictions every 1000 seconds (red if there is no correction, green if there is correction)\n",
    "        20. Save data\n",
    "        21. Move all experiments back to the original folder\n",
    "\n",
    "\n",
    "    \n",
    "    Scaling is done using also the polarization and time measurements that become static features. This was done so\n",
    "    that the ML algorithm can easily \"figure out\" that those parameters are related to polarizations and time moments.\n",
    "    Another equally valid approach is to only scale the intermediate points and then scale the parameters independently.\n",
    "    The downside of this first approach is that those static features get scaled twice (once as regular polarizations and\n",
    "    time values and a second time between all static features). In practice both methods showed close to no difference and\n",
    "    the second approach needs to juggle three scalers at once when working with the static vector. This proved to be difficult\n",
    "    and, again, showed no real improvement.\n",
    "    This scalers are obtained using MinMaxScaler. It finds the maximum and minimum of all values and normalizes all\n",
    "    of them uniformly (if X is a value on the array, then it gets replaced by (X-min)/(max-min))\n",
    "    Here is a list of all variables used\n",
    "    \n",
    "    OG for original scale, SC for scaled\n",
    "        all_time: OG. All time values\n",
    "        all_y: OG. All polarization values\n",
    "        scaler_time: Scaler of ALL time\n",
    "        scaler_y: Scaler of ALL polarizations\n",
    "        scaler_static: Scaler of ALL parameters (WITH t_ini, t_fin, P_ini, P_fin)\n",
    "\n",
    "        X_static_raw. OG. All static features\n",
    "        X_time_raw. OG. All time values without initial and final\n",
    "        y_raw. OG. All polarization values without initial and final\n",
    "        u. OG. All uncertainties\n",
    "        model. SC. \n",
    "        ____________________________________________________________________-\n",
    "        Measured values:\n",
    "\n",
    "        initial_dt. OG. t_ini\n",
    "        initial_p. OG. P_ini\n",
    "        final_dt. OG. t_fin\n",
    "        final_p. OG. P_fin\n",
    "\n",
    "        static. OG. Intermediate Isolated parameters \n",
    "        static_vector. OG. Isolated parameters (WITH t_ini, t_fin, P_ini, P_fin)\n",
    "        static_scaled. SC. Isolated parameters (WITH t_ini, t_fin, P_ini, P_fin)\n",
    "\n",
    "        time. OG. All isolated time values\n",
    "        time_intermediate. OG. Intermediate Isolated time values \n",
    "        time_scaled. SC. Intermediate Isolated time values \n",
    "\n",
    "        pol. OG. All isolated polarizations\n",
    "        pol_intermediate. OG. Intermediate Isolated polarization \n",
    "        pol_scaled. SC. Intermediate Isolated polarization \n",
    "\n",
    "        unc. OG. All Isolated uncertainties\n",
    "        unc_intermediate: OG. Intermediate Isolated uncertainties\n",
    "        ____________________________________________________________________\n",
    "        Scaling and slope:\n",
    "        \n",
    "        scaled_isolated_experiments: SC. Intermediate full list\n",
    "        X_static. SC. Isolated parameters (WITH t_ini, t_fin, P_ini, P_fin)\n",
    "        X_time: SC. Intermediate Isolated time values\n",
    "        y_true: SC. Intermediate Isolated polarization\n",
    "        u: OG. Intermediate Isolated uncertainties     \n",
    "\n",
    "        X_static_raw. OG. Isolated parameters (WITH t_ini, t_fin, P_ini, P_fin)           \n",
    "        t0_raw == initial_dt\n",
    "        p0_raw == initial_p\n",
    "        tT_raw == final_dt  \n",
    "        pT_raw == final_p\n",
    "\n",
    "        t0_scaled_correct. SC. t_ini scaled as time\n",
    "        tT_scaled_correct. SC. t_fin scaled as time\n",
    "        p0_scaled_correct. SC. P_ini scaled as polarizations\n",
    "        pT_scaled_correct. SC. P_fin scaled as polarizations\n",
    "\n",
    "        pred_initial_scaled. SC. P_ini predicted\n",
    "        pred_final_scaled. SC. P_fin predicted\n",
    "        p0_pred. OG. P_ini predicted\n",
    "        pT_pred. OG. P_fin predicted\n",
    "\n",
    "        m_raw. OG. Slope in real units\n",
    "        n_raw. OG. Origin in the Polarization axis\n",
    "        y_pred_raw. SC. Corrected polarization predicitons including extremes\n",
    "        mean_pred. OG. Corrected polarization predicitons including extremes\n",
    "        pred_polar == mean_pred\n",
    "        log_var_scaled. SC. Log variance predictions including extremes\n",
    "        var_scaled. SC. Variance predictions including extremes \n",
    "        scale_y. OG. Data-wise polarization range\n",
    "        var_rescaled. OG. Variance predictions including extremes.\n",
    "        pred_sigma. OG. Uncertainty predicitons including extremes.\n",
    "        _______________________________________________________________________\n",
    "\n",
    "        Iso_Interm_Polar_SC: SC. Intermediate Isolated polarization\n",
    "        Iso_Interm_Polar_OG: OG. Intermediate Isolated polarization\n",
    "        Iso_Interm_Uncert_OG: OG. Intermediate Isolated uncertainties\n",
    "        Iso_Interm_Time_SC: SC. Intermediate Isolated time\n",
    "        Iso_Interm_Time_OG: OG. Intermediate Isolated time\n",
    "\n",
    "        _______________________________________________________________________\n",
    "        \n",
    "        Grid_time_OG: OG. Smooth time array \n",
    "        Grid_time_SC: SC. Smooth time array\n",
    "\n",
    "        Grid_polar_Logvar_Sloped_SC: SC. Predicted smooth polarization with logvar array and slope correction\n",
    "        Grid_polar_Logvar_Sloped_OG: OG. Predicted smooth polarization with logvar array and slope correction\n",
    "        Grid_Logvar_Sloped_SC: SC. Predicted smooth logvar array and slope correction\n",
    "        Grid_Var_Sloped_SC: SC. Predicted smooth variance array and slope correction\n",
    "        Grid_Var_Sloped_OG: OG. Predicted smooth variance array and slope correction\n",
    "        Grid_Uncert_Sloped_OG: OG. Predicted smooth uncertainty array and slope correction\n",
    "        Grid_polar_Sloped_OG: OG. Predicted smooth polarization array and slope correction\n",
    "\n",
    "        Grid_polar_Logvar_SC: SC. Predicted smooth polarization with logvar array \n",
    "        Grid_polar_Logvar_OG: OG. Predicted smooth polarization with logvar array \n",
    "        Grid_Logvar_SC: SC. Predicted smooth logvar array\n",
    "        Grid_Var_SC: SC. Predicted smooth variance array\n",
    "        Grid_Var_OG: OG. Predicted smooth variance array\n",
    "        Grid_Uncert_OG: OG. Predicted smooth uncertainty array\n",
    "        Grid_polar_OG: OG. Predicted smooth polarization array\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Prepare the folder where the isolated experiments will reside\n",
    "    log_message(f\"IE: Create the isolated folder \")    \n",
    "    data_dir = Path(data_dir).resolve()\n",
    "    isolated_dir = Path(isolated_dir).resolve()\n",
    "    output_folder = Path(output_folder).resolve()\n",
    "\n",
    "    # Get all *_Parameters.txt files to derive base names\n",
    "    log_message(f\"IE: Obtaining all the names of the experiments\")\n",
    "    param_files = list(data_dir.glob(\"*_Parameters.txt\"))\n",
    "    base_names = [f.stem.replace(\"_Parameters\", \"\") for f in param_files]\n",
    "    \n",
    "    #2. Loop over all pairs of data files. Both the parameter one and the arrays one are needed\n",
    "    for base_name in base_names:\n",
    "        log_message(f\"\\n\\nIE: Begin isolation of {base_name}\")\n",
    "\n",
    "        file_param = data_dir / f\"{base_name}_Parameters.txt\"\n",
    "        file_data = data_dir / f\"{base_name}.txt\"\n",
    "\n",
    "        if not (file_param.exists() and file_data.exists()):\n",
    "            log_message(f\"****IE:      Skipping {base_name}: One of the required files does not exist.\")\n",
    "            continue\n",
    "\n",
    "        # 3. Move both files to isolated folder\n",
    "        shutil.move(long_path(str(file_param)), long_path(str(isolated_dir / file_param.name)))\n",
    "        shutil.move(long_path(str(file_data)), long_path(str(isolated_dir / file_data.name)))\n",
    "        log_message(f\"IE: Successfully sent {base_name} to MLAmorphousIsolatedExperiment\")\n",
    "\n",
    "        # 4. Create subfolder for this experiment\n",
    "        output_subfolder = output_folder / f\"Missing{base_name}\"\n",
    "        output_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "        log_message(f\"IE: Created folder: {output_subfolder}\")\n",
    "\n",
    "        #5. Train and predict using both modes (PolarizationD3\" and \"SoftPolarizationD3\"). If only one is selected change the logic with: for mode in [\"PolarizationD3\", \"SoftPolarizationD3\"]: \n",
    "        for mode in [\"PolarizationD3\"]: \n",
    "            log_message(f\"IE: Processing mode: {mode}\")\n",
    "\n",
    "            #6. Load all experiments and augment them. These experiments will only be used for training the model and creating the scalers\n",
    "            log_message(f\"IE: Load all other experiments and augment them {num_augmentations}\")            \n",
    "            experiments, static_columns = load_experiments(data_dir, polarization_column=mode)\n",
    "            encoded_experiments = augment_experiments(experiments, num_augmentations, base_seed=42)\n",
    "\n",
    "            #7. Scalers. All polarization and time values are scaled using all the experiments (except the isolated one). The reason for excluding them is to avoid information\n",
    "            #leacks. The static parameters are scaled afterwards but they are also scaled with all but one experiments\n",
    "            log_message(f\"IE: Scaling all training experiments...\")\n",
    "            all_time = []\n",
    "            all_y = []\n",
    "            \n",
    "            for static, time, pol, _ in encoded_experiments:\n",
    "                all_time.append(time)\n",
    "                all_y.append(pol)\n",
    "            \n",
    "            all_time = np.concatenate(all_time).reshape(-1, 1) #unscaled\n",
    "            all_y = np.concatenate(all_y).reshape(-1, 1) #unscaled\n",
    "            \n",
    "            scaler_time = MinMaxScaler().fit(all_time) #fit to all\n",
    "            scaler_y = MinMaxScaler().fit(all_y) #fit to all\n",
    "            \n",
    "            #8. Prepare all the experiments so that they can be introduced in the training functions\n",
    "            X_static_raw, X_time_raw, y_raw, u = build_dataset(encoded_experiments, mode=mode)\n",
    "            scaler_static = MinMaxScaler().fit(X_static_raw) #fit to all\n",
    "\n",
    "            if LogNoise:\n",
    "                log_message(f\"IE:      Scaler static min_: , {scaler_static.min_}\")\n",
    "                log_message(f\"IE:      Scaler static scale_: , {scaler_static.scale_}\")\n",
    "                log_message(f\"IE:      Scaler static data_min_: , {scaler_static.data_min_}\")\n",
    "                log_message(f\"IE:      Scaler static data_max_: , {scaler_static.data_max_}\")\n",
    "                \n",
    "                log_message(f\"IE:      Scaler time min_: , {scaler_time.min_}\")\n",
    "                log_message(f\"IE:      Scaler time scale_: , {scaler_time.scale_}\")\n",
    "                log_message(f\"IE:      Scaler time data_min_: , {scaler_time.data_min_}\")\n",
    "                log_message(f\"IE:      Scaler time data_max_: , {scaler_time.data_max_}\")\n",
    "                \n",
    "                log_message(f\"IE:      Scaler polarization min_: , {scaler_y.min_}\")\n",
    "                log_message(f\"IE:      Scaler polarization scale_: , {scaler_y.scale_}\")\n",
    "                log_message(f\"IE:      Scaler polarization data_min_: , {scaler_y.data_min_}\")\n",
    "                log_message(f\"IE:      Scaler polarization data_max_: , {scaler_y.data_max_}\")\n",
    "            \n",
    "            #9. Save the scalers. This is useful because otherwise we have no way to use the model outside of the loop (because we don't know how to unscale the values)\n",
    "            joblib.dump(scaler_static, long_path(output_subfolder / f\"scaler_static_{mode}.pkl\"))\n",
    "            joblib.dump(scaler_time, long_path(output_subfolder / f\"scaler_time_{mode}.pkl\"))\n",
    "            joblib.dump(scaler_y, long_path(output_subfolder / f\"scaler_y_{mode}.pkl\"))\n",
    "            log_message(f\"Saved scalers for mode {mode} to: {output_subfolder}\")\n",
    "\n",
    "            #10. Train and save the model on the augmented experiments. The experiments enter unscaled and get scaled inside)\n",
    "            log_message(f\"IE: Train the model\")\n",
    "            model = train(\n",
    "                encoded_experiments=encoded_experiments,  \n",
    "                scaler_static=scaler_static,\n",
    "                scaler_time=scaler_time,\n",
    "                scaler_y=scaler_y,\n",
    "                use_uncertainty=use_uncertainty)\n",
    "\n",
    "            model_path = output_subfolder / f\"model_{mode}.keras\"\n",
    "            model.save(long_path(model_path))\n",
    "            log_message(f\"IE: Saved model to: {model_path}\")\n",
    "\n",
    "            #11. Load isolated experiment. Predictions will be done on this experiment. Aligning is required\n",
    "            log_message(f\"IE: Load isolated experiment: {base_name}\")\n",
    "            isolated_experiments, static_columns_isolated = load_experiments(isolated_dir, polarization_column=mode)\n",
    "            isolated_experiments_aligned = align_static_vectors(\n",
    "                isolated_experiments,\n",
    "                static_columns_training=static_columns,\n",
    "                static_columns_isolated=static_columns_isolated) #all unscaled\n",
    "            if LogNoise:\n",
    "                log_message(f\"IE:      Number of isolated experiments aligned (should be one): {len(isolated_experiments_aligned)}\")\n",
    "                log_message(f\"IE:      Example static vector (pre-scale): {isolated_experiments_aligned[0][0]}\")\n",
    "                log_message(f\"IE:      Example time vector (pre-scale): {isolated_experiments_aligned[0][1]}\")\n",
    "                log_message(f\"IE:      Example polarization vector (pre-scale): {isolated_experiments_aligned[0][2]}\")\n",
    "\n",
    "            #12. Building manually the dataset for the isolated experiment\n",
    "            log_message(f\"IE: Scaling static vector for isolated experiment. Formatting the isolated experiment.\")\n",
    "            for static, time, pol, unc in isolated_experiments_aligned: #all unscaled\n",
    "                if len(time) < 2:\n",
    "                    log_message(f\"****IE: Skipping experiment {i} due to insufficient time points\")\n",
    "                    continue  # skip too-short experiments\n",
    "            \n",
    "                #13 Extract the initial and final points, add them as parameters and scale the entire static vector with the general scaler\n",
    "                initial_dt, initial_p = time[0], pol[0]\n",
    "                final_dt, final_p = time[-1], pol[-1]\n",
    "                static_vector = static + [initial_dt, initial_p, final_dt, final_p] #unscaled\n",
    "                if LogNoise:\n",
    "                    log_message(f\"IE:  Experiment static vector length (pre-scale): {len(static_vector)}\")\n",
    "                    log_message(f\"IE:  Experiment static vector (pre-scale): {static_vector}\")\n",
    "                static_scaled = scaler_static.transform(np.array(static_vector).reshape(1, -1)).flatten() #scaled\n",
    "                if LogNoise:\n",
    "                    log_message(f\"IE:  Experiment static vector (scaled): {static_scaled}\")\n",
    "                \n",
    "                #14 Scale time and polarization arrays (only intermediate points). This is compatible with the scaling done to the rest of the experiments\n",
    "                time_intermediate = np.array(time[1:-1]).reshape(-1, 1) # unscaled\n",
    "                time_scaled = scaler_time.transform(time_intermediate).flatten() #scaled\n",
    "                if LogNoise:\n",
    "                    log_message(f\"IE:  Experiment time (pre-scale): {time_intermediate.flatten()}\")\n",
    "                    log_message(f\"IE:  Experiment time (scaled): {time_scaled}\")\n",
    "                pol_intermediate = np.array(pol[1:-1]).reshape(-1, 1) #unscaled\n",
    "                pol_scaled = scaler_y.transform(pol_intermediate).flatten() #scaled\n",
    "                if LogNoise:\n",
    "                    log_message(f\"IE:  Experiment polarization (pre-scale): {pol_intermediate.flatten()}\")\n",
    "                    log_message(f\"IE:  Experiment polarization (scaled): {pol_scaled}\")\n",
    "                unc_intermediate = np.array(unc[1:-1]).reshape(-1, 1) #unscaled\n",
    "                if LogNoise:\n",
    "                    log_message(f\"IE:  Experiment uncertainty: {unc_intermediate.flatten()}\")\n",
    "\n",
    "                \n",
    "                #15. Combine all structures so that the shape is correct for prediction\n",
    "                scaled_isolated_experiments = []\n",
    "                scaled_isolated_experiments.append((\n",
    "                    static_scaled, time_scaled, pol_scaled, unc_intermediate.flatten()\n",
    "                )) #scaled\n",
    "                \n",
    "                \n",
    "                #16 Check for shapes and create the final NumPy arrays of the experiment\n",
    "                X_static, X_time, y_true, u = [], [], [], []\n",
    "                for static_scaled, time_scaled, pol_scaled, unc_intermediate in scaled_isolated_experiments:\n",
    "                    for t, p, err in zip(time_scaled, pol_scaled, unc_intermediate):\n",
    "                        X_static.append(static_scaled)   # already includes appended time/polarization info\n",
    "                        X_time.append([t])               # wrap to preserve 2D structure for Conv1D\n",
    "                        y_true.append(p)\n",
    "                        u.append(err)\n",
    "                X_static = np.array(X_static) #scaled\n",
    "                X_time = np.array(X_time) #scaled\n",
    "                y_true = np.array(y_true).reshape(-1, 1) #scaled\n",
    "                u = np.array(u).reshape(-1, 1) #unscaled\n",
    "\n",
    "                if LogNoise:\n",
    "                    log_message(f\"  X_static.shape: {X_static.shape}\")\n",
    "                    log_message(f\"  X_time.shape: {X_time.shape}\")\n",
    "                    log_message(f\"  y_true.shape: {y_true.shape}\")\n",
    "                    log_message(f\"  X_static[0]: {X_static[0]} (scaled)\")\n",
    "\n",
    "            #17 Extract the initial and final polarizations and keep a scaled and unscaled version of all. Then predict at those time points. \n",
    "            #Unscaling needs to be done with static_scaler but then we need to scale them as polarizations or as time points with their respective scalers so that we can use model_predict\n",
    "            log_message(f\"IE:   Use slope correction.\")\n",
    "            X_static_raw = scaler_static.inverse_transform(X_static)\n",
    "            # Extract static input (already scaled)\n",
    "            x_static_single = X_static[0:1]  # shape (1, D_static)\n",
    "            t0_raw = X_static_raw[0, -4].reshape(1, -1) #We don't use initial_dt, final_dt,... to guarantee that they are model-like values to help the model identify them clearer\n",
    "            p0_raw = X_static_raw[0, -3].reshape(1, -1)\n",
    "            tT_raw = X_static_raw[0, -2].reshape(1, -1)\n",
    "            pT_raw = X_static_raw[0, -1].reshape(1, -1)\n",
    "\n",
    "            t0_scaled_correct = scaler_time.transform(t0_raw)\n",
    "            tT_scaled_correct = scaler_time.transform(tT_raw)\n",
    "            p0_scaled_correct = scaler_y.transform(p0_raw)\n",
    "            pT_scaled_correct = scaler_y.transform(pT_raw)\n",
    "            \n",
    "            # Predict scaled polarizations at those time points\n",
    "            pred_initial_scaled = model_prediction(model, x_static_single, t0_scaled_correct)\n",
    "            pred_final_scaled   = model_prediction(model, x_static_single, tT_scaled_correct)\n",
    "            \n",
    "            # Inverse transform predictions to physical (real) polarizations\n",
    "            p0_pred = scaler_y.inverse_transform(pred_initial_scaled[:, 0:1])\n",
    "            pT_pred = scaler_y.inverse_transform(pred_final_scaled[:, 0:1])\n",
    "\n",
    "            if LogNoise:\n",
    "                log_message(f\"t0 (real) = {t0_raw}, tT (real) = {tT_raw}\")\n",
    "                log_message(f\"p0_pred (real) = {p0_pred}, pT_pred (real) = {pT_pred}\")\n",
    "                log_message(f\"p0 (true) = {p0_raw}, pT (true) = {pT_raw}\")\n",
    "\n",
    "\n",
    "            #18 Prepare the linear correction and predict in the time points where we have data\n",
    "            m_raw = ( (p0_pred-p0_raw) - (pT_pred-pT_raw) ) / (t0_raw-tT_raw)\n",
    "            n_raw = (p0_pred-p0_raw) - m_raw * t0_raw\n",
    "            if Correction == True:\n",
    "                log_message(f\"IE:   Fixing initial and final points by substracting P(t) = {float(np.squeeze(m_raw)):.3e} * t + {float(np.squeeze(n_raw)):.3e}\")\n",
    "                y_pred_raw = model_predict_sloped(model, X_static, X_time, m_raw, n_raw, scaler_y, scaler_time) #Input es scaled, parametros unsacles, output es scaled también\n",
    "            else:\n",
    "                y_pred_raw = model_prediction(model, X_static, X_time)\n",
    "                \n",
    "            #To obtain the results we separate the value predictions form the log variance. The results need to be unscaled to real units while the uncertainty needs to be \n",
    "            #exponenciated (to obtain a variance), multiplied by the same factor that is used in MinMaxScaler and finally convert to uncertainty (take the square root)\n",
    "            mean_pred = scaler_y.inverse_transform(y_pred_raw[:, 0:1]).flatten()\n",
    "            log_var_scaled = y_pred_raw[:, 1]  # Keep in original scale\n",
    "            var_scaled = np.exp(log_var_scaled)\n",
    "            scale_y = scaler_y.data_max_[0] - scaler_y.data_min_[0]  # scale factor from MinMaxScaler \n",
    "            # Variance rescales by the square of the scale factor\n",
    "            var_rescaled = var_scaled * (scale_y ** 2)\n",
    "            pred_sigma = np.sqrt(var_rescaled)\n",
    "            pred_polar = mean_pred\n",
    "\n",
    "\n",
    "            #19. Plot the raw values (black), the predictions for those time points (blue) and predictions every 1000 seconds (red if there is no correction, green if there is correction)\n",
    "            if use_uncertainty:\n",
    "                suffix=\"Uncertainty\"\n",
    "            else:\n",
    "                suffix=\"noUncertainty\"\n",
    "            #Begin plotting:\n",
    "            color_true = \"black\"\n",
    "            color_pred = \"blue\"\n",
    "            color_grid = \"green\"\n",
    "            color_raw = \"red\"\n",
    "            log_message(f\"IE:   Plot raw values, predictions for those time points and predictions every 1000 seconds.\")\n",
    "\n",
    "            #Flatten inputs\n",
    "            Iso_Interm_Polar_SC = y_true.flatten() #scaled\n",
    "            Iso_Interm_Uncert_OG = u.flatten() #unscaled\n",
    "            Iso_Interm_Time_SC = X_time.flatten()  #scaled\n",
    "            # Reshape to (-1, 1) for scaler inverse transform\n",
    "            Iso_Interm_Polar_SC_reshaped = Iso_Interm_Polar_SC.reshape(-1, 1)\n",
    "            Iso_Interm_Time_SC_reshaped = Iso_Interm_Time_SC.reshape(-1, 1)\n",
    "            \n",
    "            # Inverse transform to get unscaled values\n",
    "            Iso_Interm_Polar_OG = scaler_y.inverse_transform(Iso_Interm_Polar_SC_reshaped).flatten()\n",
    "            Iso_Interm_Time_OG = scaler_time.inverse_transform(Iso_Interm_Time_SC_reshaped).flatten()\n",
    "\n",
    "            # Define grid for red line\n",
    "\n",
    "            Grid_time_OG = np.arange(0, max(Iso_Interm_Time_OG) + 1000, 1000).reshape(-1, 1)\n",
    "            \n",
    "            static_scaled = np.tile(X_static[0], (len(Grid_time_OG), 1)) #SCALED\n",
    "\n",
    "            Grid_time_SC = scaler_time.transform(Grid_time_OG)\n",
    "\n",
    "            if Correction == True:\n",
    "                Grid_polar_Logvar_Sloped_SC = model_predict_sloped(model, static_scaled, Grid_time_SC, m_raw, n_raw, scaler_y, scaler_time) #scaled\n",
    "                Grid_polar_Sloped_OG = scaler_y.inverse_transform(Grid_polar_Logvar_Sloped_SC[:, 0:1]) #unscaled\n",
    "                Grid_Logvar_Sloped_SC = Grid_polar_Logvar_Sloped_SC[:, 1:2]  # Don't transform this\n",
    "                Grid_Var_Sloped_SC = np.exp(Grid_Logvar_Sloped_SC)\n",
    "                # Variance rescales by the square of the scale factor\n",
    "                Grid_Var_Sloped_OG = Grid_Var_Sloped_SC * (scale_y ** 2)\n",
    "                Grid_Uncert_Sloped_OG = np.sqrt(Grid_Var_Sloped_OG) #unscaled    \n",
    "\n",
    "            Grid_polar_Logvar_SC = model_prediction(model, static_scaled, Grid_time_SC) #scaled\n",
    "            Grid_polar_OG = scaler_y.inverse_transform(Grid_polar_Logvar_SC[:, 0:1]) #unscaled\n",
    "            Grid_Logvar_SC = Grid_polar_Logvar_SC[:, 1:2]  # Don't transform this\n",
    "            Grid_Var_SC = np.exp(Grid_Logvar_SC)\n",
    "            # Variance rescales by the square of the scale factor\n",
    "            Grid_Var_OG = Grid_Var_SC * (scale_y ** 2)\n",
    "            Grid_Uncert_OG = np.sqrt(Grid_Var_OG) #unscaled   \n",
    "                \n",
    "           \n",
    "            fig, ax = plt.subplots(figsize=(8, 5))\n",
    "            # base_name = \"PolarizationD3_CaFeAl_1_5_6_24_0_MillerIndex_(0,0,2)\"\n",
    "            \n",
    "            # Split by underscores\n",
    "            parts = base_name.split(\"_\")\n",
    "            \n",
    "            # Extract the fixed portion you want: \"CaFeAl_1_5_6_24_0\"\n",
    "            core_name = \"_\".join(parts[1:6])  \n",
    "            \n",
    "            # Build new title\n",
    "            title = f\"{core_name}_{Complexity}_{num_augmentations}\"\n",
    "            ax.set_title(title)\n",
    "            ax.grid(True)\n",
    "            \n",
    "            # Black dots with error bars (truth). Using real units\n",
    "            ax.scatter(Iso_Interm_Time_OG, Iso_Interm_Polar_OG, color=color_true, marker='o', s=40, label=\"Measurements\")\n",
    "            ax.errorbar(Iso_Interm_Time_OG, Iso_Interm_Polar_OG, yerr=Iso_Interm_Uncert_OG, fmt='none',\n",
    "                        ecolor=color_true, alpha=0.6, capsize=3, label=\"Measured ±σ\")\n",
    "            \n",
    "            # Blue dots with model error. should already be in real units\n",
    "            ax.scatter(Iso_Interm_Time_OG, mean_pred, color=color_pred, marker='x', s=40, label=\"Prediction\")\n",
    "            ax.errorbar(\n",
    "                Iso_Interm_Time_OG,\n",
    "                mean_pred,\n",
    "                yerr=pred_sigma,\n",
    "                fmt='none',\n",
    "                ecolor=color_pred,\n",
    "                alpha=0.6,\n",
    "                capsize=3,\n",
    "                label=\"Prediction ±σ\"\n",
    "            )\n",
    "            \n",
    "            # Green line + band for prediction grid\n",
    "            ax.scatter(Grid_time_OG.flatten(), Grid_polar_Sloped_OG.flatten(), color=color_grid, alpha=0.7, label=\"Smooth Prediction\", marker='x', s=40)\n",
    "            ax.fill_between(\n",
    "                Grid_time_OG.flatten(),\n",
    "                (Grid_polar_Sloped_OG - Grid_Uncert_Sloped_OG).flatten(),\n",
    "                (Grid_polar_Sloped_OG + Grid_Uncert_Sloped_OG).flatten(),\n",
    "                color=color_grid,\n",
    "                alpha=0.2,\n",
    "                label=\"Smooth Prediction ±1σ\")\n",
    "            \n",
    "            #Red for no corrections\n",
    "            ax.scatter(Grid_time_OG.flatten(), Grid_polar_OG.flatten(), color=color_raw, alpha=0.7, label=\"Smooth no Correction\", marker='x', s=40)\n",
    "            ax.fill_between(\n",
    "                Grid_time_OG.flatten(),\n",
    "                (Grid_polar_OG - Grid_Uncert_OG).flatten(),\n",
    "                (Grid_polar_OG + Grid_Uncert_OG).flatten(),\n",
    "                color='red',\n",
    "                alpha=0.1,\n",
    "                label=\"Smooth no Correction ±1σ\")\n",
    "            \n",
    "            \n",
    "            y_min = np.min(Iso_Interm_Polar_OG - Iso_Interm_Uncert_OG) - 0.02\n",
    "            y_max = np.max(Iso_Interm_Polar_OG + Iso_Interm_Uncert_OG) + 0.02\n",
    "            ax.set_ylim([y_min, y_max])\n",
    "            \n",
    "            ax.legend()\n",
    "            fig.tight_layout()\n",
    "            \n",
    "            # Save Figure\n",
    "            fig_path = output_subfolder / f\"Missing_{base_name}.jpg\"\n",
    "            save_path = long_path(str(fig_path))\n",
    "            fig.savefig(save_path, dpi=100)\n",
    "            plt.close(fig)\n",
    "            del fig, ax\n",
    "            gc.collect()\n",
    "            log_message(f\"IE:   Saved figure to: {fig_path}\")\n",
    "            \"\"\"\n",
    "            # Plot difference\n",
    "            delta_polar = mean_pred = mean_pred - Iso_Interm_Polar_OG\n",
    "            delta_uncertainty = np.sqrt(pred_sigma**2 + Iso_Interm_Uncert_OG**2)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(8, 5))\n",
    "            ax.set_title(f\"Difference_{base_name}\")\n",
    "            ax.grid(True)\n",
    "            ax.scatter(Iso_Interm_Time_OG, delta_polar, color=\"blue\", marker=\"o\", s=40, label=\"Predicted - True\")\n",
    "            ax.errorbar(Iso_Interm_Time_OG, delta_polar, yerr=delta_uncertainty, fmt='none', ecolor=\"blue\", alpha=0.6, capsize=3, label=\"±σ\")\n",
    "            ax.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "            ax.set_xlabel(\"DeltaTime\")\n",
    "            ax.set_ylabel(\"Predicted - True Polarization\")\n",
    "            ax.legend()\n",
    "            fig.tight_layout()\n",
    "            \n",
    "            fig_path = output_subfolder / f\"Difference_{base_name}.jpg\"\n",
    "            save_path = long_path(str(fig_path))\n",
    "            fig.savefig(save_path, dpi=100)\n",
    "            plt.close(fig)\n",
    "            log_message(f\"IE:   Saved difference plot to: {fig_path}\")\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            #20. Save data\n",
    "            diff_data_path = output_subfolder / f\"Difference_{base_name}.txt\"\n",
    "            with open(long_path(diff_data_path), 'w') as f:\n",
    "                f.write(\"DeltaTime\\tDeltaPolarization\\tDeltaUncertainty\\n\")\n",
    "                for t, dp, du in zip(Iso_Interm_Time_OG, delta_polar, delta_uncertainty):\n",
    "                    f.write(f\"{t:.6f}\\t{dp:.6f}\\t{du:.6f}\\n\")\n",
    "            log_message(f\"IE:   Saved difference data to: {diff_data_path}\")\n",
    "            \"\"\"\n",
    "            raw_data_path = output_subfolder / f\"RawData_{mode}_Missing{base_name}.txt\"\n",
    "            with open(long_path(raw_data_path), 'w') as f:\n",
    "                f.write(\"DeltaTime\\tRealPolarizationD3\\tErrRealPolarizationD3\\n\")\n",
    "                for t, p, e in zip(time_intermediate, pol_intermediate, unc_intermediate):\n",
    "                    f.write(f\"{float(t):.6f}\\t{float(p):.6f}\\t{float(e):.6f}\\n\")\n",
    "            log_message(f\"IE:   Saved raw data to: {raw_data_path}\")\n",
    "\n",
    "            predicted_data_path = output_subfolder / f\"PredictedData_{mode}_Missing{base_name}.txt\"\n",
    "            with open(long_path(predicted_data_path), 'w') as f:\n",
    "                f.write(\"GridTime\\tPredictedPolarizationD3\\tErrPredictedPolarizationD3\\n\")\n",
    "                for t, p, s in zip(Grid_time_OG.flatten(), Grid_polar_Sloped_OG.flatten(), Grid_Uncert_Sloped_OG.flatten()):\n",
    "                    f.write(f\"{t:.6f}\\t{p:.6f}\\t{s:.6f}\\n\")\n",
    "            log_message(f\"IE:   Saved predicted data to: {predicted_data_path}\")\n",
    "\n",
    "            # Save predicted values at original timepoints (green points)\n",
    "            predicted_points_path = output_subfolder / f\"PredictedPoints_{mode}_Missing{base_name}.txt\"\n",
    "            with open(long_path(predicted_points_path), 'w') as f:\n",
    "                f.write(\"DeltaTime\\tPredictedPolarizationD3\\tErrPredictedPolarizationD3\\n\")\n",
    "                for t, p, s in zip(Iso_Interm_Time_OG, pred_polar, pred_sigma):\n",
    "                    f.write(f\"{t:.6f}\\t{p:.6f}\\t{s:.6f}\\n\")\n",
    "            log_message(f\"IE:   Saved predicted point data to: {predicted_points_path}\")\n",
    "            import psutil, os\n",
    "            process = psutil.Process(os.getpid())\n",
    "\n",
    "            def mem():\n",
    "                return process.memory_info().rss / 1e9\n",
    "            tf.keras.backend.clear_session()\n",
    "            del model\n",
    "            gc.collect()\n",
    "            print(f\"Memory after cleanup: {mem():.2f} GB\")\n",
    "\n",
    "        #21. Move all experiments back to the original folder\n",
    "        shutil.move(long_path(isolated_dir / os.path.basename(file_param.name)), long_path(file_param))\n",
    "        shutil.move(long_path(isolated_dir / os.path.basename(file_data.name)), long_path(file_data))\n",
    "        log_message(f\"IE: Returned {base_name} to MLDataBase\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af8b46-8194-43a4-8ba1-677a594202ad",
   "metadata": {},
   "source": [
    "## 7 Main Code\n",
    "\n",
    "Here we just have a code cell that uses isolate_experiments to do the Leave-One-Out process. It creates the folders and loops over all model names and number fo augmentations. For each combination, it runs the _isolated\\_experiments_ function and records the time it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633981cf-b021-4125-b6dd-3ac758d8bc90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Naif241016'}\n",
      "WARNING:tensorflow:From C:\\Users\\gopeb\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Memory after cleanup: 0.43 GB\n",
      "Memory after cleanup: 0.35 GB\n",
      "Memory after cleanup: 0.37 GB\n",
      "Memory after cleanup: 0.39 GB\n",
      "Memory after cleanup: 0.41 GB\n",
      "Memory after cleanup: 0.39 GB\n",
      "WARNING:tensorflow:5 out of the last 32 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022A350D4EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Memory after cleanup: 0.36 GB\n",
      "Memory after cleanup: 0.37 GB\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022A37974EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Memory after cleanup: 0.39 GB\n",
      "Memory after cleanup: 0.41 GB\n",
      "Memory after cleanup: 0.41 GB\n",
      "Memory after cleanup: 0.43 GB\n",
      "Memory after cleanup: 0.44 GB\n",
      "Memory after cleanup: 0.45 GB\n",
      "Memory after cleanup: 0.47 GB\n",
      "Memory after cleanup: 0.48 GB\n",
      "Memory after cleanup: 0.45 GB\n",
      "Memory after cleanup: 0.47 GB\n",
      "Memory after cleanup: 0.48 GB\n",
      "Memory after cleanup: 0.46 GB\n",
      "Memory after cleanup: 0.48 GB\n",
      "Memory after cleanup: 0.49 GB\n",
      "Memory after cleanup: 0.51 GB\n",
      "Memory after cleanup: 0.48 GB\n",
      "Memory after cleanup: 0.49 GB\n",
      "Memory after cleanup: 0.48 GB\n",
      "Memory after cleanup: 0.50 GB\n",
      "Memory after cleanup: 0.49 GB\n",
      "{'Naif241032'}\n",
      "Memory after cleanup: 0.50 GB\n",
      "Memory after cleanup: 0.52 GB\n",
      "Memory after cleanup: 0.51 GB\n",
      "Memory after cleanup: 0.46 GB\n",
      "Memory after cleanup: 0.49 GB\n",
      "Memory after cleanup: 0.49 GB\n",
      "Memory after cleanup: 0.50 GB\n",
      "Memory after cleanup: 0.49 GB\n",
      "Memory after cleanup: 0.51 GB\n",
      "Memory after cleanup: 0.53 GB\n",
      "Memory after cleanup: 0.54 GB\n",
      "Memory after cleanup: 0.54 GB\n",
      "Memory after cleanup: 0.54 GB\n",
      "Memory after cleanup: 0.56 GB\n",
      "Memory after cleanup: 0.56 GB\n",
      "Memory after cleanup: 0.53 GB\n",
      "Memory after cleanup: 0.54 GB\n",
      "Memory after cleanup: 0.56 GB\n",
      "Memory after cleanup: 0.58 GB\n",
      "Memory after cleanup: 0.60 GB\n",
      "Memory after cleanup: 0.61 GB\n",
      "Memory after cleanup: 0.60 GB\n",
      "Memory after cleanup: 0.62 GB\n",
      "Memory after cleanup: 0.63 GB\n",
      "Memory after cleanup: 0.65 GB\n",
      "Memory after cleanup: 0.66 GB\n",
      "Memory after cleanup: 0.64 GB\n",
      "Memory after cleanup: 0.65 GB\n",
      "{'NaifTwice1D883316'}\n",
      "Memory after cleanup: 0.69 GB\n",
      "Memory after cleanup: 0.71 GB\n",
      "Memory after cleanup: 0.73 GB\n",
      "Memory after cleanup: 0.71 GB\n",
      "Memory after cleanup: 0.73 GB\n",
      "Memory after cleanup: 0.75 GB\n",
      "Memory after cleanup: 0.77 GB\n",
      "Memory after cleanup: 0.78 GB\n",
      "Memory after cleanup: 0.80 GB\n",
      "Memory after cleanup: 0.82 GB\n",
      "Memory after cleanup: 0.84 GB\n",
      "Memory after cleanup: 0.85 GB\n",
      "Memory after cleanup: 0.87 GB\n",
      "Memory after cleanup: 0.89 GB\n",
      "Memory after cleanup: 0.90 GB\n",
      "Memory after cleanup: 0.91 GB\n",
      "Memory after cleanup: 0.93 GB\n",
      "Memory after cleanup: 0.94 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14380/1026900024.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mlog_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Folder does not exist: {folder_to_delete}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0moutput_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"..\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"ML\"\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"Tests\"\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34mf\"CrystallineAllTestsFolder_NOVALNOLOSS{Complexity}_{num_augmentations}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         isolate_experiments(\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mlong_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mlong_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misolated_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14380/927447702.py\u001b[0m in \u001b[0;36misolate_experiments\u001b[1;34m(data_dir, isolated_dir, output_folder, use_uncertainty, num_augmentations, Correction, build_model)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;31m#10. Train and save the model on the augmented experiments. The experiments enter unscaled and get scaled inside)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[0mlog_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"IE: Train the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m             model = train(\n\u001b[0m\u001b[0;32m    239\u001b[0m                 \u001b[0mencoded_experiments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoded_experiments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0mscaler_static\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaler_static\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14380/3612076882.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(encoded_experiments, scaler_static, scaler_time, scaler_y, use_uncertainty)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mlog_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"    train: Training final model with epochs={epochs}, batch_size={batch_size} and use_uncertainty = {use_uncertainty}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mmodel_fitting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_static_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_time_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14380/3612076882.py\u001b[0m in \u001b[0;36mmodel_fitting\u001b[1;34m(model, X_static_scaled, X_time_scaled, y_scaled, epochs, batch_size, verbose, callbacks)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \"\"\"\n\u001b[0;32m     21\u001b[0m     \u001b[0mlog_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"    Model_fitting: Training the model.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     return model.fit(\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mX_static_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_time_scaled\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0my_scaled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py\u001b[0m in \u001b[0;36m_enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_seen\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_seen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    707\u001b[0m             \u001b[1;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m             \"not be specified.\")\n\u001b[1;32m--> 709\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    746\u001b[0m             self._flat_output_types)\n\u001b[0;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3476\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3477\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3478\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3479\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   3480\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "isolated_dir = Path(\"../FileReadingStoring/CrystallineMLIsolatedExperiment\").resolve()\n",
    "data_dir = Path(\"../FileReadingStoring/CrystallineMLDataBase/\").resolve()\n",
    "log_file = Path.cwd().resolve() / \"CrystallineExecution_time.txt\"\n",
    "log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "os.makedirs(long_path(isolated_dir), exist_ok=True)\n",
    "os.makedirs(long_path(data_dir), exist_ok=True) \n",
    "Correction = True\n",
    "use_uncertainty = True\n",
    "\n",
    "# Move all files back in case some files got left behind on the isolated folder\n",
    "for file_path in isolated_dir.iterdir():  # iterates Path objects\n",
    "    if file_path.is_file():  # only move files\n",
    "        dst = data_dir / file_path.name\n",
    "        shutil.move(long_path(file_path), long_path(dst))\n",
    "        log_message(f\"Moved {file_path.name} back to {data_dir}\")\n",
    "#\"Complex\", \"Simple\", \"Naif\", \"Naif834\", \"Naif838\", \"Naif8316\", \"Naif8332\", \"Naif854\", \"Naif858\", \"Naif8516\", \"Naif8532\", \"Naif8104\", \"Naif8108\", \"Naif81016\", \"Naif81032\", \"Naif1634\", \"Naif1638\", \"Naif16316\", \"Naif16332\", \"Naif1654\", \"Naif1658\", \"Naif16516\", \"Naif16532\", \"Naif16104\", \"Naif16108\", \"Naif161016\", \"Naif161032\", \"Naif2434\", \"Naif2438\", \"Naif24316\", \"Naif24332\", \"Naif2454\", \"Naif2458\", \"Naif24516\", \"Naif24532\", \"Naif24104\", \"Naif24108\", \n",
    "for Complexity in [\"Naif241016\", \"Naif241032\", \"NaifTwice1D883316\", \"NaifTwice1D883332\", \"NaifTwice1D883516\", \"NaifTwice1D883532\", \"NaifTwice1D885316\", \"NaifTwice1D885332\", \"NaifTwice1D885516\", \"NaifTwice1D885532\", \"NaifTwice1D843316\", \"NaifTwice1D843332\", \"NaifTwice1D843516\", \"NaifTwice1D843532\", \"NaifTwice1D845316\", \"NaifTwice1D845332\", \"NaifTwice1D845516\", \"NaifTwice1D845532\", \"NaifTwice1D483316\", \"NaifTwice1D483332\", \"NaifTwice1D483516\", \"NaifTwice1D483532\", \"NaifTwice1D485316\", \"NaifTwice1D485332\", \"NaifTwice1D485516\", \"NaifTwice1D485532\", \"NaifTwice1D443316\", \"NaifTwice1D443332\", \"NaifTwice1D443516\", \"NaifTwice1D443532\", \"NaifTwice1D445316\", \"NaifTwice1D445332\", \"NaifTwice1D445516\", \"NaifTwice1D445532\", \"NaifTwiceDense414\", \"NaifTwiceDense418\", \"NaifTwiceDense4116\", \"NaifTwiceDense4124\", \"NaifTwiceDense814\", \"NaifTwiceDense818\", \"NaifTwiceDense8116\", \"NaifTwiceDense8124\", \"NaifTwiceDense1614\", \"NaifTwiceDense1618\", \"NaifTwiceDense16116\", \"NaifTwiceDense16124\", \"NaifTwiceDense2414\", \"NaifTwiceDense2418\", \"NaifTwiceDense24116\", \"NaifTwiceDense24124\", \"NaifTwiceDense424\", \"NaifTwiceDense428\", \"NaifTwiceDense4216\", \"NaifTwiceDense4224\", \"NaifTwiceDense824\", \"NaifTwiceDense828\", \"NaifTwiceDense8216\", \"NaifTwiceDense8224\", \"NaifTwiceDense1624\", \"NaifTwiceDense1628\", \"NaifTwiceDense16216\", \"NaifTwiceDense16224\", \"NaifTwiceDense2424\", \"NaifTwiceDense2428\", \"NaifTwiceDense24216\", \"NaifTwiceDense24224\", \"SimpleNoDropout444\", \"SimpleNoDropout448\", \"SimpleNoDropout484\", \"SimpleNoDropout488\", \"SimpleNoDropout4164\", \"SimpleNoDropout4168\", \"SimpleNoDropout844\", \"SimpleNoDropout848\", \"SimpleNoDropout884\", \"SimpleNoDropout888\", \"SimpleNoDropout8164\", \"SimpleNoDropout8168\", \"SimpleNoDropout1644\", \"SimpleNoDropout1648\", \"SimpleNoDropout1684\", \"SimpleNoDropout1688\", \"SimpleNoDropout16164\", \"SimpleNoDropout16168\", \"Simple414414\", \"Simple414418\", \"Simple414424\", \"Simple414428\", \"Simple414814\", \"Simple414818\", \"Simple414824\", \"Simple414828\", \"Simple418414\", \"Simple418418\", \"Simple418424\", \"Simple418428\", \"Simple418814\", \"Simple418818\", \"Simple418824\", \"Simple418828\", \"Simple424414\", \"Simple424418\", \"Simple424424\", \"Simple424428\", \"Simple424814\", \"Simple424818\", \"Simple424824\", \"Simple424828\", \"Simple428414\", \"Simple428418\", \"Simple428424\", \"Simple428428\", \"Simple428814\", \"Simple428818\", \"Simple428824\", \"Simple428828\", \"Simple814414\", \"Simple814418\", \"Simple814424\", \"Simple814428\", \"Simple814814\", \"Simple814818\", \"Simple814824\", \"Simple814828\", \"Simple818414\", \"Simple818418\", \"Simple818424\", \"Simple818428\", \"Simple818814\", \"Simple818818\", \"Simple818824\", \"Simple818828\", \"Simple824414\", \"Simple824418\", \"Simple824424\", \"Simple824428\", \"Simple824814\", \"Simple824818\", \"Simple824824\", \"Simple824828\", \"Simple828414\", \"Simple828418\", \"Simple828424\", \"Simple828428\", \"Simple828814\", \"Simple828818\", \"Simple828824\", \"Simple828828\"]:\n",
    "    build_model = Define_Complexity(Complexity)\n",
    "    log_message(f\"Begin tests with model type {Complexity}\")\n",
    "    print({Complexity})\n",
    "    for num_augmentations in [5]:  # Augmentations\n",
    "        log_message(f\"Begin {Complexity} model with {num_augmentations} augmentations\")\n",
    "        # Delete folder if exists\n",
    "        # Move all files back\n",
    "        for file_path in isolated_dir.iterdir():  # iterates Path objects\n",
    "            if file_path.is_file():  # only move files\n",
    "                dst = data_dir / file_path.name\n",
    "                shutil.move(long_path(file_path), long_path(dst))\n",
    "                log_message(f\"  Moved {file_path.name} back to {data_dir}\")\n",
    "\n",
    "        # Path to the log file\n",
    "        #log_file = Path(\"CrystallineExecution_time.txt\")\n",
    "        #os.makedirs(long_path(log_file.parent), exist_ok=True)\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        folder_to_delete = Path(f\"CrystallineAllTestsFolder_{Complexity}_{num_augmentations}\").resolve()\n",
    "        if folder_to_delete.exists() and folder_to_delete.is_dir():\n",
    "            shutil.rmtree(long_path(folder_to_delete))\n",
    "            log_message(f\"Deleted folder: {folder_to_delete}\")\n",
    "        else:\n",
    "            log_message(f\"Folder does not exist: {folder_to_delete}\")\n",
    "        output_folder = Path(\"..\") / \"ML\" / \"Tests\" / f\"CrystallineAllTestsFolder_NOVALNOLOSS{Complexity}_{num_augmentations}\"\n",
    "        isolate_experiments(\n",
    "            long_path(data_dir),\n",
    "            long_path(isolated_dir),\n",
    "            long_path(output_folder),\n",
    "            use_uncertainty,\n",
    "            num_augmentations,\n",
    "            Correction=Correction,\n",
    "            build_model=build_model\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        with open(long_path(log_file), \"a\") as f:\n",
    "            f.write(f\"Execution time={elapsed_time:.6f} seconds for Crystal {Complexity} {num_augmentations}\\n\")\n",
    "        log_message(f\"Execution finished in {elapsed_time:.6f} seconds. Logged to {log_file}\")\n",
    "        log_message(f\"\\n _______________________________________________________ \\n \")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0610ba02-5a3e-4bbd-86e4-8dd8ccf1b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee659f-338e-4fe3-b223-26bfcdfa8ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508d65a-e2ae-4b45-8a80-6e85a5b4648d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
