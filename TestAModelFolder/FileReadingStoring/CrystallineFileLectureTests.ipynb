{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5059eead-ff18-4aaf-b391-2243cf7d355a",
   "metadata": {},
   "source": [
    "<h1>CrystalineFileLectureTests.ipynb</h1>\n",
    "\n",
    "Reads from D3Files all the files it is going to process\n",
    "\n",
    "Outputs the following folders and files:\n",
    "\n",
    "1. CrystallineLog_Reading_Creation.txt\n",
    "Logs all the prints and every step the code does. If you trust the code, it is irrelevant. If you don't trust it or want to change it then this txt file will tell you how each experiment file has been processed and where there might have been issues.\n",
    "\n",
    "\n",
    "2. Crystalline_CellID\n",
    "Contains the Cell IDs found on all the files. Needed for the ML code.\n",
    "\n",
    "\n",
    "3. AmorphousPlotResults\n",
    "This folder will store the graphs of all the experiments that were accepted. Not needed for anything but it is nice to see the files that will be fed to the model. For each experiment you can find the following files:\n",
    "\n",
    "3.1 Crystalline_Reading_Summary.txt Shows the Score of each experiment. If you use the default criteria for deciding if a file is adequate (a.k.a method FilteringMethodInt = 12) then all files under 1.4 are rejected and the files that have been forcefully accepted or rejected appear as well indicated here. IF YOU RECKON AN EXPERIMENT SHOULD BE ACCEPTED DESPITE BEING SHOWN HERE AS REJECTED PLEASE FIND THE LIST force_reject_files AND ADD YOUR ECPERIMENT USING THE SAME STRUCTURE. IDEM FOR ACCEPTED FILES THAT SHOULD NOT BE ACCEPTED. As a side note, in order to know visullay if a file is good or not, open the associated graph and just check that it doesn't do any funky business (see the examples already present to get a feel on what the word \"funky\" means)\n",
    "\n",
    "3.2 {base_name}_N_{N}.png \n",
    "It shows the values of the experiment in black, a linear fit in blue and the area needed for 75% of the points to be inside the rectangular region (the area inside y_min=mx+n-N<y<mx+n+N<y_max). Here you can find a good estimate on how good the overall decreasing tendencies are.\n",
    "\n",
    "3.3 {base_name}_ExtendedArea.png \n",
    "It shows the same pot and also the range y_min=mx+n-1.3*N<y<mx+n+1.3*N<y_max. The points outside the green area will be discarded as they are considered to be too off to be considered correct.\n",
    "\n",
    "3.4 {base_name}_Derivatives.png\n",
    "It shows the number of negative slopes between points and how steep they are\n",
    "\n",
    "\n",
    "4. CrystallineMLDataBase\n",
    "Contains the .txt files NECESSARY for the ML algorithm. There are two per experiment\n",
    "\n",
    "4.1 {base_name}.txt \n",
    "Contains DeltaTime (the time of the measurement measured from the first VALID polarization measurement), PolarizationD3, SoftPolarizationD3 (the polarization after using a Savitzky-Golay filter) and ErrPolarizationD3 (the uncertainty)\n",
    "\n",
    "4.2 {base_name}_Parameters.txt\n",
    "Contains the CellID, Pressure, LabPolarization (the polarization measured at the lab) and LabTimeCellID (the time when it was measured)\n",
    "\n",
    "\n",
    "5. CrystallineFailuresTest\n",
    "Contains the plots 3.2 and 3.4 but for the experiments that failed the overall decreasing test. Check them if you can to see if any of your experiments has been placed there by mistake\n",
    "\n",
    "\n",
    "\n",
    "6. CrystallineDataBase\n",
    "Contains all the .fli files that were attempted to be read\n",
    "\n",
    "\n",
    "7. CrystallineBadFiles\n",
    "Contains all the .fli separated in experiment sets folders that were rejected (not enough points, negative polarizations, etc.)\n",
    "\n",
    "\n",
    "_________________________________________________________________________________________\n",
    "\n",
    "Process it follows:\n",
    "\n",
    "1. REMOVAL OF PREVIOUS ITERATIONS\n",
    "To avoid leaks and duplications, all files are erased before running the code file\n",
    "\n",
    "2. ZIP FOLDER TREATMENT\n",
    "The code will take all the zip files, extract them, remove duplicates using the name AND hash sha256.\n",
    "\n",
    "3. SEPARATION OF FLI FILES ACCORDING TO EXPERIMENTS\n",
    "\n",
    "Some fli files have the wrong structure (they are not polarization measurementes) and if they are polarization files they can have more than one experiment per file.\n",
    "For evey fli file we will read the contents and try to find the header (a string in an entire line). This symbolizes the beginning of an experiment\n",
    "If there are numerical values before the first header, that means that the process of saving the file occured before changing something of the experiment. These data rows will be skipped\n",
    "A correct fli file will have the following structure:\n",
    "    polariser cell info ge18004 pressure/init. polar 2.29 0.79 initial date/time 17 09 23 @ 10:39\n",
    "    37391   4.000   0.000   1.000 18/09/23 06:20:44     155.03  +z +z     0.8391    0.0156   11.4270    1.2031     120.00\n",
    "    37392   4.000   0.000   1.000 18/09/23 06:26:49     155.05  +x +x     0.8255    0.0110   10.4610    0.7211     300.00\n",
    "    ...\n",
    "\n",
    "Which corresponds to the following information:\n",
    "    String:'polariser cell info', CellID, String:'pressure/init. polar', Pressure(unknown units), InitialLabPolarization, String:'date/time', Day, Month, Year, String:'@', Hour:Minute\n",
    "    Measurement Number, First Miller Index, Second Miller Index, Third Miller Index, Date Of Measurement, Time Of Measurement, Temperature [Kelvin],\n",
    "                        Direction Of Polarization In The First Polarizer Cell (Direction of the quantum operator S_x,S_y,S_z), Direction Of Polarization In The Second Polarizer Cell,\n",
    "                        Polarization, Polarization uncertainty, Flipping Ratio, FlippingRatio Uncertainty, Duration of the measurement\n",
    "\n",
    "The direction +z is chosen to be pointing away from the ground.\n",
    "The direction +x is the direction of the flow of neutrons, i.e, the direction of Scattering.\n",
    "The direction +y is the orthogonal to both of them.\n",
    "D3 uses two polariser cells, one between the reactor and the sample and a second between the sample and the sensor. The first one guarantees that only neutrons with the correct spin direction\n",
    "interacts with the sample. The second one guarantees that only the neutrons that have unchanged spin direction after interacting with the sample are detected by the sensor. This is\n",
    "the reason why the directions (+z,+y,+x,-z,-y,-x) appear twice.\n",
    "We have considered that temperature is not a relevant factor and the flipping ratio has no new information that polarization alrady posseses.\n",
    "First, the code will first locate the first header (ignoring eveything before) and save all the data afterwards (until the next header or end of the document) in a file with the suffix Array_{i} (i is the number of headers already processed in that fli file)\n",
    "Second, it will save the header as a file with the suffix Parameters.\n",
    "Third, the header row and the columns of Measurement Number, Temperature, Flipping Ratio, FlippingRatio Uncertainty and Time Between Measurements will be erased\n",
    "Fourth, as all data measurement uses the +z,+z combination, all other combinations are erased\n",
    "Fifth, not all data from all Miller Index combinations are polarization measurements. Even some of the ones that are polarization measurements are tampered (playing with magnetic fields for example).\n",
    "This means that there needs to be a way to select the correct combination. For starters, irrational Miller indices are not used for measurements with the samples (they need to be discarded)\n",
    "The integer Miller indices combination will be put to the test by all the functions defined before.\n",
    "Sixth, It computes a score depending on how many derivatives are negative, (200 / (200 - percent_neg) - 1) to be precise. This is a normalized score (0-1) with a 1/x evolution. Also it computes a score depending of the size of N, 2 * (-0.5 + 1 / (1 + needed_N / 8.54e-2)) to be precise. 8.54e-2 is the maximum of the data set. If a new maximum is achieved, the score wont be normalized (0-1) but won't break. The final score combines both of these values (addition). Manually I have seen that 1.4 is a good threshold. If Score>1.4 the file will be accepted. If it is smaller it will be discarded by the main code (a False will be returned). It does the m<0 test, writes everything in Summary_txt (filename, Score associated with N, Score associated with Derivatives nad the total Score). If the file was chosen to be forcefully accepted or rejected, a string will appear in the .txt file. Finally it will save plots of both filters in PlotResults if it is correct and in FailureTest if it is considered a bad file. Again, if a new file is added it may be wise to check your experiment in these folders. For more info read the description of FilteringMethodInt = 0 inside the code\n",
    "Seventh, it will try each Miller index combination for a set i value, apply a filter, and return filtered df + PrettyCombination. it will only add the filtered column if enough points & data changes significantly. If it doesn't change too much, the column PolarizationD3 will be duplicated with the new name\n",
    "Eight, it repeats the process of obtaining the area in m*x+n-N < y < m*x+n+N where 75% of the points are inside the area. It also multiplies the value of N by a factor AcceptableMultiplier and erases all points outisde this bigger area. As uncertainties are clearly underestimated I tried to make them reasonable (looking at the dispersion of the points it is clear there is systematic uncertainties. Under the hypothesis that the polarization curve should be a soft curve (at least C^1) we will try to use χ^2 to add a provisional uncertainty margin fitting to a linear expression. This is a very inaccurate uncertainty increase but it is an improvement of the underestimated uncertainties (and the lack of ways to quantify the systematic uncertainty sources). The enlarged area will be plotted and saved in PlotResults for both the normal data and the softened data (Savitzky–Golay filter)    \n",
    "    \n",
    "4. CellID SAVING\n",
    "It will safely store in a txt file all the cell ids so that the code in ML can use them\n",
    "\n",
    "5. DUPLICATION REMOVAL\n",
    "It will check if the files created for the ML algorithm are duplicates and erases them in that case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49322542-78cf-472d-9a46-e19d84cd3390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "\"\"\"\n",
    "1- LIBRARIES\n",
    "\"\"\" \n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import curve_fit\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "import hashlib\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "2- PRINTING AND LOG DETAILS. LONG PATH CORRECTIONS\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Here we have a custom function for log_messageing and logging everything on a .txt file to be able to know what has happened on the code\n",
    "\n",
    "\"\"\"\n",
    "PrintDebug = True #This Bool will determine if all logs should be log_messageed on screen on the Python Notebook. The log writing is always on. If False the code will be faster.\n",
    "ShowPlot = False #This Bool works the same but with showing on screen the plots (they are always saved even with this variable being False). Reduces program cost if False\n",
    "log_file_path = os.path.join(\".\", \"CrystallineLog_Reading_Creation.txt\")\n",
    "# Initialize log file at the start of the script\n",
    "with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(\"=== Log started ===\\n\")\n",
    "\n",
    "def log_message(message):\n",
    "    if PrintDebug:\n",
    "        print(message)\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "        log_file.write(message + \"\\n\")\n",
    "\n",
    "def win_long_path(path):\n",
    "    # Convert to Path and resolve to absolute\n",
    "    path = Path(path).resolve()\n",
    "\n",
    "    # Convert to string\n",
    "    path_str = str(path)\n",
    "\n",
    "    # Prepend \\\\?\\ if not already present\n",
    "    if not path_str.startswith(\"\\\\\\\\?\\\\\"):\n",
    "        path_str = \"\\\\\\\\?\\\\\" + path_str\n",
    "\n",
    "    return path_str\n",
    "to_erase = [\n",
    "    \"CrystallineBadFiles\", \"CrystallineDataBase\",\"CrystallineFailuresTest\",\n",
    "    \"CrystallineMLDataBase\", \"CrystallinePlotResults\", \"Crystalline_CellID.txt\",\n",
    "    \"CrystallineLogTesting_Creation.txt\",\n",
    "\n",
    "]\n",
    "for item in to_erase:\n",
    "    path = os.path.abspath(item)  # full path\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                log_message(f\"Deleted file: {path}\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                log_message(f\"Deleted folder: {path}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\" Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"Not found (skipped): {path}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3- FUNCTIONS\n",
    "\"\"\"\n",
    "#3.1-Time function for the conversion of time into seconds\n",
    "def Time(Day_Ref, Hour_Ref):\n",
    "    \"\"\"\n",
    "    Expected variables (strings) should be in the form\n",
    "     Day_Ref = 'DD/MM/YY' a.k.a Day/Month/Year\n",
    "     Hour_Ref = 'HH:MM' a.k.a Hour:Month\n",
    "    Seconds will be ignored if fed to the function (there is a check before the call of the function in the main code that erases the seconds).\n",
    "    The function takes this strings and converts them to seconds\n",
    "    Could be an improvement to consider seconds but the headers don't use seconds, there is no information to prove that the time variables are precise to the second and ~30 seconds is negligible when working with time periods of up to 70000 seconds\n",
    "    \"\"\"\n",
    "    match = re.match(r\"(\\d+)/(\\d+)/(\\d+)\", Day_Ref)\n",
    "    if match:\n",
    "        DD = int(match.group(1))\n",
    "        MM = int(match.group(2))\n",
    "        YY = int(match.group(3))\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid date format: {Day_Ref}\")\n",
    "\n",
    "    # Parse time\n",
    "    match = re.match(r\"(\\d+):(\\d+):(\\d+)\", Hour_Ref)\n",
    "    if match:\n",
    "        Hour = int(match.group(1))\n",
    "        Minute = int(match.group(2))\n",
    "        Second = int(match.group(3))\n",
    "    else:\n",
    "        # Try HH:MM\n",
    "        match = re.match(r\"(\\d+):(\\d+)\", Hour_Ref)\n",
    "        if match:\n",
    "            Hour = int(match.group(1))\n",
    "            Minute = int(match.group(2))\n",
    "            Second = 0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time format: {Hour_Ref}\")\n",
    "\n",
    "    return datetime(YY + 2000 if YY < 100 else YY, MM, DD, Hour, Minute, Second)\n",
    "\n",
    "#3.2- Time function to compute the difference in time between two sets fo time\n",
    "def deltatime(AIni,BIni, AFin,BFin):\n",
    "    \"\"\"\n",
    "    There is nothing as \"Absolute time\". We want to compare durations or intervals of time.\n",
    "    The A variables are of the type 'DD/MM/YY' and the B ones are 'HH:MM'\n",
    "     A is for the time moment considered as reference\n",
    "     B is for the time moment that has been used for measuring something\n",
    "    \"\"\"\n",
    "    time1 = Time(AIni, BIni)\n",
    "    time2 = Time(AFin, BFin)\n",
    "    return( int((time2 - time1).total_seconds()))\n",
    "\n",
    "#3.3 Conversion to integers\n",
    "def format_combination(comb):\n",
    "    \"\"\"\n",
    "    The Miller Indeces in the are writen as strings with float like numbers in them, for example (4.0,1.0,2.0)\n",
    "    As only integer Miller Indices are required we can convert them to integers with this function.\n",
    "    \"\"\"\n",
    "    if comb is None:\n",
    "        return \"(None)\"\n",
    "    ints = tuple(int(float(x)) for x in comb)\n",
    "    return f\"({','.join(map(str, ints))})\"\n",
    "\n",
    "#3.4 Folder name changes\n",
    "def sanitize(name):\n",
    "    \"\"\"\n",
    "    Remove invalid characters for folder names. [<>:\"/\\\\|?*] all turn to _\n",
    "    \"\"\"\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', name)\n",
    "\n",
    "#3.5 Savitzky–Golay filter window parameters\n",
    "def savgol_params_func(n_points):\n",
    "    \"\"\"\n",
    "    Receives tha number of points of the file that the filter will be tried to be used on\n",
    "    Window length can't be greater than the number of points. To avoid Python Errors this function gives the appropiate window length and order of the polygone\n",
    "    \"\"\"\n",
    "    window_length = min(default_window_length, n_points)\n",
    "    if window_length % 2 == 0:\n",
    "        window_length -= 1\n",
    "    if window_length < polyorder + 2:\n",
    "        window_length = polyorder + 2\n",
    "        if window_length % 2 == 0:\n",
    "            window_length += 1\n",
    "    return {'window_length': window_length, 'polyorder': polyorder}\n",
    "\n",
    "#3.6 Filtering Methods\n",
    "def Overall_Decrease (df_filtered, filename):\n",
    "    \"\"\"\n",
    "    The function that accpets files if they are decreasing sets of data (polarization evolution is a decay ALWAYS)\n",
    "    Will save plots of good files in PlotResults so that the user can see what has been accepted\n",
    "    Will save plots of bad files in FailuresTest so that the user can see the polts being discarded.\n",
    "    IMPORTANT: IF YOU ADD A NEW EXPERIMENT AND YOU ARE CONFIDENT IT IS A GOOD FILE BUT IS SENT TO FailuresTest PLEASE MANUALLY ADD THE TXT FILE TO THE force_accept_files. The program should rename eveything in SeparatedFolder so you can get the name from there  adding _Filtered if it is missing\n",
    "    \"\"\"\n",
    "    FilteringMethodInt = 12 \n",
    "    \"\"\"\n",
    "    This int chooses the method for filtering the wrong data. The options are:\n",
    "        FilteringMethodInt = 0   \n",
    "        FilteringMethodInt = 1\n",
    "        FilteringMethodInt = 2\n",
    "        FilteringMethodInt = 12\n",
    "        FilteringMethodInt = 3\n",
    "    \"\"\"\n",
    "    \n",
    "    force_accept_files = [\n",
    "        \"PolarizationD3_EuAgAs_29_8_23_0_MillerIndex_(0,0,4)_Filtered.txt\",\n",
    "        \"PolarizationD3_EuAgAs_1_30_8_23_5_MillerIndex_(3,0,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_SmI3_26_9_23_1_MillerIndex_(0,3,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_SmI3_1_28_9_23_3_MillerIndex_(0,3,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_hor_1_22_3_24_1_MillerIndex_(3,0,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_hor_15_3_24_0_MillerIndex_(0,0,2)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_ver_14_3_24_1_(1,0,0)_Filtered.txt\"]\n",
    "    force_reject_files = [\n",
    "        \"PolarizationD3_EuAgAs_1_29_8_23_0_MillerIndex_(1,1,1)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_ver_14_3_24_1_MillerIndex_(1,0,0)_Filtered.txt\"] #The values were in the 0.3 to 0.2 range. Not a polarization\n",
    "    \n",
    "    \"\"\"\n",
    "    Some files failed the FilteringMethodInt = 12 method despite being considered good files. Others passed the test but were clearly tampered.\n",
    "    These lists override the testing and accepts/rejets them immediately\n",
    "    \"\"\"\n",
    "    output_folder = Path.cwd() / \"CrystallinePlotResults\"\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    failures_folder = Path.cwd() / \"CrystallineFailuresTest\"\n",
    "    failures_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    # Ensure paths are Windows long-path safe\n",
    "    output_folder = Path(win_long_path(output_folder))\n",
    "    failures_folder = Path(win_long_path(failures_folder))\n",
    "\n",
    "    x = df_filtered['DeltaTime'].values\n",
    "    y = df_filtered['SoftPolarizationD3'].values #The tests are done with the data set filtered by the Savitzky–Golay for better results\n",
    "    if FilteringMethodInt == 0:\n",
    "        \"\"\"\n",
    "        This method computes the area in the plot that satisfies mx+n-σ_n < y < mx+n+σ_n\n",
    "        Then it finds the value of N so that mx+n-Nkσ_n < y < mx+n+k*σ_n contains 75% of the points\n",
    "        This method seemed innefective as good files with lots of points needed aburd values of k despite being the only really good files\n",
    "        The only good filtering capability is checking if the slope m is positive (if it is positive it is impossible to have a decreasing polarization evolution)\n",
    "        The summary of all files and thier scores with the test are being saved in Sigmas.txt (if you can't find it is because this method has not been used (it is useless anyways)\n",
    "        The method is saved only as a \"show of concept\"\n",
    "        \"\"\"\n",
    "        def linear_func(x, m, n):\n",
    "            return m*x + n\n",
    "        try:\n",
    "            popt, pcov = curve_fit(linear_func, x, y)\n",
    "            m, n = popt\n",
    "            sigma_n = np.sqrt(np.diag(pcov))[1]  # uncertainty in n\n",
    "    \n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error fitting data: {e}\")\n",
    "            return False\n",
    "    \n",
    "        # Step 4: check slope\n",
    "        if m > 0:\n",
    "            log_message(f\"      Overall slope is positive. Can't be polarization information. Skipping Combination\")\n",
    "            return False\n",
    "    \n",
    "        # Step 5–7: try sigma_n bands\n",
    "        num_points = len(x)\n",
    "        sorted_idx = np.argsort(x)\n",
    "        x_sorted = x[sorted_idx]\n",
    "        y_sorted = y[sorted_idx]\n",
    "    \n",
    "        needed_sigma = None\n",
    "        max_sigma = 20\n",
    "    \n",
    "        for k in range(1, max_sigma+1):\n",
    "            upper_band = linear_func(x_sorted, m, n) + k*sigma_n\n",
    "            lower_band = linear_func(x_sorted, m, n) - k*sigma_n\n",
    "    \n",
    "            inside = np.logical_and(y_sorted <= upper_band, y_sorted >= lower_band)\n",
    "            percent_inside = np.sum(inside) / num_points * 100\n",
    "    \n",
    "            if percent_inside >= 75:\n",
    "                needed_sigma = k\n",
    "                # Step 6: plot\n",
    "                plt.figure(figsize=(8,5))\n",
    "                plt.plot(x_sorted, y_sorted, 'o', label='Data')\n",
    "                plt.plot(x_sorted, linear_func(x_sorted, m, n), '-', label='Fit')\n",
    "                plt.fill_between(x_sorted, lower_band, upper_band, color='gray', alpha=0.3,\n",
    "                                 label=f'Band ±{k}·sigma_n')\n",
    "                plt.xlabel(r'$\\Delta t$ (s)')   # Delta time in seconds\n",
    "                plt.ylabel(r'$P_{\\mathrm{soft}}$')  # SoftPolarizationD3\n",
    "\n",
    "                plt.title(r'Fit: $y = %.2e \\cdot x + %.2e$, $\\sigma_n=%.2e$' % (m, n, sigma_n))\n",
    "\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                # Save plot\n",
    "                plot_filename = output_folder / f\"{filename}_AutomaticRange.png\"\n",
    "                plt.savefig(plot_filename)\n",
    "                log_message(f\"      Plot saved to {plot_filename}\")\n",
    "                plt.close()\n",
    "                break\n",
    "    \n",
    "        # Step 8: write txt file with needed sigma\n",
    "        # Common file in the same folder\n",
    "        sigmas_txt_path = output_folder / \"Sigmas.txt\"\n",
    "        \n",
    "        with open(sigmas_txt_path, 'a') as f:\n",
    "            if needed_sigma is not None:\n",
    "                f.write(f\"{filename}: needed sigma multiplier = {needed_sigma}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{filename}: needed sigma multiplier > {max_sigma}\\n\")\n",
    "        log_message(f\"      Sigma info saved to {sigmas_txt_path}\")\n",
    "    \n",
    "    \n",
    "        # Step 9: final decision\n",
    "        if needed_sigma is None or needed_sigma > max_sigma:\n",
    "            log_message(f\"      Needed > {max_sigma}·sigma_n → file considered bad\")\n",
    "            return False\n",
    "        else:\n",
    "            log_message(f\"      Approximately decreasing. Needed sigma multiplier: {needed_sigma}\")\n",
    "            return True\n",
    "\n",
    "\n",
    "    \n",
    "    elif FilteringMethodInt == 1:\n",
    "        \"\"\"\n",
    "        This method computes the area in the plot that satisfies mx+n-N < y < mx+n+N\n",
    "        Then it finds the value of N so that mx+n-N*σ_n < y < mx+n+N*σ_n contains 75% of the points\n",
    "        This method seemed much more efficient as it didn't depend of the linearity of the fit but the dispersion of the points from a first order approxiamtion\n",
    "        Still, it is not bullet-proof. Again, just a \"show of concept\"\n",
    "        Saves everything in Bands.txt and the plots have in the title the value of N.\n",
    "        Also does the m<0 test (really important)\n",
    "        \"\"\"\n",
    "        # === New method: fixed N bands ===\n",
    "        def linear_func(x, m, n):\n",
    "            return m * x + n\n",
    "\n",
    "        try:\n",
    "            popt, _ = curve_fit(linear_func, x, y)\n",
    "            m, n = popt\n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error fitting data: {e}\")\n",
    "            return False\n",
    "\n",
    "        if m > 0:\n",
    "            log_message(f\"      Overall slope is positive. Can't be polarization information. Skipping Combination\")\n",
    "            return False\n",
    "\n",
    "        num_points = len(x)\n",
    "        sorted_idx = np.argsort(x)\n",
    "        x_sorted = x[sorted_idx]\n",
    "        y_sorted = y[sorted_idx]\n",
    "\n",
    "        needed_N = None\n",
    "        N_start = 0.0001\n",
    "        N_step = 0.0001\n",
    "        N_max = 0.4\n",
    "\n",
    "        N = N_start\n",
    "        while N <= N_max:\n",
    "            upper_band = linear_func(x_sorted, m, n) + N\n",
    "            lower_band = linear_func(x_sorted, m, n) - N\n",
    "\n",
    "            inside = np.logical_and(y_sorted <= upper_band, y_sorted >= lower_band)\n",
    "            percent_inside = np.sum(inside) / num_points * 100\n",
    "\n",
    "            if percent_inside >= 75:\n",
    "                needed_N = N\n",
    "                plt.figure(figsize=(8,5))\n",
    "                plt.plot(x_sorted, y_sorted, 'o', label='Data')\n",
    "                plt.plot(x_sorted, linear_func(x_sorted, m, n), '-', label='Fit')\n",
    "                plt.fill_between(x_sorted, lower_band, upper_band, color='gray', alpha=0.3,\n",
    "                                 label=f'Band ±{N:.2e}')\n",
    "                plt.xlabel(r'$\\Delta t$ (s)')   # Delta time in seconds\n",
    "                plt.ylabel(r'$P_{\\mathrm{soft}}$')  # SoftPolarizationD3\n",
    "\n",
    "                plt.title(r'Fit: $y = %.2e \\cdot x + %.2e$, $N=%.2e$' % (m, n, N))\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plot_filename = output_folder / f\"{filename}_N_{N:.2f}_ManualInterval.png\"\n",
    "                plt.savefig(plot_filename)\n",
    "                log_message(f\"      Plot saved to {plot_filename}\")\n",
    "                plt.close()\n",
    "                break\n",
    "\n",
    "            N += N_step\n",
    "\n",
    "        bands_txt_path = output_folder / \"Bands.txt\"\n",
    "        with open(bands_txt_path, 'a') as f:\n",
    "            if needed_N is not None:\n",
    "                f.write(f\"{filename}: needed band width = ±{needed_N:.2e}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{filename}: needed band width > ±{N_max:.2e}\\n\")\n",
    "        log_message(f\"      Band info saved to {bands_txt_path}\")\n",
    "\n",
    "        if needed_N is None:\n",
    "            log_message(f\"      Needed band width > ±{N_max:.2e} → file considered bad\")\n",
    "            return False\n",
    "        else:\n",
    "            log_message(f\"      Approximately decreasing. Needed band width: ±{needed_N:.2e}\")\n",
    "            return True\n",
    "\n",
    "    \n",
    "    elif FilteringMethodInt == 2:\n",
    "        \"\"\"\n",
    "        This method computes the \"number of negative derivatives\".\n",
    "        The defintion of derivative of a point is $f'(x_0) = lim_x\\righarrow {x_0}\\frac{f(x+x_0)-f(x_0)}{x-x_0}$.\n",
    "        It is the division of the substraction of two images from two points infinitey close over their distance (metric space needed).\n",
    "        If the data set is not continious we can obtain the slope between two points using a similar idea $f'(x_j):=\\frac{y_{j+1}-y_j}{x_{j+1}-x_j}$\n",
    "        If 50% of slopes are negative we could say the overall curve is decreasing (which may not be true)\n",
    "        A 100% can't be asked as noise in measurement can make some \"derivatives\" to be positive. \n",
    "        Again, another show of concept\n",
    "        Does not do the m<0 test\n",
    "        \"\"\"\n",
    "        # === New method: compute discrete derivatives ===\n",
    "        num_points = len(x)\n",
    "        derivatives = []\n",
    "    \n",
    "        # Compute derivative for sequential pairs\n",
    "        for i in range(num_points - 1):\n",
    "            x1, y1 = x[i], y[i]\n",
    "            x2, y2 = x[i+1], y[i+1]\n",
    "            if x2 != x1:\n",
    "                derivative = (y2 - y1) / (x2 - x1)\n",
    "                derivatives.append(derivative)\n",
    "            else:\n",
    "                log_message(f\"      Skipping derivative at index {i} due to zero delta x\")\n",
    "    \n",
    "        derivatives = np.array(derivatives)\n",
    "        num_derivatives = len(derivatives)\n",
    "    \n",
    "        num_neg = np.sum(derivatives < 0)\n",
    "        num_pos = np.sum(derivatives > 0)\n",
    "    \n",
    "        percent_neg = num_neg / num_derivatives * 100 if num_derivatives > 0 else 0\n",
    "        percent_pos = num_pos / num_derivatives * 100 if num_derivatives > 0 else 0\n",
    "    \n",
    "        # Log results\n",
    "        bands_txt_path = output_folder / \"Bands.txt\"\n",
    "        with open(bands_txt_path, 'a') as f:\n",
    "            f.write(f\"{filename}: Total derivatives: {num_derivatives}\\n\")\n",
    "            f.write(f\"{filename}: Negative derivatives: {num_neg} ({percent_neg:.2f}%)\\n\")\n",
    "            f.write(f\"{filename}: Positive derivatives: {num_pos} ({percent_pos:.2f}%)\\n\")\n",
    "    \n",
    "        log_message(f\"      Derivatives computed: {num_derivatives}\")\n",
    "        log_message(f\"      Negative: {num_neg} ({percent_neg:.2e}%)\")\n",
    "        log_message(f\"      Positive: {num_pos} ({percent_pos:.2e}%)\")\n",
    "        log_message(f\"      Band info saved to {bands_txt_path}\")\n",
    "        mid_x = (x[:-1] + x[1:]) / 2\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(mid_x, derivatives, marker='o')\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.xlabel(r'$\\Delta t$ (s)')\n",
    "        plt.ylabel(r'Derivative')\n",
    "        plt.title(f'Derivatives of {filename}')\n",
    "        plot_filename = output_folder / f\"{filename}_plot_Derivatives.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "\n",
    "        # Placeholder decision: if ≥ 50% negative, return True\n",
    "        if percent_neg >= 50:\n",
    "            log_message(f\"      Majority negative derivatives → file considered decreasing\")\n",
    "            return True\n",
    "        else:\n",
    "            log_message(f\"      Majority positive derivatives → file considered NOT decreasing\")\n",
    "            return False\n",
    "            \n",
    "    elif FilteringMethodInt == 12:\n",
    "        \"\"\"\n",
    "        This method combine method 1 and 2 (that is the reason why it is called 12)\n",
    "        It computes a score depending on how many derivatives are negative, (200 / (200 - percent_neg) - 1) to be precise. This is a normalized score (0-1) with a 1/x evolution\n",
    "        It computes a score depending of the size of N, 2 * (-0.5 + 1 / (1 + needed_N / 8.54e-2)) to be precise. 8.54e-2 is the maximum of the data set. If a new maximum is achieved, the score wont be normalized (0-1) but won't break\n",
    "        The final score combines both of these values (addition). Manually I have seen that 1.4 is a good threshold. If Score>1.4 the file will be accepted. If it is smaller it will be discarded by the main code (a False will be returned)\n",
    "        Does the m<0 test\n",
    "        Write eveythin in Summary_txt (filename, Score associated with N, Score associated with Derivatives nad the total Score). If the file was chosen to be forcefully accepted or rejected, a string will appear in the .txt file.\n",
    "        Saves plots of both filters in PlotResults if it is correct and in FailureTest if it is considered a bad file. Again, if a new file is added it may be wise to check your experiment in these folders. For more info read the description of FilteringMethodInt = 0\n",
    "        \"\"\"\n",
    "        output_folder = Path.cwd() / \"CrystallinePlotResults\"\n",
    "        output_folder.mkdir(exist_ok=True)\n",
    "        failures_folder = Path.cwd() / \"CrystallineFailuresTest\"\n",
    "        failures_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "        # Ensure paths are Windows long-path safe\n",
    "        output_folder = Path(win_long_path(output_folder))\n",
    "        failures_folder = Path(win_long_path(failures_folder))\n",
    "    \n",
    "        x = df_filtered['DeltaTime'].values\n",
    "        y = df_filtered['SoftPolarizationD3'].values\n",
    "    \n",
    "        def linear_func(x, m, n):\n",
    "            return m * x + n\n",
    "    \n",
    "        try:\n",
    "            popt, _ = curve_fit(linear_func, x, y)\n",
    "            m, n = popt\n",
    "            log_message(f\"      Linear fit slope m={m:.4e}, intercept n={n:.4f}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error fitting data: {e}\")\n",
    "            return False\n",
    "        if m > 0:\n",
    "            log_message(f\"      Overall slope is positive. Can't be polarization information. Skipping Combination\")\n",
    "            return False\n",
    "\n",
    "        BandWidthScore = None\n",
    "        DerivativeScore = None\n",
    "        needed_N = None\n",
    "    \n",
    "        # N obtainment\n",
    "        num_points = len(x)\n",
    "        sorted_idx = np.argsort(x)\n",
    "        x_sorted = x[sorted_idx]\n",
    "        y_sorted = y[sorted_idx]\n",
    "    \n",
    "        N_start = 0.0001\n",
    "        N_step = 0.0001\n",
    "        N_max = 0.4\n",
    "        N = N_start\n",
    "        while N <= N_max:\n",
    "            upper_band = linear_func(x_sorted, m, n) + N\n",
    "            lower_band = linear_func(x_sorted, m, n) - N\n",
    "            inside = np.logical_and(y_sorted <= upper_band, y_sorted >= lower_band)\n",
    "            percent_inside = np.sum(inside) / num_points * 100\n",
    "    \n",
    "            if percent_inside >= 75:\n",
    "                needed_N = N\n",
    "                break\n",
    "            N += N_step\n",
    "    \n",
    "        if needed_N is not None:\n",
    "            # Band N test\n",
    "            BandWidthScore = 2 * (-0.5 + 1 / (1 + needed_N / 8.54e-2))\n",
    "            log_message(f\"      Needed N=±{needed_N:.2e}\")\n",
    "            log_message(f\"      BandWidthScore={BandWidthScore:.4f}\")\n",
    "        else:\n",
    "            log_message(f\"      No N found ≤ {N_max:.2e}\")\n",
    "    \n",
    "        # Derivative test\n",
    "        derivatives = np.diff(y_sorted) / np.diff(x_sorted)\n",
    "        num_derivatives = len(derivatives)\n",
    "        num_neg = np.sum(derivatives < 0)\n",
    "        percent_neg = (num_neg / num_derivatives) * 100 if num_derivatives > 0 else None\n",
    "    \n",
    "        if percent_neg is not None:\n",
    "            log_message(f\"      Derivatives computed: {num_derivatives}\")\n",
    "            log_message(f\"      Negative: {num_neg} ({percent_neg:.2f}%)\")\n",
    "            if percent_neg > 1:\n",
    "                DerivativeScore = (200 / (200 - percent_neg) - 1)\n",
    "            else:\n",
    "                DerivativeScore = (2 / (2 - percent_neg) - 1)\n",
    "            log_message(f\"      DerivativeScore={DerivativeScore:.4f}\")\n",
    "        else:\n",
    "            log_message(f\"      No derivatives computed\")\n",
    "    \n",
    "        # Final Score\n",
    "        if BandWidthScore is not None and DerivativeScore is not None:\n",
    "            score = BandWidthScore + DerivativeScore\n",
    "        elif BandWidthScore is not None:\n",
    "            log_message(f\"      Missing DerivativeScore in experiment {filename}\")\n",
    "            score = 2 * BandWidthScore\n",
    "        elif DerivativeScore is not None:\n",
    "            log_message(f\"      Missing BandWidthScore in experiment {filename}\")\n",
    "            score = 2 * DerivativeScore\n",
    "        else:\n",
    "            log_message(f\"      Missing BandWidthScore and DerivativeScore in experiment {filename}\")\n",
    "            score = 0\n",
    "    \n",
    "        log_message(f\"      Final score={score:.4f}\")\n",
    "    \n",
    "        # Combined Test\n",
    "        threshold = 1.4\n",
    "        is_good = score > threshold\n",
    "        was_force_accepted = False\n",
    "        was_force_rejected = False\n",
    "        if filename in force_accept_files:\n",
    "            log_message(f\"      File {filename} is in force_accept_files → passing filter even if score is low\")\n",
    "            is_good = True\n",
    "            was_force_accepted = True\n",
    "        elif filename in force_reject_files:\n",
    "            log_message(f\"      File {filename} is in force_reject_files → rejecting even if score is high\")\n",
    "            is_good = False\n",
    "            was_force_rejected = True\n",
    "        summary_path = output_folder / \"CrystallineSummary_Testing.txt\"\n",
    "        with open(win_long_path(summary_path), 'a') as summary_file:\n",
    "            summary_file.write(\n",
    "                f\"{filename}: N={'No N found' if needed_N is None else f'{needed_N:.2e}'}, \"\n",
    "                f\"BandWidthScore={'None' if BandWidthScore is None else f'{BandWidthScore:.4f}'}, \"\n",
    "                f\"Negative derivatives={'No Deriv found' if percent_neg is None else f'{percent_neg:.2f}%'}, \"\n",
    "                f\"DerivativeScore={'None' if DerivativeScore is None else f'{DerivativeScore:.4f}'}, \"\n",
    "                f\"TotalScore={score:.4f}\"\n",
    "            )\n",
    "            if was_force_accepted:\n",
    "                summary_file.write(\" [FORCE ACCEPTED]\")\n",
    "            elif was_force_rejected:\n",
    "                summary_file.write(\" [FORCE Rejected]\")\n",
    "            elif not is_good:\n",
    "                summary_file.write(\" ***\")\n",
    "            summary_file.write(\"\\n\")\n",
    "        \n",
    "        # Make and save plots in the correct folder\n",
    "        plot_folder = output_folder if is_good else failures_folder\n",
    "    \n",
    "        def make_clean_name(filename: str) -> str:\n",
    "            \"\"\"\n",
    "            Strip unnecessary prefixes/suffixes from filenames.\n",
    "            Example:\n",
    "            PolarizationD3_155K_2_18_9_23_1_MillerIndex_(4,0,1)_Filtered.txt\n",
    "              -> 155K_2_18_9_23_1_(4,0,1)\n",
    "            \"\"\"\n",
    "            base = Path(filename).stem  # remove extension\n",
    "            if base.startswith(\"PolarizationD3_\"):\n",
    "                base = base[len(\"PolarizationD3_\"):]\n",
    "            if base.endswith(\"_Filtered\"):\n",
    "                base = base[:-len(\"_Filtered\")]\n",
    "            base = base.replace(\"MillerIndex_\", \"\")\n",
    "            return base\n",
    "\n",
    "        # Build a clean name for titles / saving\n",
    "        clean_name = (\n",
    "            filename.replace(\"PolarizationD3_\", \"\")\n",
    "                    .replace(\"MillerIndex_\", \"\")\n",
    "                    .replace(\"_Filtered.txt\", \"\")\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        def clean_plot_filename(filename: str, needed_N: Optional[float], plot_folder: Path) -> Path:\n",
    "            \"\"\"\n",
    "            Build a clean filename for linear fit plots.\n",
    "            Example:\n",
    "            155K_2_18_9_23_1_(4,0,1)_N_4.30e-03.png\n",
    "            or\n",
    "            155K_2_18_9_23_1_(4,0,1)_NoNFound.png\n",
    "            \"\"\"\n",
    "            base = make_clean_name(filename)\n",
    "            suffix = f\"_N_{needed_N:.2e}\" if needed_N else \"_NoNFound\"\n",
    "            return plot_folder / f\"{base}{suffix}.png\"\n",
    "        \n",
    "        def derivative_plot_filename(filename: str, plot_folder: Path) -> Path:\n",
    "            \"\"\"\n",
    "            Build a clean filename for derivative plots.\n",
    "            Example:\n",
    "            CaFeAl_13_7_6_24_2_(0,0,2)_Derivatives.png\n",
    "            \"\"\"\n",
    "            base = make_clean_name(filename)\n",
    "            return plot_folder / f\"{base}_Derivatives.png\"\n",
    "        x = df_filtered['DeltaTime'].values\n",
    "        y = df_filtered['SoftPolarizationD3'].values\n",
    "        Err = df_filtered['ErrPolarizationD3'].values if 'ErrPolarizationD3' in df_filtered.columns else np.zeros_like(y)\n",
    "\n",
    "        \n",
    "        def make_clean_name(filename: str) -> str:\n",
    "            \"\"\"\n",
    "            Turn e.g.\n",
    "              PolarizationD3_CaFeAl_13_7_6_24_2_MillerIndex_(0,0,2)_Filtered.txt\n",
    "            into:\n",
    "              CaFeAl_13_7_6_24_2_(0,0,2)\n",
    "            and handle cases where filename contains '/' or '\\\\' (dates like DD/MM/YY).\n",
    "            \"\"\"\n",
    "            s = str(filename).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")  # prevent path splitting\n",
    "            base = Path(s).stem  # remove extension if present\n",
    "            if base.startswith(\"PolarizationD3_\"):\n",
    "                base = base[len(\"PolarizationD3_\"):]\n",
    "            if base.endswith(\"_Filtered\"):\n",
    "                base = base[:-len(\"_Filtered\")]\n",
    "            base = base.replace(\"MillerIndex_\", \"\")\n",
    "            return base\n",
    "        \n",
    "        def extended_area_plot_filename(filename: str) -> str:\n",
    "            \"\"\"EuAgAs_5_31_10_23_0_(3,0,0)_ExtendedArea.png\"\"\"\n",
    "            return f\"{make_clean_name(filename)}_ExtendedArea.png\"\n",
    "\n",
    "        # === Linear Fit Plot ===\n",
    "        plt.figure(figsize=(8,5))\n",
    "        \n",
    "        # Black points with error bars\n",
    "        plt.scatter(x_sorted, y_sorted, color='black', s=30, label='Data', marker='o')\n",
    "        if Err is not None:\n",
    "            plt.errorbar(x_sorted, y_sorted, yerr=Err, fmt='none', ecolor='black', alpha=0.6, capsize=2)\n",
    "        \n",
    "        # Optional shaded uncertainty band for measured points\n",
    "\n",
    "        \n",
    "        # Blue linear fit\n",
    "        plt.plot(x_sorted, linear_func(x_sorted, m, n), '-', color='blue', label='Fit')\n",
    "        \n",
    "        # Light blue band around the fit\n",
    "        if needed_N is not None:\n",
    "            upper_band = linear_func(x_sorted, m, n) + needed_N\n",
    "            lower_band = linear_func(x_sorted, m, n) - needed_N\n",
    "            plt.fill_between(\n",
    "                x_sorted,\n",
    "                lower_band,\n",
    "                upper_band,\n",
    "                color='lightblue',\n",
    "                alpha=0.4,\n",
    "                label=f'Band ±{needed_N:.2e}'\n",
    "            )\n",
    "        \n",
    "        plt.xlabel(r'$\\Delta t$ (s)')\n",
    "        plt.ylabel(r'$P_{\\mathrm{soft}}$')\n",
    "        plt.title(f\"Linear fit for {clean_name}\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = clean_plot_filename(filename, needed_N, plot_folder)\n",
    "        plt.savefig(win_long_path(plot_path), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # === Derivative Plot ===\n",
    "        mid_x = (x_sorted[:-1] + x_sorted[1:]) / 2\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(mid_x, derivatives, marker='o', color='black')\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.xlabel(r'$\\Delta t$ (s)')\n",
    "        plt.ylabel('Derivative')\n",
    "        plt.title(f\"Derivatives of {clean_name}\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = derivative_plot_filename(filename, plot_folder)\n",
    "        plt.savefig(win_long_path(plot_path), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "        return is_good\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif FilteringMethodInt == 3:\n",
    "        \"\"\"\n",
    "        The N bands with a sinh, cosh fit. Was useless. Saved so that no one loses their time with it\n",
    "        \"\"\"\n",
    "        def fit_func(x, a, b, c, d):\n",
    "            return a * np.sinh(b * x) + c * np.cosh(d * x)\n",
    "    \n",
    "        try:\n",
    "            popt, _ = curve_fit(fit_func, x, y, maxfev=1000000)\n",
    "            a, b, c, d = popt\n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error fitting data: {e}\")\n",
    "            return False\n",
    "            \n",
    "        num_points = len(x)\n",
    "        sorted_idx = np.argsort(x)\n",
    "        x_sorted = x[sorted_idx]\n",
    "        y_sorted = y[sorted_idx]\n",
    "    \n",
    "        needed_N = None\n",
    "        N_start = 0.0001\n",
    "        N_step = 0.0001\n",
    "        N_max = 0.4\n",
    "    \n",
    "        N = N_start\n",
    "        while N <= N_max:\n",
    "            fitted_curve = fit_func(x_sorted, a, b, c, d)\n",
    "            upper_band = fitted_curve + N\n",
    "            lower_band = fitted_curve - N\n",
    "    \n",
    "            inside = np.logical_and(y_sorted <= upper_band, y_sorted >= lower_band)\n",
    "            percent_inside = np.sum(inside) / num_points * 100\n",
    "    \n",
    "            if percent_inside >= 75:\n",
    "                needed_N = N\n",
    "                plt.figure(figsize=(8,5))\n",
    "                plt.plot(x_sorted, y_sorted, 'o', label='Data')\n",
    "                plt.plot(x_sorted, fitted_curve, '-', label='Fit')\n",
    "                plt.fill_between(x_sorted, lower_band, upper_band, color='gray', alpha=0.3,\n",
    "                                 label=f'Band ±{N:.2e}')\n",
    "                plt.xlabel(r'$\\Delta t$ (s)')\n",
    "                plt.ylabel(r'$P_{\\mathrm{soft}}$')\n",
    "    \n",
    "                plt.title(\n",
    "                    r'Fit: $a \\cdot \\sinh(bx) + c \\cdot \\cosh(dx)$' + '\\n' +\n",
    "                    r'$a=%.2e$, $b=%.2e$, $c=%.2e$, $d=%.2e$, $N=%.2e$' % (a, b, c, d, N)\n",
    "                )\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plot_filename = output_folder / f\"FIT_{filename}_N_{N:.2f}_plot.png\"\n",
    "                plt.plot()\n",
    "                plt.savefig(plot_filename)\n",
    "                log_message(f\"      Plot saved to {plot_filename}\")\n",
    "                plt.close()\n",
    "                break\n",
    "    \n",
    "            N += N_step\n",
    "    \n",
    "        bands_txt_path = output_folder / \"Bands.txt\"\n",
    "        with open(bands_txt_path, 'a') as f:\n",
    "            if needed_N is not None:\n",
    "                f.write(f\"{filename}: needed band width = ±{needed_N:.2e}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{filename}: needed band width > ±{N_max:.2e}\\n\")\n",
    "        log_message(f\"      Band info saved to {bands_txt_path}\")\n",
    "    \n",
    "        if needed_N is None:\n",
    "            log_message(f\"      Needed band width > ±{N_max:.2e} → file considered bad\")\n",
    "            return False\n",
    "        else:\n",
    "            log_message(f\"      Approximately fits inside band. Needed band width: ±{needed_N:.2e}\")\n",
    "            return True\n",
    "\n",
    "    else:\n",
    "        # === Add other methods here as needed ===\n",
    "        log_message(f\"      Unknown FilteringMethodInt = {FilteringMethodInt}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "#3.7 Function that decides the correct set of Miller Indices    \n",
    "def filter_best_combination(i,\n",
    "    df,\n",
    "    filter_func,\n",
    "    filter_column_idx,\n",
    "    new_column_name,\n",
    "    filter_params_func,\n",
    "    min_points_required=3,\n",
    "    tolerance=1e-8,\n",
    "    time_column_idx=None,\n",
    "    error_column_idx=None):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        i is the integer that separates the array files (the original .fli files have more than one experiment with different headers. Each experiment is a different i.\n",
    "        df is the pandas dataframe (the data)\n",
    "        filter_func is the type of filter to smooth the data. Only savgol_filter can be used\n",
    "        filter_column_idx is the name of the column in df that will be used for filtering ad all the other tests. Always choose 'PolarizationD3'\n",
    "        new_column name is the name that will be added to the new column. If you cange it from SoftPolarizationD3 you might need to change manually this name in the rest of the code\n",
    "        filter_params_func asks for the parameters for the filter. The function savgol_params_func was made especifically for this \n",
    "        tolerance is a measurement to know it the filter was changed anything or not\n",
    "        time_column_idx works like filter_column_idx. Please keep it as 'DeltaTime'\n",
    "        error_column_idx works like filter_column_idx. Please keep it as 'ErrPolarizationD3'\n",
    "    Outputs:\n",
    "        df_filtered is the df dataframe with the new column for SoftPolarization and only the data points of the Miller Inidices Combination that has passed all the filters\n",
    "        PrettyCombination is the Miller Indices combination that has passed the filters. Should be something like (4,1,0) \n",
    "    Try each Miller index combination for a set i value, apply a filter, and return filtered df + PrettyCombination.\n",
    "    Only adds the filtered column if enough points & data changes significantly. If it doesn't change too much, the column PolarizationD3 will be duplicated with the new name\n",
    "    \n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    folder_name = FileName.replace(\".fli\", \"\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Group by first three columns (Miller indices)\n",
    "    combination_counts = (\n",
    "        df.groupby([df.columns[0], df.columns[1], df.columns[2]])\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    log_message(f\"Analyzing combinations in file: {folder_name}_Array_{i}.fli\")\n",
    "    #Read the three numbers from the .fli file\n",
    "    for comb, count in combination_counts.items(): \n",
    "        log_message(f\"Combination {comb} occurs {count} times in file {folder_name}.fli. Trying this combination\")\n",
    "        mask = (\n",
    "            (df.iloc[:,0] == comb[0]) &\n",
    "            (df.iloc[:,1] == comb[1]) &\n",
    "            (df.iloc[:,2] == comb[2])\n",
    "        )\n",
    "        PrettyCombination = format_combination(comb)\n",
    "        \n",
    "\n",
    "        filtered_df = df.loc[mask].copy()\n",
    "        # Requisites for the Combination to be valid:\n",
    "        # Requisite 1: Have data in the data\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      {PrettyCombination} has no data\")\n",
    "            continue\n",
    "        \n",
    "        # Requisite 2: Check if data column exists\n",
    "        if filtered_df.shape[1] <= filter_column_idx:\n",
    "            log_message(f\"      Expected column index {filter_column_idx} not found. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert to numeric all columns (all columns are considered as object type)\n",
    "        filtered_df = filtered_df.apply(pd.to_numeric, errors='coerce')\n",
    "        filtered_df = filtered_df.dropna()  # drops any rows with NaNs introduced by coercion\n",
    "        \n",
    "        # Check dtypes\n",
    "        all_numeric = all(dtype.kind in ('f', 'i') for dtype in filtered_df.dtypes)\n",
    "        \n",
    "        if all_numeric:\n",
    "            log_message(f\"      All columns have been successfully converted to numbers.\")\n",
    "        else:\n",
    "            log_message(f\"      Not all columns are numbers. Current dtypes:\")\n",
    "            log_message(f\"      {filtered_df.dtypes}\")\n",
    "            log_message(f\"      Expect Error Message from Python. Perhaps removing this file might be wise unless all files have the same issue\")\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      All rows dropped after conversion to numeric. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Requisite 3: Polarization is ALWAYS positive. If any is negative, that is not a polarization. Immediately sent to the Bad Files Folder\n",
    "        if (filtered_df.iloc[:, filter_column_idx] < 0).any():\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "            badfile_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "            filtered_df.to_csv(win_long_path(badfiles_txt_path), index=False, sep='\\t')\n",
    "            log_message(f\"      {PrettyCombination} has negative polarization values. Sent to BadFiles with name {filename}. Skipping to next Combination\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        \n",
    "        # Requisite 4: Have at least three rows (otherwise we can't teach the ML algorithm anything).\n",
    "        if len(filtered_df) < min_points_required:\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "            badfile_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "            filtered_df.to_csv(win_long_path(badfiles_txt_path), index=False, sep='\\t')\n",
    "            log_message(\n",
    "                f\"      {PrettyCombination} has only {len(filtered_df)} rows (< {min_points_required}). \"\n",
    "                f\"Sent to BadFiles with name {filename}. Skipping to next Combination\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Requisite 5: Be worthy of having the filter used.\n",
    "        y = filtered_df.iloc[:, filter_column_idx].values\n",
    "        filter_params = filter_params_func(len(y))\n",
    "        try:\n",
    "            y_filtered = filter_func(y, **filter_params)\n",
    "            diff = np.abs(y - y_filtered)\n",
    "            changed_count = np.sum(diff > tolerance)\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "            if changed_count > 0:\n",
    "                log_message(f\"      Filter changed {changed_count}/{len(y)} points. Adding column '{new_column_name}'.\")\n",
    "            else:\n",
    "                log_message(f\"      Filter applied but data unchanged. Adding '{new_column_name}' as duplicated values.\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error applying filter to combination {comb}: {e}\")\n",
    "            log_message(f\"      Adding '{new_column_name}' as duplicated values to proceed anyway.\")\n",
    "            # Just duplicate the original column\n",
    "            y_filtered = y.copy()\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "        #log_message(filtered_df)\n",
    "        # Requisite 5: Add the Derivative and BandWidth filtering logic\n",
    "        if Overall_Decrease(filtered_df, f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"):\n",
    "            log_message(f\"      {PrettyCombination} has surpassed all tests. Proceding with it.\")\n",
    "            return filtered_df, PrettyCombination\n",
    "        else:\n",
    "            log_message(f\"      {PrettyCombination} failed the Overall_Decrease test. Trying next combination.\")\n",
    "            continue\n",
    "    return None, None\n",
    "\n",
    "#3.8 Function that removes the points considered bad and fixes the underestimated Uncertainty  \n",
    "def RemoveOutcast_FixUncertainty(df_filtered, PrettyCombination, filename, AcceptableMultiplier=2.0, ShowPlot=False):\n",
    "    \"\"\"\n",
    "    Repeats the process of obtaining the area in m*x+n-N < y < m*x+n+N where 75% of the points are inside the area.\n",
    "    Multiplies the value of N by a factor AcceptableMultiplier and erases all points outisde this bigger area.\n",
    "    As uncertainties are clearly underestimated I tried to make them reasonable (looking at the dispersion of the points it is clear there is systematic uncertainties\n",
    "    Under the hypothesis that the polarization curve should be a soft curve (at least C^1) we will try to use χ^2 to add a provisional uncertainty margin fitting to a linear expression.\n",
    "    This is a very inaccurate uncertainty increase but it is an improvement of the underestimated uncertainties (and the lack of ways to quantify the systematic uncertainty sources)\n",
    "    The enlarged area will be plotted and saved in PlotResults for both the normal data and the softened data (Savitzky–Golay filter)\n",
    "    \"\"\"\n",
    "    output_folder = Path.cwd() / \"CrystallinePlotResults\"\n",
    "    output_folder = Path(win_long_path(output_folder))\n",
    "\n",
    "\n",
    "    x = df_filtered['DeltaTime'].values\n",
    "    y_hard = df_filtered['PolarizationD3'].values\n",
    "    y_soft = df_filtered['SoftPolarizationD3'].values\n",
    "\n",
    "    #Linear fit\n",
    "    def linear_func(x, m, n):\n",
    "        return m * x + n\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(linear_func, x, y_hard)\n",
    "        m, n = popt\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error fitting data: {e}\")\n",
    "        return df_filtered  # Return original if error\n",
    "\n",
    "    #Find smallest N for 75% within band\n",
    "    num_points = len(x)\n",
    "    sorted_idx = np.argsort(x)\n",
    "    x_sorted = x[sorted_idx]\n",
    "    y_sorted = y_hard[sorted_idx]\n",
    "\n",
    "    N_start = 0.0001\n",
    "    N_step = 0.0001\n",
    "    N_max = 0.4\n",
    "    N = N_start\n",
    "    needed_N = None\n",
    "\n",
    "    while N <= N_max:\n",
    "        y_fit = linear_func(x_sorted, m, n)\n",
    "        upper = y_fit + N\n",
    "        lower = y_fit - N\n",
    "        inside = np.logical_and(y_sorted <= upper, y_sorted >= lower)\n",
    "        percent_inside = np.sum(inside) / num_points * 100\n",
    "\n",
    "        if percent_inside >= 75:\n",
    "            needed_N = N\n",
    "            break\n",
    "        N += N_step\n",
    "\n",
    "    if needed_N is None:\n",
    "        log_message(f\"[{filename}] No N found to contain 75% within ±{N_max}\")\n",
    "        return df_filtered  # Return original if no good N\n",
    "\n",
    "    # Compute extended band and filter out the points outside the extended band\n",
    "    y_fit_full = linear_func(x, m, n)\n",
    "    upper_band = y_fit_full + needed_N * AcceptableMultiplier\n",
    "    lower_band = y_fit_full - needed_N * AcceptableMultiplier\n",
    "\n",
    "    mask = np.logical_and(y_hard <= upper_band, y_hard >= lower_band)\n",
    "    df_cleaned = df_filtered[mask].copy()\n",
    "\n",
    "    log_message(f\"[{filename}] Filtering kept {np.sum(mask)} of {len(mask)} rows (±{needed_N * AcceptableMultiplier:.2e})\")\n",
    "\n",
    "    \n",
    "    # Rescale uncertainties using reduced chi-squared \n",
    "\n",
    "    sigma = df_filtered['ErrPolarizationD3'].values\n",
    "    \n",
    "    # Fit using curve_fit with uncertainties\n",
    "    popt, pcov = curve_fit(linear_func, x, y_hard, sigma=sigma, absolute_sigma=True)\n",
    "    \n",
    "    # Extract best-fit parameters and their uncertainties\n",
    "    m_fit, n_fit = popt\n",
    "    m_err, n_err = np.sqrt(np.diag(pcov))\n",
    "    \n",
    "    # Recalculate the reduced chi-squared \n",
    "    residuals = (y_hard - linear_func(x, *popt)) / sigma\n",
    "    dof = len(x) - len(popt)\n",
    "    chi_squared_red = np.sum(residuals**2) / dof #Maybe the reduced chi-squaed is better\n",
    "    correction_factor = np.sqrt(chi_squared_red)\n",
    "    \n",
    "    # Automatically apply correction if needed\n",
    "    if correction_factor > 1:\n",
    "        df_cleaned['ErrPolarizationD3'] *= correction_factor\n",
    "        log_message(f\"[{filename}] Applied uncertainty correction factor: √(χ²) = {correction_factor:.2f}\")\n",
    "    else:\n",
    "        log_message(f\"[{filename}] No correction applied: √(χ²) = {correction_factor:.2f}\")\n",
    "    \n",
    "    # Optional: log the fit results\n",
    "    log_message(f\"[{filename}] Fit results: m = {m_fit:.4e} ± {m_err:.4e}, n = {n_fit:.4e} ± {n_err:.4e}\")\n",
    "\n",
    "\n",
    "    def make_clean_name(filename: str) -> str:\n",
    "        \"\"\"\n",
    "        Turn e.g.\n",
    "          PolarizationD3_CaFeAl_13_7_6_24_2_MillerIndex_(0,0,2)_Filtered.txt\n",
    "        into:\n",
    "          CaFeAl_13_7_6_24_2_(0,0,2)\n",
    "        and handle cases where filename contains '/' or '\\\\' (dates like DD/MM/YY).\n",
    "        \"\"\"\n",
    "        s = str(filename).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")  # prevent path splitting\n",
    "        base = Path(s).stem  # remove extension if present\n",
    "        if base.startswith(\"PolarizationD3_\"):\n",
    "            base = base[len(\"PolarizationD3_\"):]\n",
    "        if base.endswith(\"_Filtered\"):\n",
    "            base = base[:-len(\"_Filtered\")]\n",
    "        base = base.replace(\"MillerIndex_\", \"\")\n",
    "        return base\n",
    "    \n",
    "    def extended_area_plot_filename(filename: str) -> str:\n",
    "        \"\"\"EuAgAs_5_31_10_23_0_(3,0,0)_ExtendedArea.png\"\"\"\n",
    "        return f\"{make_clean_name(filename)}_ExtendedArea.png\"\n",
    "    # Extract values\n",
    "    # Data\n",
    "    T = df_filtered['DeltaTime'].values\n",
    "    P_soft = df_filtered['SoftPolarizationD3'].values\n",
    "    P_hard = df_filtered['PolarizationD3'].values\n",
    "    Err = df_filtered['ErrPolarizationD3'].values if 'ErrPolarizationD3' in df_filtered.columns else np.zeros_like(P_soft)\n",
    "    \n",
    "    # Clean title + filename\n",
    "    clean = make_clean_name(filename)\n",
    "    save_name = extended_area_plot_filename(filename)  # ends with _ExtendedArea.png\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Black points with error bars\n",
    "    plt.scatter(T, P_hard, s=30, color=\"black\", label=\"PolarizationD3\", marker='o')\n",
    "    if Err is not None:\n",
    "        plt.errorbar(T, P_hard, yerr=Err, fmt='none', ecolor='black', alpha=0.6, capsize=2)\n",
    "    \n",
    "    # Blue linear fit\n",
    "    fit = linear_func(T, m, n)\n",
    "    plt.plot(T, fit, '-', color='blue', label=\"Linear Fit\")\n",
    "    \n",
    "    # Bands: light blue (±N) and translucent green (±N*AcceptableMultiplier)\n",
    "    if needed_N is not None:\n",
    "        upper_narrow = fit + needed_N\n",
    "        lower_narrow = fit - needed_N\n",
    "        upper_wide   = fit + needed_N * AcceptableMultiplier\n",
    "        lower_wide   = fit - needed_N * AcceptableMultiplier\n",
    "    \n",
    "        plt.fill_between(T, lower_narrow, upper_narrow, color='lightblue', alpha=0.35, label=f'Band ±{needed_N:.2e}')\n",
    "        plt.fill_between(T, lower_wide,   upper_wide,   color='green',     alpha=0.18, label=f'Filter Band ±{(needed_N*AcceptableMultiplier):.2e}')\n",
    "    \n",
    "    # Labels\n",
    "    plt.xlabel(\"DeltaTime\")\n",
    "    plt.ylabel(\"PolarizationD3\")\n",
    "    plt.title(f\"{clean}_ExtendedArea\")  # title matches saved name (sans .png)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # --- Y limits that include error bars and bands ---\n",
    "    vals_min = [np.nanmin(P_hard - Err), np.nanmin(P_hard + Err)]\n",
    "    vals_max = [np.nanmax(P_hard - Err), np.nanmax(P_hard + Err)]\n",
    "    if needed_N is not None:\n",
    "        vals_min += [np.nanmin(lower_narrow), np.nanmin(lower_wide)]\n",
    "        vals_max += [np.nanmax(upper_narrow), np.nanmax(upper_wide)]\n",
    "    # Small margins\n",
    "    ymin = np.nanmin(vals_min)\n",
    "    ymax = np.nanmax(vals_max)\n",
    "    pad = 0.02 * (ymax - ymin if np.isfinite(ymax - ymin) and (ymax - ymin) > 0 else 1.0)\n",
    "    plt.ylim(ymin - pad, ymax + pad)\n",
    "    \n",
    "    # Save\n",
    "    plot_path_hard = output_folder / save_name\n",
    "    plot_path_hard.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(win_long_path(plot_path_hard), dpi=300, bbox_inches='tight')\n",
    "    if ShowPlot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. ZIP FOLDER TREATMENT\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "To make it easier for the user to use, you can just download the \"processed\" files in the ILL data base using the zip file. To avoid duplications\n",
    "and other issues, the files won't be deleted. This reduces speed as it needs to reprocess all the files but we make sure that it works\n",
    "This code and the next will take all the zip files, extract them, remove duplicates using the name AND hash sha256.\n",
    "\"\"\"\n",
    "to_erase = [\n",
    "    \"CrystallineLog_Testing_Creation.txt\",\n",
    "    \"Crystalline_CellID.txt\",\n",
    "    \"CrystallineSeparatedFolder\",\n",
    "    \"CrystallinePlotResults\",\n",
    "    \"CrystallineMLDataBase\",\n",
    "    \"CrystallineFailuresTest\",\n",
    "    \"CrystallineDataBase\",\n",
    "    \"CrystallineBadFiles\"\n",
    "]\n",
    "\n",
    "for item in to_erase:\n",
    "    path = os.path.abspath(item)  # full path\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                log_message(f\"Deleted file: {path}\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                log_message(f\"Deleted folder: {path}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\" Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"Not found (skipped): {path}\")\n",
    "def file_hash(filepath, algo=\"sha256\", block_size=65536):\n",
    "    \"\"\"Compute hash of a file (default SHA256).\"\"\"\n",
    "    h = hashlib.new(algo)\n",
    "    with open(win_long_path(filepath), \"rb\") as f:\n",
    "        for block in iter(lambda: f.read(block_size), b\"\"):\n",
    "            h.update(block)\n",
    "    return h.hexdigest()\n",
    "\n",
    "folder = Path(\"D3Files\")  # Folder where all the raw data folders reside\n",
    "zip_files = [f.name for f in folder.glob(\"*.zip\")]  # List all zip files\n",
    "\n",
    "log_message(f\"Reading ZIP files. Checking for true duplicates by content...\")\n",
    "base_names = set()\n",
    "seen_hashes = {}\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    zip_path = folder / zip_file\n",
    "    name, ext = os.path.splitext(zip_file)\n",
    "\n",
    "    # Compute hash of this file\n",
    "    filehash = file_hash(zip_path)\n",
    "\n",
    "    if filehash in seen_hashes:\n",
    "        log_message(f\"Duplicate confirmed by hash! Removing: {zip_file} (same as {seen_hashes[filehash]})\")\n",
    "        os.remove(win_long_path(zip_path))   # Safe long path\n",
    "    else:\n",
    "        seen_hashes[filehash] = zip_file\n",
    "        base_names.add(name)\n",
    "\n",
    "log_message(f\"\\n All duplicates (by content) removed. Begin unzipping...\\n\")\n",
    "\n",
    "# Refresh zip_files list after removals\n",
    "zip_files = [f.name for f in folder.glob(\"*.zip\")]\n",
    "\n",
    "# Unzip and remove original zip files\n",
    "for zip_file in zip_files:\n",
    "    zip_path = folder / zip_file\n",
    "    if zipfile.is_zipfile(win_long_path(zip_path)):\n",
    "        folder_name = sanitize(zip_file.stem if isinstance(zip_file, Path) else os.path.splitext(zip_file)[0])\n",
    "        extract_dir = folder / folder_name\n",
    "        log_message(f\"Unzipping: {zip_file} -> {extract_dir}\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(win_long_path(zip_path), 'r') as zip_ref:\n",
    "                zip_ref.extractall(win_long_path(extract_dir))\n",
    "        except Exception as e:\n",
    "            log_message(f\"WARNING: Error extracting {zip_file}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"WARNING: Skipping invalid zip file: {zip_file}\")\n",
    "\n",
    "log_message(f\"\\n Finished Unzipping. Experiments stored in individual folders substituting the zip files\\n\")\n",
    "\n",
    "# --- Extraction of .fli files ---\n",
    "source_folder = folder\n",
    "database_folder = Path(\"CrystallineDataBase\")\n",
    "database_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_message(f\"\\n\\n\\n Scanning all folders for .fli files...\\n \")\n",
    "for item in source_folder.iterdir():\n",
    "    if item.is_dir():\n",
    "        log_message(f\"Processing folder: {item.name}\")\n",
    "        for root, dirs, files in os.walk(win_long_path(item)):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(\".fli\"):\n",
    "                    src_file = Path(root) / file\n",
    "                    dest_file = database_folder / file\n",
    "\n",
    "                    # Handle duplicate names\n",
    "                    counter = 1\n",
    "                    base_name, ext = os.path.splitext(file)\n",
    "                    while dest_file.exists():\n",
    "                        dest_file = database_folder / f\"{base_name}_{counter}{ext}\"\n",
    "                        counter += 1\n",
    "\n",
    "                    log_message(f\"Copying: {src_file} -> {dest_file}\")\n",
    "                    shutil.copy2(win_long_path(src_file), win_long_path(dest_file))\n",
    "        \n",
    "        # After processing all .fli files, delete the original folder\n",
    "        log_message(f\"Deleting folder: {item}\")\n",
    "        shutil.rmtree(win_long_path(item))\n",
    "\n",
    "log_message(f\"\\n All .fli files collected, sent from folder {source_folder} to folder {database_folder}.\\n\")\n",
    "\n",
    "\"\"\"\n",
    "6 SEPARATION OF FLI FILES ACCORDING TO EXPERIMENTS\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Some fli files have the wrong structure (they are not polarization measurementes) and if they are polarization files they can have more than one experiment per file.\n",
    "For evey fli file we will read the contents and try to find the header (a string in an entire line). This symbolizes the beginning of an experiment\n",
    "If there are numerical values before the first header, that means that the process of saving the file occured before changing something of the experiment. These data rows will be skipped\n",
    "A correct fli file will have the following structure:\n",
    "    polariser cell info ge18004 pressure/init. polar 2.29 0.79 initial date/time 17 09 23 @ 10:39\n",
    "    37391   4.000   0.000   1.000 18/09/23 06:20:44     155.03  +z +z     0.8391    0.0156   11.4270    1.2031     120.00\n",
    "    37392   4.000   0.000   1.000 18/09/23 06:26:49     155.05  +x +x     0.8255    0.0110   10.4610    0.7211     300.00\n",
    "    ...\n",
    "\n",
    "Which corresponds to the following information:\n",
    "    String:'polariser cell info', CellID, String:'pressure/init. polar', Pressure(unknown units), InitialLabPolarization, String:'date/time', Day, Month, Year, String:'@', Hour:Minute\n",
    "    Measurement Number, First Miller Index, Second Miller Index, Third Miller Index, Date Of Measurement, Time Of Measurement, Temperature [Kelvin],\n",
    "                        Direction Of Polarization In The First Polarizer Cell (Direction of the quantum operator S_x,S_y,S_z), Direction Of Polarization In The Second Polarizer Cell,\n",
    "                        Polarization, Polarization uncertainty, Flipping Ratio, FlippingRatio Uncertainty, Duration of the measurement\n",
    "\n",
    "The direction +z is chosen to be pointing away from the ground.\n",
    "The direction +x is the direction of the flow of neutrons, i.e, the direction of Scattering.\n",
    "The direction +y is the orthogonal to both of them.\n",
    "D3 uses two polariser cells, one between the reactor and the sample and a second between the sample and the sensor. The first one guarantees that only neutrons with the correct spin direction\n",
    "interacts with the sample. The second one guarantees that only the neutrons that have unchanged spin direction after interacting with the sample are detected by the sensor. This is\n",
    "the reason why the directions (+z,+y,+x,-z,-y,-x) appear twice.\n",
    "We have considered that temperature is not a relevant factor and the flipping ratio has no new information that polarization alrady posseses.\n",
    "First, the code will first locate the first header (ignoring eveything before) and save all the data afterwards (until the next header or end of the document) in a file with the suffix Array_{i} (i is the number of headers already processed in that fli file)\n",
    "Second, it will save the header as a file with the suffix Parameters.\n",
    "Third, the header row and the columns of Measurement Number, Temperature, Flipping Ratio, FlippingRatio Uncertainty and Time Between Measurements will be erased\n",
    "Fourth, as all data measurement uses the +z,+z combination, all other combinations are erased\n",
    "Fifth, not all data from all Miller Index combinations are polarization measurements. Even some of the ones that are polarization measurements are tampered (playing with magnetic fields for example).\n",
    "This means that there needs to be a way to select the correct combination. For starters, irrational Miller indices are not used for measurements with the samples (they need to be discarded)\n",
    "The integer Miller indices combination will be put to the test by all the functions defined before.\n",
    "For evey succesful experiment we will output:\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Multiplier={Multiplier}.png\" in PlotResults. Shows the plot with the extended area with the raw data\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Multiplier={Multiplier}_Soft.png\" in PlotResults. Shows the plot with the extended area with the filtered data\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt_plot_Derivatives.png\" in PlotResults. Shows the evolution of the \"derivatives\"\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt_N_{N}_ManualInterval.png\" in PlotResults. Shows the plot with the non-exteded area\n",
    "    Txt:    \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}.txt\" in MLDataBase. It contains the four data columns (DeltaTime, PolarizationD3, SoftPolarizationD3, ErrPolarizationD3)\n",
    "    Txt:    \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\" in MLDataBase. It contains the parameters (CellID, Pressure, LabPolarization, LabTime)\n",
    "These plots are not necessary but are saved for the user to know what all the files look like.\n",
    "The files that are wrong or useless when all is done are the folowing:\n",
    "    Txt:    \"{folder_name}_Arrays_{i}.txt\" in SeparatedFolder/{folder_name}. It still has the header and useless columns. It is the fli file of evey chunk, of every recorded experiment (correct or incorrect)\n",
    "    Txt:    \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}.txt\" in SeparatedFolder/{folder_name}. It is the same as the one in MLDataBase (a duplicate)\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}.png\" in SeparatedFolder/{folder_name}. It plots (with error bars) PolarizationD3\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Combined.png\" in SeparatedFolder/{folder_name}. It plots (with error bars) both PolarizationD3 and SoftPolarizationD3\n",
    "    Image:  \"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Softened.png\" in SeparatedFolder/{folder_name}. It plots (with error bars) SoftPolarizationD3\n",
    "    Folder: \"FailuresTest\" contains all the graphs of the data sets that were considered not worthy but had more points that the ones saved. Check them if your experiment was not properly added\n",
    "    Folder: \"DataBase\" has the raw fli files. Once the code has been used they are no longer important (if you don't find the folder I may have added a line of code to erase it. Sorry in advance for any inconveniences)  \n",
    "\"\"\"\n",
    "# Path to the original folder and the final folder\n",
    "DataBase = Path('CrystallineDataBase')\n",
    "output_base = Path('CrystallineSeparatedFolder')\n",
    "\n",
    "# List all .fli files in that folder, prepare folders\n",
    "FileNameList = [f.name for f in DataBase.glob('*.fli')]\n",
    "polyorder = 2\n",
    "default_window_length = 5\n",
    "SeparatedFolder = Path(\"CrystallineSeparatedFolder\")\n",
    "BadFilesFolder = Path(\"CrystallineBadFiles\")\n",
    "MLDataBaseFolder = Path(\"CrystallineMLDataBase\")\n",
    "BadFilesFolder.mkdir(exist_ok=True, parents=True)\n",
    "MLDataBaseFolder.mkdir(exist_ok=True, parents=True)\n",
    "log_message(f\"\\n\\n Files in the data base that will be (tried) to be used\\n {FileNameList}\\n\")\n",
    "\n",
    "for FileName in FileNameList:\n",
    "    \"\"\"READ THE FILE AND SEPRATE IT INTO EACH EXPERIMENT USING THE POLARIZATION CELL\"\"\"\n",
    "    # 1- Open file\n",
    "    folder_name = FileName.replace(\".fli\", \"\")\n",
    "    output_folder = output_base / folder_name\n",
    "    file_path = DataBase / FileName\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(win_long_path(file_path), \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    \n",
    "    # 2- Locate the header with CellID, Pressure, etc. Chunks are the data rows sandwiched between two 'polariser cell info' strings\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    started = False  # flag to know when we found first header\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"polariser cell info\"):  # All before polariser cell info will be forgotten\n",
    "            if started and current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = [line]\n",
    "            started = True\n",
    "        else:\n",
    "            if started:\n",
    "                current_chunk.append(line)\n",
    "            # else: we are before the first header, so ignore these lines\n",
    "\n",
    "    if not started:\n",
    "        log_message(f\" File '{FileName}' does NOT contain any 'polariser cell info' header. Skipping.\\n\")\n",
    "        continue\n",
    "    else:\n",
    "        log_message(f\" File '{FileName}' contains at least one 'polariser cell info' header.\")\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    # 3- Save .fli files for every correct chunk\n",
    "    base_name = FileName.replace(\".fli\", \"\")  # remove .fli for clean filenames\n",
    "    log_message(f\"Creating all the Array files \\n\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        fli_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "        fli_path = output_folder / fli_filename  # full path\n",
    "        with open(win_long_path(fli_path), \"w\", encoding='utf-8') as f_out:\n",
    "            f_out.writelines(chunk)\n",
    "    # 4- As CellID can be exchanged with real parameters, it is written in an independent file\n",
    "    cell_id_file = Path.cwd() / \"Crystalline_CellID.txt\"\n",
    "    try:\n",
    "        with open(win_long_path(cell_id_file), 'r', encoding='utf-8') as file:\n",
    "            seen_strings = set(line.strip() for line in file)\n",
    "    except FileNotFoundError:\n",
    "        seen_strings = set()\n",
    "    \n",
    "    # 5- Open each Array file and work with it (The Array file still has the header)\n",
    "    with open(win_long_path(cell_id_file), 'a', encoding='utf-8') as file:\n",
    "        for i in range(len(chunks)):\n",
    "            FLI_filename = f\"{base_name}_Arrays_{i}.fli\"  # Name of the Array file\n",
    "            FLI_path = output_folder / FLI_filename  # Full path\n",
    "            if not FLI_path.exists():\n",
    "                log_message(f\"WARNING: Array file does not exist: {FLI_path}\")\n",
    "                continue\n",
    "            df = pd.read_csv(win_long_path(FLI_path), sep=r'\\s+', header=None, on_bad_lines='skip')  # Read file\n",
    "            log_message(f\"Reading {FLI_path}, removing ***WARNING No centering scan found \")\n",
    "            warning_str = \"***WARNING No centering scan found\"\n",
    "\n",
    "            #5.1 Combine first 4 columns as strings, join them with space, and filter rows containing this phrase (it is not important for us)\n",
    "            df = df[~df.iloc[:, :5].astype(str).agg(' '.join, axis=1).str.contains('No centering scan found', regex=False)] \n",
    "            \n",
    "            #5.2 Extract useful information from the header. Hopefully, CellID, Pressure, LabPolarization, Year, Month, Day, time of lab measurement before first experiment measurement (negative time) will be stored locally\n",
    "            log_message(f\"Header Information Extraction...\")\n",
    "            CellID =          df.iloc[0].tolist()[3]\n",
    "            Pressure =        df.iloc[0].tolist()[6]\n",
    "            LabPolarization = df.iloc[0].tolist()[7]\n",
    "\n",
    "            try:\n",
    "                HM, DD, MM, YY = df.iloc[0].tolist()[14], int(df.iloc[0].tolist()[10]), int(df.iloc[0].tolist()[11]), int(df.iloc[0].tolist()[12])\n",
    "                Day_Ref = f\"{DD:02d}/{MM:02d}/{YY:02d}\"\n",
    "                dt = Time(Day_Ref, HM)\n",
    "            except Exception as e:\n",
    "                log_message(f\"Skipping file {file_path} because of invalid header data: {e}\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            #5.3 All redundant/useless information is removed\n",
    "            log_message(f\"Removing Measurement Index, Temperature, Flipping Ratio, Uncertainty of Flipping Ratio and Time between measurements,...\")\n",
    "            df = df.iloc[1:].reset_index(drop=True)\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "            df = df.drop(df.columns[5], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            log_message(f\"Saving only polarization values for the Spin Directions wanted in both Polarizer Cells, i.e. (+z,+z)\")\n",
    "\n",
    "            #5.4 Keep only rows where both are +z\n",
    "            df = df[(df[7] == '+z') & (df[8] == '+z')].copy()\n",
    "            if df.empty:\n",
    "                log_message(f\"No valid '+z' rows in file {FileName}_Arrays_{i}.fli, skipping\")\n",
    "                continue  # skip to next file in your loop\n",
    "            df = df.drop(df.columns[[5,6]], axis=1, errors='ignore')\n",
    "            #5.5 Convert Miller index columns into integers. From string or object to float and if the float is close to an integer (tolerance is 1e-8) then save as integer. Otherwise remove row\n",
    "            cols_to_convert = [1, 2, 3]\n",
    "            df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce').astype(float)            \n",
    "            mask = np.isclose(df[cols_to_convert], np.round(df[cols_to_convert]), atol=1e-8)\n",
    "            df = df[mask.all(axis=1)].copy()\n",
    "            log_message(f\"All Spin directions removed. All irrational Miller Indices removed. Adding DeltaTime\")\n",
    "            \n",
    "            #5.5 The time columns are converted into difference of time being the referenced time the first +z,+z measurement that has survived at this point\n",
    "            if df.shape[0] < 2:\n",
    "                log_message(f\"Not enough valid rows after filtering, skipping chunk\")\n",
    "                continue\n",
    "            df['DeltaTime'] = df.apply(\n",
    "                lambda row: deltatime(df[4].iloc[0], df[5].iloc[0], row[4], row[5]), axis=1 )\n",
    "            ref_dt = Time(df[4].iloc[0], df[5].iloc[0])\n",
    "            LabTime = int((dt - ref_dt).total_seconds())\n",
    "\n",
    "            #5.6 Rename the columns PolarizationD3, ErrPolarizationD3 (the polarization column and its uncertainty). The other one with name is DeltaTime. The rest are numbers (will be erased).\n",
    "            #Also we remove the time strings (with DeltaTime they have no new information)\n",
    "            log_message(f\"Renaming PolarizationD3 and ErrPolarizationD3\")\n",
    "            df.rename(columns={\n",
    "                df.columns[5]: 'PolarizationD3',\n",
    "                df.columns[6]: 'ErrPolarizationD3'\n",
    "            }, inplace=True)\n",
    "            df.drop(columns=[df.columns[3], df.columns[4]], inplace=True)\n",
    "            log_message(f\"Dropped Time Strings\")\n",
    "\n",
    "            \n",
    "            #5.7 Begin filtering and softening with previous functions\n",
    "            log_message(f\"Begin removal of Bad files and softening with Savitzky-Golay filter\")\n",
    "            filtered_df, PrettyCombination = filter_best_combination(i,\n",
    "                df,\n",
    "                filter_func=savgol_filter,\n",
    "                filter_column_idx=df.columns.get_loc('PolarizationD3'),\n",
    "                new_column_name='SoftPolarizationD3',\n",
    "                filter_params_func=savgol_params_func,\n",
    "                min_points_required=3,\n",
    "                tolerance=1e-8,\n",
    "                time_column_idx=df.columns.get_loc('DeltaTime'),\n",
    "                error_column_idx=df.columns.get_loc('ErrPolarizationD3')\n",
    "                )\n",
    "            #If nothing survived the filters/purge then use'continue' and go for the next experiment\n",
    "            if filtered_df is None and PrettyCombination is None:\n",
    "                log_message(f\"Chunk {i}: No suitable combination found. Skipping to next chunk or file.\")\n",
    "                log_message(f\"_______________________________________________________________\\n\")\n",
    "                continue  # skip to next chunk\n",
    "            \n",
    "            #5.8 Removal of Miller indices (we have all the information they could give us)\n",
    "            log_message(f\"Removing Miller Indices columns\")\n",
    "            #log_message(filtered_df)\n",
    "            filtered_df = filtered_df.iloc[:, 3:]\n",
    "            desired_order = [\"DeltaTime\", \"PolarizationD3\", \"SoftPolarizationD3\", \"ErrPolarizationD3\"]\n",
    "\n",
    "            \n",
    "            # 5.9 Remove the points that won't be useful for the ML algorithm\n",
    "            columns_to_save = [col for col in desired_order if col in filtered_df.columns]  # Keep only the columns that exist\n",
    "            df_SEMIFINAL = filtered_df[columns_to_save].copy()\n",
    "            df_FINAL = filtered_df = RemoveOutcast_FixUncertainty(\n",
    "                df_SEMIFINAL,\n",
    "                PrettyCombination,\n",
    "                filename=f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}\",\n",
    "                AcceptableMultiplier=1.3,\n",
    "                ShowPlot=False\n",
    "            )\n",
    "            \n",
    "            # 5.10 Plot the successful experiments\n",
    "            log_message(f\"Plot of Data. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            T = pd.to_numeric(df_FINAL[\"DeltaTime\"], errors='coerce')\n",
    "            P = pd.to_numeric(df_FINAL[\"PolarizationD3\"], errors='coerce')\n",
    "            Err = pd.to_numeric(df_FINAL[\"ErrPolarizationD3\"], errors='coerce')\n",
    "            P_soft = pd.to_numeric(df_FINAL[\"SoftPolarizationD3\"], errors='coerce')\n",
    "            \n",
    "            # Scatter plot\n",
    "            plt.scatter(T, P, linewidth=1, label='Original') \n",
    "            plt.plot(T, P, linestyle='--', color='blue', alpha=0.7)\n",
    "            plt.errorbar(T, P, yerr=Err, fmt='none', ecolor='gray', alpha=0.5)\n",
    "            \n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"PolarizationD3\")\n",
    "            plot_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}.png\"\n",
    "            plt.title(plot_filename)\n",
    "            plt.ylim(np.min(P - Err), np.max(P + Err))\n",
    "            plt.yticks(np.linspace(np.min(P - Err), np.max(P + Err), 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path = win_long_path(output_folder / plot_filename)\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            if ShowPlot:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            # Softened data plot\n",
    "            log_message(f\"Plot of Filtered Data. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Softened\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.scatter(T, P_soft, linewidth=1, label='Filtered')\n",
    "            plt.plot(T, P_soft, linestyle='--', color='green', alpha=0.7)\n",
    "            plt.errorbar(T, P_soft, yerr=Err, fmt='none', ecolor='gray', alpha=0.5)\n",
    "            \n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"SoftPolarizationD3\")\n",
    "            plot_filename_soft = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Softened.png\"\n",
    "            plt.title(plot_filename_soft)\n",
    "            plt.ylim(np.min(P_soft - Err), np.max(P_soft + Err))\n",
    "            plt.yticks(np.linspace(np.min(P_soft - Err), np.max(P_soft + Err), 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path_soft = win_long_path(output_folder / plot_filename_soft)\n",
    "            plt.savefig(plot_path_soft, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            if ShowPlot:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            # Comparison plot\n",
    "            log_message(f\"Comparison Plot. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Comparison\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.scatter(T, P, linewidth=1, color='blue', alpha=0.6, label='Original')\n",
    "            plt.plot(T, P, linestyle='--', color='blue', alpha=0.5)\n",
    "            plt.scatter(T, P_soft, linewidth=1, color='green', alpha=0.6, label='Filtered')\n",
    "            plt.plot(T, P_soft, linestyle='--', color='green', alpha=0.5)\n",
    "            plt.errorbar(T, P, yerr=Err, fmt='none', ecolor='gray', alpha=0.3)\n",
    "            plt.errorbar(T, P_soft, yerr=Err, fmt='none', ecolor='gray', alpha=0.3)\n",
    "            \n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"Polarization\")\n",
    "            plot_filename_combined = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Combined.png\"\n",
    "            plt.title(plot_filename_combined)\n",
    "            min_y = min(np.min(P - Err), np.min(P_soft - Err))\n",
    "            max_y = max(np.max(P + Err), np.max(P_soft + Err))\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(min_y, max_y)\n",
    "            plt.yticks(np.linspace(min_y, max_y, 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path_combined = win_long_path(output_folder / plot_filename_combined)\n",
    "            plt.savefig(plot_path_combined, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            if ShowPlot:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            # 5.11 Save the files\n",
    "            log_message(f\"Finally we save the chunk\")\n",
    "            csv_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}.txt\"\n",
    "            Parameter_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\"\n",
    "            csv_path = output_folder / csv_filename\n",
    "            \n",
    "            # Save CSV\n",
    "            df_FINAL['DeltaTime'] = df_FINAL['DeltaTime'] - df_FINAL['DeltaTime'].iloc[0]\n",
    "\n",
    "            df_FINAL.to_csv(win_long_path(csv_path), index=False, sep=',')\n",
    "            \n",
    "            # Save a copy to ML database\n",
    "            ml_txt_path = MLDataBaseFolder / csv_filename\n",
    "            df_FINAL.to_csv(win_long_path(ml_txt_path), index=False, sep=',')\n",
    "            log_message(f\"Saved: {csv_filename}\")\n",
    "            \n",
    "            # Parameter file\n",
    "            ml_param_path = MLDataBaseFolder / Parameter_filename\n",
    "            with open(win_long_path(ml_param_path), 'w', encoding='utf-8') as f:\n",
    "                f.write(\"CellID,Pressure,LabPolarization,LabTime\\n\")\n",
    "                f.write(f\"{CellID},{Pressure},{LabPolarization},{LabTime}\")\n",
    "            log_message(f\"Saved: {Parameter_filename}\")\n",
    "            log_message(f\"Parameter and Array files saved to ML database: {MLDataBaseFolder}\\n_______________________________________________________________\\n\\n\")\n",
    "            \n",
    "            # 5.12 Remove unwanted array files\n",
    "            for i in range(len(chunks)):\n",
    "                temp_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "                temp_path = output_folder / temp_filename\n",
    "                try:\n",
    "                    temp_path.unlink()  # delete the file\n",
    "                except FileNotFoundError:\n",
    "                    pass  # skip if missing\n",
    "            \n",
    "            log_message(f\"Created and saved {len(chunks)} CSV files from file called {FileName}.\")\n",
    "            \n",
    "            # Remove empty folder\n",
    "            if output_folder.exists() and not any(output_folder.iterdir()):\n",
    "                output_folder.rmdir()\n",
    "                log_message(f\"Removed empty folder: {output_folder}\")\n",
    "            \n",
    "            log_message('\\n\\n')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "7. CELLID FILE PROCESSING\n",
    "\"\"\"\n",
    "ml_database_folder = Path(\"CrystallineMLDataBase\")\n",
    "\n",
    "# Find all txt files whose names end with Parameters.txt (case insensitive)\n",
    "parameter_files = list(ml_database_folder.glob('*Parameters.txt'))\n",
    "\n",
    "log_message(f\"Found {len(parameter_files)} parameter files.\")\n",
    "\n",
    "unique_cell_ids = []\n",
    "seen = set()\n",
    "\n",
    "for filepath in parameter_files:\n",
    "    try:\n",
    "        with open(win_long_path(filepath), 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 2:\n",
    "                second_row = lines[1].strip()\n",
    "                parts = second_row.split(',')\n",
    "                if parts:\n",
    "                    cell_id = parts[0]\n",
    "                    if cell_id not in seen:\n",
    "                        seen.add(cell_id)\n",
    "                        unique_cell_ids.append(cell_id)\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to read {filepath}: {e}\")\n",
    "\n",
    "# Write to Crystalline_CellID.txt\n",
    "cellid_file = Path.cwd() / \"Crystalline_CellID.txt\"\n",
    "with open(win_long_path(cellid_file), \"w\", encoding='utf-8') as f:\n",
    "    for cell_id in unique_cell_ids:\n",
    "        f.write(f\"{cell_id}\\n\")\n",
    "\n",
    "log_message(f\"Saved {len(unique_cell_ids)} unique cell IDs to {cellid_file.name}.\")\n",
    "\n",
    "# Remove the separated folder\n",
    "folder_to_delete = Path.cwd() / \"CrystallineSeparatedFolder\"\n",
    "if folder_to_delete.exists():\n",
    "    shutil.rmtree(win_long_path(folder_to_delete))\n",
    "    log_message(f\"Folder '{folder_to_delete}' has been deleted.\")\n",
    "else:\n",
    "    log_message(f\"Folder '{folder_to_delete}' does not exist.\")\n",
    "\n",
    "\"\"\"\n",
    "Removal of Duplicates\n",
    "\"\"\"\n",
    "hash_map = defaultdict(list)\n",
    "\n",
    "def file_sha256(filepath, block_size=65536):\n",
    "    \"\"\"Compute SHA256 hash of a file (safe for large files).\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(win_long_path(filepath), \"rb\") as f:\n",
    "        while chunk := f.read(block_size):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "# Scan all .txt files (only base files without '_Parameters')\n",
    "for root, _, files in os.walk(win_long_path(ml_database_folder)):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".txt\") and \"_parameters\" not in file.lower():\n",
    "            path = Path(root) / file\n",
    "            file_hash = file_sha256(path)\n",
    "            hash_map[file_hash].append(path)\n",
    "\n",
    "# Report & delete duplicates\n",
    "duplicates_found = False\n",
    "for file_hash, paths in hash_map.items():\n",
    "    if len(paths) > 1:\n",
    "        duplicates_found = True\n",
    "        log_message(f\"\\nDuplicate group (hash={file_hash}):\")\n",
    "        log_message(f\"   Keeping: {paths[0]}\")\n",
    "\n",
    "        # All but the first are duplicates\n",
    "        for p in paths[1:]:\n",
    "            base_name, ext = os.path.splitext(p)\n",
    "            param_file = Path(f\"{base_name}_Parameters{ext}\")\n",
    "\n",
    "            try:\n",
    "                os.remove(win_long_path(p))\n",
    "                log_message(f\"   Deleted duplicate base file: {p}\")\n",
    "            except Exception as e:\n",
    "                log_message(f\"   Could not delete base file {p}: {e}\")\n",
    "\n",
    "            # Also try deleting the corresponding parameter file\n",
    "            if param_file.exists():\n",
    "                try:\n",
    "                    os.remove(win_long_path(param_file))\n",
    "                    log_message(f\"   Deleted parameter file: {param_file}\")\n",
    "                except Exception as e:\n",
    "                    log_message(f\"   Could not delete parameter file {param_file}: {e}\")\n",
    "\n",
    "if not duplicates_found:\n",
    "    log_message(\"No duplicates found in MLDataBase!\")\n",
    "else:\n",
    "    log_message(\"\\n Duplicate cleanup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569183da-1101-4d3b-9610-b06650bfc02e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e9695-0670-410d-96ef-9a6b7160aae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eda6d3-c5d6-473c-adf1-e99288e1f6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436bb6d-6996-4913-9a20-7d66841902cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
