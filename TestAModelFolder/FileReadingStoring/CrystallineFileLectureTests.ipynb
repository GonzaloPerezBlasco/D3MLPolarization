{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5059eead-ff18-4aaf-b391-2243cf7d355a",
   "metadata": {},
   "source": [
    "<h1>CrystalineFileLectureTests.ipynb</h1>\n",
    "\n",
    "Reads from D3Files all the files it is going to process\n",
    "\n",
    "Outputs the following folders and files:\n",
    "\n",
    "1. CrystallineLog_Reading_Creation.txt\n",
    "Logs all the prints and every step the code does. If you trust the code, it is irrelevant. If you don't trust it or want to change it then this txt file will tell you how each experiment file has been processed and where there might have been issues.\n",
    "\n",
    "\n",
    "2. Crystalline_CellID\n",
    "Contains the Cell IDs found on all the files. Needed for the ML code.\n",
    "\n",
    "\n",
    "3. AmorphousPlotResults\n",
    "This folder will store the graphs of all the experiments that were accepted. Not needed for anything but it is nice to see the files that will be fed to the model. For each experiment you can find the following files:\n",
    "\n",
    "3.1 Crystalline_Reading_Summary.txt Shows the Score of each experiment. If you use the default criteria for deciding if a file is adequate (a.k.a method FilteringMethodInt = 12) then all files under 1.4 are rejected and the files that have been forcefully accepted or rejected appear as well indicated here. IF YOU RECKON AN EXPERIMENT SHOULD BE ACCEPTED DESPITE BEING SHOWN HERE AS REJECTED PLEASE FIND THE LIST force_reject_files AND ADD YOUR ECPERIMENT USING THE SAME STRUCTURE. IDEM FOR ACCEPTED FILES THAT SHOULD NOT BE ACCEPTED. As a side note, in order to know visullay if a file is good or not, open the associated graph and just check that it doesn't do any funky business (see the examples already present to get a feel on what the word \"funky\" means)\n",
    "\n",
    "3.2 {base_name}_N_{N}.png \n",
    "It shows the values of the experiment in black, a linear fit in blue and the area needed for 75% of the points to be inside the rectangular region (the area inside y_min=mx+n-N<y<mx+n+N<y_max). Here you can find a good estimate on how good the overall decreasing tendencies are.\n",
    "\n",
    "3.3 {base_name}_ExtendedArea.png \n",
    "It shows the same pot and also the range y_min=mx+n-1.3*N<y<mx+n+1.3*N<y_max. The points outside the green area will be discarded as they are considered to be too off to be considered correct.\n",
    "\n",
    "3.4 {base_name}_Derivatives.png\n",
    "It shows the number of negative slopes between points and how steep they are\n",
    "\n",
    "\n",
    "4. CrystallineMLDataBase\n",
    "Contains the .txt files NECESSARY for the ML algorithm. There are two per experiment\n",
    "\n",
    "4.1 {base_name}.txt \n",
    "Contains DeltaTime (the time of the measurement measured from the first VALID polarization measurement), PolarizationD3, SoftPolarizationD3 (the polarization after using a Savitzky-Golay filter) and ErrPolarizationD3 (the uncertainty)\n",
    "\n",
    "4.2 {base_name}_Parameters.txt\n",
    "Contains the CellID, Pressure, LabPolarization (the polarization measured at the lab) and LabTimeCellID (the time when it was measured)\n",
    "\n",
    "\n",
    "5. CrystallineFailuresTest\n",
    "Contains the plots 3.2 and 3.4 but for the experiments that failed the overall decreasing test. Check them if you can to see if any of your experiments has been placed there by mistake\n",
    "\n",
    "\n",
    "\n",
    "6. CrystallineDataBase\n",
    "Contains all the .fli files that were attempted to be read\n",
    "\n",
    "\n",
    "7. CrystallineBadFiles\n",
    "Contains all the .fli separated in experiment sets folders that were rejected (not enough points, negative polarizations, etc.)\n",
    "\n",
    "\n",
    "_________________________________________________________________________________________\n",
    "\n",
    "Process it follows:\n",
    "\n",
    "1. REMOVAL OF PREVIOUS ITERATIONS\n",
    "To avoid leaks and duplications, all files are erased before running the code file\n",
    "\n",
    "2. ZIP FOLDER TREATMENT\n",
    "The code will take all the zip files, extract them, remove duplicates using the name AND hash sha256.\n",
    "\n",
    "3. SEPARATION OF FLI FILES ACCORDING TO EXPERIMENTS\n",
    "\n",
    "Some fli files have the wrong structure (they are not polarization measurementes) and if they are polarization files they can have more than one experiment per file.\n",
    "For evey fli file we will read the contents and try to find the header (a string in an entire line). This symbolizes the beginning of an experiment\n",
    "If there are numerical values before the first header, that means that the process of saving the file occured before changing something of the experiment. These data rows will be skipped\n",
    "A correct fli file will have the following structure:\n",
    "    polariser cell info ge18004 pressure/init. polar 2.29 0.79 initial date/time 17 09 23 @ 10:39\n",
    "    37391   4.000   0.000   1.000 18/09/23 06:20:44     155.03  +z +z     0.8391    0.0156   11.4270    1.2031     120.00\n",
    "    37392   4.000   0.000   1.000 18/09/23 06:26:49     155.05  +x +x     0.8255    0.0110   10.4610    0.7211     300.00\n",
    "    ...\n",
    "\n",
    "Which corresponds to the following information:\n",
    "    String:'polariser cell info', CellID, String:'pressure/init. polar', Pressure(unknown units), InitialLabPolarization, String:'date/time', Day, Month, Year, String:'@', Hour:Minute\n",
    "    Measurement Number, First Miller Index, Second Miller Index, Third Miller Index, Date Of Measurement, Time Of Measurement, Temperature [Kelvin],\n",
    "                        Direction Of Polarization In The First Polarizer Cell (Direction of the quantum operator S_x,S_y,S_z), Direction Of Polarization In The Second Polarizer Cell,\n",
    "                        Polarization, Polarization uncertainty, Flipping Ratio, FlippingRatio Uncertainty, Duration of the measurement\n",
    "\n",
    "The direction +z is chosen to be pointing away from the ground.\n",
    "The direction +x is the direction of the flow of neutrons, i.e, the direction of Scattering.\n",
    "The direction +y is the orthogonal to both of them.\n",
    "D3 uses two polariser cells, one between the reactor and the sample and a second between the sample and the sensor. The first one guarantees that only neutrons with the correct spin direction\n",
    "interacts with the sample. The second one guarantees that only the neutrons that have unchanged spin direction after interacting with the sample are detected by the sensor. This is\n",
    "the reason why the directions (+z,+y,+x,-z,-y,-x) appear twice.\n",
    "We have considered that temperature is not a relevant factor and the flipping ratio has no new information that polarization alrady posseses.\n",
    "First, the code will first locate the first header (ignoring eveything before) and save all the data afterwards (until the next header or end of the document) in a file with the suffix Array_{i} (i is the number of headers already processed in that fli file)\n",
    "Second, it will save the header as a file with the suffix Parameters.\n",
    "Third, the header row and the columns of Measurement Number, Temperature, Flipping Ratio, FlippingRatio Uncertainty and Time Between Measurements will be erased\n",
    "Fourth, as all data measurement uses the +z,+z combination, all other combinations are erased\n",
    "Fifth, not all data from all Miller Index combinations are polarization measurements. Even some of the ones that are polarization measurements are tampered (playing with magnetic fields for example).\n",
    "This means that there needs to be a way to select the correct combination. For starters, irrational Miller indices are not used for measurements with the samples (they need to be discarded)\n",
    "The integer Miller indices combination will be put to the test by all the functions defined before.\n",
    "Sixth, It computes a score depending on how many derivatives are negative, (200 / (200 - percent_neg) - 1) to be precise. This is a normalized score (0-1) with a 1/x evolution. Also it computes a score depending of the size of N, 2 * (-0.5 + 1 / (1 + needed_N / 8.54e-2)) to be precise. 8.54e-2 is the maximum of the data set. If a new maximum is achieved, the score wont be normalized (0-1) but won't break. The final score combines both of these values (addition). Manually I have seen that 1.4 is a good threshold. If Score>1.4 the file will be accepted. If it is smaller it will be discarded by the main code (a False will be returned). It does the m<0 test, writes everything in Summary_txt (filename, Score associated with N, Score associated with Derivatives nad the total Score). If the file was chosen to be forcefully accepted or rejected, a string will appear in the .txt file. Finally it will save plots of both filters in PlotResults if it is correct and in FailureTest if it is considered a bad file. Again, if a new file is added it may be wise to check your experiment in these folders. For more info read the description of FilteringMethodInt = 0 inside the code\n",
    "Seventh, it will try each Miller index combination for a set i value, apply a filter, and return filtered df + PrettyCombination. it will only add the filtered column if enough points & data changes significantly. If it doesn't change too much, the column PolarizationD3 will be duplicated with the new name\n",
    "Eight, it repeats the process of obtaining the area in m*x+n-N < y < m*x+n+N where 75% of the points are inside the area. It also multiplies the value of N by a factor AcceptableMultiplier and erases all points outisde this bigger area. As uncertainties are clearly underestimated I tried to make them reasonable (looking at the dispersion of the points it is clear there is systematic uncertainties. Under the hypothesis that the polarization curve should be a soft curve (at least C^1) we will try to use χ^2 to add a provisional uncertainty margin fitting to a linear expression. This is a very inaccurate uncertainty increase but it is an improvement of the underestimated uncertainties (and the lack of ways to quantify the systematic uncertainty sources). The enlarged area will be plotted and saved in PlotResults for both the normal data and the softened data (Savitzky–Golay filter)    \n",
    "    \n",
    "4. CellID SAVING\n",
    "It will safely store in a txt file all the cell ids so that the code in ML can use them\n",
    "\n",
    "5. DUPLICATION REMOVAL\n",
    "It will check if the files created for the ML algorithm are duplicates and erases them in that case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fade60-e8b5-4a6a-8019-36da7e15df91",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ab732-92c4-4df5-a0cc-82291fbb9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import curve_fit\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "import hashlib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849a8c35-a65a-471c-bbcb-9da7cf01f232",
   "metadata": {},
   "source": [
    "## 2. Auxiliary Functions and log file creation\n",
    "\n",
    "1. _PrintDebug_ is a flag that allows the code to output on screen all the steps. If it is set to false, it won´t show anything. However, all information will be properly logged whether this flag is set to true or false. The name of the log is determined by the variable *log_file_path*. The code runs faster if it is set to False.\n",
    "\n",
    "2. _ShowPlot_ is a similar flag that allows the code to show on screen all plots that are being produced. They are all stored independently of whether this flag is True or False. The code runs faster if it is set to False.\n",
    "\n",
    "3. **log_message** is a function used for writting on the log file\n",
    "\n",
    "4. **win_long_path** is a function that \"fixes\" directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75d92d-c790-440a-aa59-b90d238872dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintDebug = True \n",
    "ShowPlot = False \n",
    "log_file_path = os.path.join(\".\", \"CrystallineLog_Reading_Creation.txt\")\n",
    "# Initialize log file at the start of the script\n",
    "with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(\"=== Log started ===\\n\")\n",
    "\n",
    "def log_message(message):\n",
    "    if PrintDebug:\n",
    "        print(message)\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "        log_file.write(str(message) + \"\\n\")\n",
    "\n",
    "def win_long_path(path):\n",
    "    # Convert to Path and resolve to absolute\n",
    "    path = Path(path).resolve()\n",
    "\n",
    "    # Convert to string\n",
    "    path_str = str(path)\n",
    "\n",
    "    # Prepend \\\\?\\ if not already present\n",
    "    if not path_str.startswith(\"\\\\\\\\?\\\\\"):\n",
    "        path_str = \"\\\\\\\\?\\\\\" + path_str\n",
    "\n",
    "    return path_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3475a75f-7dc9-498e-956f-7441696b4822",
   "metadata": {},
   "source": [
    "## 3. Functions\n",
    "\n",
    "1. **Time** is a function that converts time to a universal format\n",
    "\n",
    "2. **deltatime** is a funtions that computes the difference in time between two sets of time, in seconds \n",
    "\n",
    "3. **format_combination** is just an aesthetic change in the Miller index combination variable\n",
    "\n",
    "4. **sanitize** is a funtion that fixes any directory path with \"illegal\" variables\n",
    "\n",
    "5. **savgol_params_func** is a function that ensures that the window length is odd and large enough for the polynomial order.\n",
    "\n",
    "6. Formating functions: Just optimizations for the names of the files being stored. Nothing of interest\n",
    "\n",
    "6.  **Overall_Decrease** is a function that carries out a set of tests to see if the data set is overall decreasing (a compulsary condition for any polarization _decay_. \n",
    "\n",
    "7. **filter_best_combination**  is a function that discards all problematic data sets (negative polarization, small sets of data and also uses **Overall_Decrease**  \n",
    "            \n",
    "8. **RemoveOutcast_FixUncertainty** is a funtion that removes points that are clear outliers, corrects the underestimation of experimental uncertainty and plots succesful experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae3ac29-b81e-47d6-9dc2-6b2f09842ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Time(Day_Ref, Hour_Ref):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        Day_Ref (str): 'DD/MM/YY' a.k.a Day/Month/Year\n",
    "        Hour_Ref (str): = 'HH:MM' or 'HH:MM:SS' a.k.a Hour:Month:Second\n",
    "        \n",
    "    Returns:\n",
    "        A 'datetime' object with format (year, month, day, hour, minute, second).\n",
    "        \n",
    "    Notes:\n",
    "        If there is no information about the seconds, they will be considered 0\n",
    "    \"\"\"\n",
    "    match = re.match(r\"(\\d+)/(\\d+)/(\\d+)\", Day_Ref)\n",
    "    if match:\n",
    "        DD = int(match.group(1))\n",
    "        MM = int(match.group(2))\n",
    "        YY = int(match.group(3))\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid date format: {Day_Ref}\")\n",
    "\n",
    "    match = re.match(r\"(\\d+):(\\d+):(\\d+)\", Hour_Ref)\n",
    "    if match:\n",
    "        Hour = int(match.group(1))\n",
    "        Minute = int(match.group(2))\n",
    "        Second = int(match.group(3))\n",
    "    else:\n",
    "        # Try HH:MM\n",
    "        match = re.match(r\"(\\d+):(\\d+)\", Hour_Ref)\n",
    "        if match:\n",
    "            Hour = int(match.group(1))\n",
    "            Minute = int(match.group(2))\n",
    "            Second = 0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time format: {Hour_Ref}\")\n",
    "\n",
    "    return datetime(YY + 2000 if YY < 100 else YY, MM, DD, Hour, Minute, Second)\n",
    "\n",
    "################################################################\n",
    "\n",
    "\n",
    "def deltatime(AIni,BIni, AFin,BFin):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        AIni (str): 'DD/MM/YY' a.k.a Day/Month/Year for the initial time\n",
    "        BIni (str): = 'HH:MM' or 'HH:MM:SS' a.k.a Hour:Month:Second for the initial time\n",
    "        AFin (str): 'DD/MM/YY' a.k.a Day/Month/Year for the final time\n",
    "        BFin (str): = 'HH:MM' or 'HH:MM:SS' a.k.a Hour:Month:Second for the final time\n",
    "    Returns:\n",
    "        The time variation in seconds\n",
    "        \n",
    "    Notes:\n",
    "        Requires the function \"Time\"\n",
    "    \"\"\"\n",
    "    time1 = Time(AIni, BIni)\n",
    "    time2 = Time(AFin, BFin)\n",
    "    return( int((time2 - time1).total_seconds()))\n",
    "\n",
    "#################################################################\n",
    "\n",
    "def format_combination(comb):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        comb (float, float, float): A set of three floats characterizing the Miller indices.\n",
    "\n",
    "    Returns:\n",
    "        An object (int,int,int): With the floor integer of those float variables. (3.0 -> 3)\n",
    "    \"\"\"\n",
    "    if comb is None:\n",
    "        return \"(None)\"\n",
    "    ints = tuple(int(float(x)) for x in comb)\n",
    "    return f\"({','.join(map(str, ints))})\"\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def sanitize(name):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        name (str): Directory string \n",
    "    Returns:\n",
    "        The same string but with symbols [,<,>,:,\",/,\\\\,|,?,*,] converted to _\n",
    "    \"\"\"\n",
    "    \n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', name)\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def savgol_params_func(n_points):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        n_points (int): Number of points where the filtered will be used\n",
    "    Returns:\n",
    "        A dictionary containing valid parameters for a Savitzky–Golay filter.\n",
    "    Notes:    \n",
    "        It ensures that the window length is odd and large enough for the polynomial order.\n",
    "    \"\"\"\n",
    "    window_length = min(default_window_length, n_points)\n",
    "    if window_length % 2 == 0:\n",
    "        window_length -= 1\n",
    "    if window_length < polyorder + 2:\n",
    "        window_length = polyorder + 2\n",
    "        if window_length % 2 == 0:\n",
    "            window_length += 1\n",
    "    return {'window_length': window_length, 'polyorder': polyorder}\n",
    "\n",
    "###################################################\n",
    "\n",
    "# This are function used to write titles and names of files in a more concise way. Unimportant for the main pipeline.\n",
    "\n",
    "def make_clean_name(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Turn e.g.\n",
    "      PolarizationD3_CaFeAl_13_7_6_24_2_MillerIndex_(0,0,2)_Filtered.txt\n",
    "    into:\n",
    "      CaFeAl_13_7_6_24_2_(0,0,2)\n",
    "    and handle cases where filename contains '/' or '\\\\' (dates like DD/MM/YY).\n",
    "    \"\"\"\n",
    "    s = str(filename).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")  # prevent path splitting\n",
    "    base = Path(s).stem  # remove extension if present\n",
    "    if base.startswith(\"PolarizationD3_\"):\n",
    "        base = base[len(\"PolarizationD3_\"):]\n",
    "    if base.endswith(\"_Filtered\"):\n",
    "        base = base[:-len(\"_Filtered\")]\n",
    "    base = base.replace(\"MillerIndex_\", \"\")\n",
    "    return base\n",
    "\n",
    "\n",
    "def clean_plot_filename(filename: str, needed_N: Optional[float], plot_folder: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Build a clean filename for linear fit plots.\n",
    "    Example:\n",
    "    155K_2_18_9_23_1_(4,0,1)_N_4.30e-03.png\n",
    "    or\n",
    "    155K_2_18_9_23_1_(4,0,1)_NoNFound.png\n",
    "    \"\"\"\n",
    "    base = make_clean_name(filename)\n",
    "    suffix = f\"_N_{needed_N:.2e}\" if needed_N else \"_NoNFound\"\n",
    "    return plot_folder / f\"{base}{suffix}.png\"\n",
    "\n",
    "def derivative_plot_filename(filename: str, plot_folder: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Build a clean filename for derivative plots.\n",
    "    Example:\n",
    "    CaFeAl_13_7_6_24_2_(0,0,2)_Derivatives.png\n",
    "    \"\"\"\n",
    "    base = make_clean_name(filename)\n",
    "    return plot_folder / f\"{base}_Derivatives.png\"\n",
    "\n",
    "def extended_area_plot_filename(filename: str) -> str:\n",
    "    \"\"\"EuAgAs_5_31_10_23_0_(3,0,0)_ExtendedArea.png\"\"\"\n",
    "    return f\"{make_clean_name(filename)}_ExtendedArea.png\"\n",
    "    \n",
    "\n",
    "#####################################################\n",
    "\n",
    "def Overall_Decrease (df_filtered, filename):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        df_filtered (pandas object): It is something like this:\n",
    "                1    2    3  PolarizationD3  ErrPolarizationD3  DeltaTime  \n",
    "            0  3.0  3.0  3.0          0.5552             0.0022          0   \n",
    "            1  3.0  3.0  3.0          0.5522             0.0021        279   \n",
    "            2  3.0  3.0  3.0          0.5464             0.0022       4148\n",
    "            ...\n",
    "        filename (str): A string with information about the experiment. For example:\n",
    "            f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "    Returns:\n",
    "        True if df_filtered shows a decay and False otherwise\n",
    "        \n",
    "    Notes:    \n",
    "        The code can run 5 different filtering methods. However the most promising one was FilteringMethod12 as it is the most complete one.\n",
    "        You can check the explanation of it when the elif is set to FilteringMethod12. It is the first one used.\n",
    "        Crystaline experiments are less stable than the amorphous ones. Therefore, more checks are needed.\n",
    "        The other methods are left as vestigial code so that any one who dares tackle them can know that those ideas were not good enough.\n",
    "    \"\"\"\n",
    "    FilteringMethodInt = 12 \n",
    "    \"\"\"\n",
    "    This int chooses the method for filtering the wrong data. The options are:\n",
    "        FilteringMethodInt = 0   \n",
    "        FilteringMethodInt = 1\n",
    "        FilteringMethodInt = 2\n",
    "        FilteringMethodInt = 12\n",
    "        FilteringMethodInt = 3\n",
    "    \"\"\"\n",
    "    \n",
    "    force_accept_files = [\n",
    "        \"PolarizationD3_EuAgAs_29_8_23_0_MillerIndex_(0,0,4)_Filtered.txt\",\n",
    "        \"PolarizationD3_EuAgAs_1_30_8_23_5_MillerIndex_(3,0,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_SmI3_26_9_23_1_MillerIndex_(0,3,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_SmI3_1_28_9_23_3_MillerIndex_(0,3,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_hor_1_22_3_24_1_MillerIndex_(3,0,0)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_hor_15_3_24_0_MillerIndex_(0,0,2)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_ver_14_3_24_1_(1,0,0)_Filtered.txt\"]\n",
    "    force_reject_files = [\n",
    "        \"PolarizationD3_EuAgAs_1_29_8_23_0_MillerIndex_(1,1,1)_Filtered.txt\",\n",
    "        \"PolarizationD3_MnSn-c_ver_14_3_24_1_MillerIndex_(1,0,0)_Filtered.txt\"] #The values were in the 0.3 to 0.2 range. Not a polarization\n",
    "    \n",
    "    \"\"\"\n",
    "    Some files failed the FilteringMethodInt = 12 method despite being considered good files. Others passed the test but were clearly tampered.\n",
    "    These lists override the testing and accepts/rejets them immediately\n",
    "    \"\"\"\n",
    "    output_folder = Path.cwd() / \"CrystallinePlotResults\"\n",
    "    failures_folder = Path.cwd() / \"CrystallineFailuresTest\"\n",
    "\n",
    "    # Use win_long_path() to bypass Windows MAX_PATH limit\n",
    "    os.makedirs(win_long_path(output_folder), exist_ok=True)\n",
    "    os.makedirs(win_long_path(failures_folder), exist_ok=True)\n",
    "    x = df_filtered['DeltaTime'].values\n",
    "    y = df_filtered['SoftPolarizationD3'].values #The tests are done with the data set filtered by the Savitzky–Golay for better results\n",
    "    if FilteringMethodInt == 12:\n",
    "        \"\"\"\n",
    "        This method combines method 1 and 2 (that is the reason why it is called 12)\n",
    "        It computes a score depending on how many derivatives are negative, (200 / (200 - percent_neg) - 1) to be precise. This is a normalized score (0-1) with a 1/x evolution\n",
    "        It computes a score depending of the size of N, 2 * (-0.5 + 1 / (1 + needed_N / 8.54e-2)) to be precise. 8.54e-2 is the maximum of the data set.\n",
    "        If a new maximum is achieved, the score wont be normalized (0-1) but it won't break either (future-proof).\n",
    "        The final score combines both of these values (addition). Manually I have seen that 1.4 is a good threshold. If Score>1.4 the file will be accepted.\n",
    "        If it is smaller it will be discarded by the main code (a False will be returned).\n",
    "        Does the m<0 test.\n",
    "        Write eveything in Summary_txt (filename, Score associated with N, Score associated with Derivatives nad the total Score).\n",
    "        If the file was chosen to be forcefully accepted or rejected, a string will appear in the .txt file.\n",
    "        Saves plots of both filters in PlotResults if it is correct and in FailureTest if it is considered a bad file.\n",
    "        Again, if a new file is added it may be wise to check your experiment in these folders. For more info read the description of FilteringMethodInt = 0\n",
    "        \"\"\"\n",
    "        output_folder = Path.cwd() / \"CrystallinePlotResults\"\n",
    "        failures_folder = Path.cwd() / \"CrystallineFailuresTest\"\n",
    "        os.makedirs(win_long_path(output_folder), exist_ok=True)\n",
    "        os.makedirs(win_long_path(failures_folder), exist_ok=True)\n",
    "    \n",
    "        x = df_filtered['DeltaTime'].values\n",
    "        y = df_filtered['SoftPolarizationD3'].values\n",
    "    \n",
    "        def linear_func(x, m, n):\n",
    "            return m * x + n\n",
    "    \n",
    "        try:\n",
    "            popt, _ = curve_fit(linear_func, x, y)\n",
    "            m, n = popt\n",
    "            log_message(f\"      Linear fit slope m={m:.4e}, intercept n={n:.4f}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error fitting data: {e}\")\n",
    "            return False\n",
    "        if m > 0:\n",
    "            log_message(f\"      Overall slope is positive. Can't be polarization information. Skipping Combination\")\n",
    "            return False\n",
    "\n",
    "        BandWidthScore = None\n",
    "        DerivativeScore = None\n",
    "        needed_N = None\n",
    "    \n",
    "        # N obtainment\n",
    "        num_points = len(x)\n",
    "        sorted_idx = np.argsort(x)\n",
    "        x_sorted = x[sorted_idx]\n",
    "        y_sorted = y[sorted_idx]\n",
    "    \n",
    "        N_start = 0.0001\n",
    "        N_step = 0.0001\n",
    "        N_max = 0.4\n",
    "        N = N_start\n",
    "        while N <= N_max:\n",
    "            upper_band = linear_func(x_sorted, m, n) + N\n",
    "            lower_band = linear_func(x_sorted, m, n) - N\n",
    "            inside = np.logical_and(y_sorted <= upper_band, y_sorted >= lower_band)\n",
    "            percent_inside = np.sum(inside) / num_points * 100\n",
    "    \n",
    "            if percent_inside >= 75:\n",
    "                needed_N = N\n",
    "                break\n",
    "            N += N_step\n",
    "    \n",
    "        if needed_N is not None:\n",
    "            # Band N test\n",
    "            BandWidthScore = 2 * (-0.5 + 1 / (1 + needed_N / 8.54e-2))\n",
    "            log_message(f\"      Needed N=±{needed_N:.2e}\")\n",
    "            log_message(f\"      BandWidthScore={BandWidthScore:.4f}\")\n",
    "        else:\n",
    "            log_message(f\"      No N found ≤ {N_max:.2e}\")\n",
    "    \n",
    "        # Derivative test\n",
    "        derivatives = np.diff(y_sorted) / np.diff(x_sorted)\n",
    "        num_derivatives = len(derivatives)\n",
    "        num_neg = np.sum(derivatives < 0)\n",
    "        percent_neg = (num_neg / num_derivatives) * 100 if num_derivatives > 0 else None\n",
    "    \n",
    "        if percent_neg is not None:\n",
    "            log_message(f\"      Derivatives computed: {num_derivatives}\")\n",
    "            log_message(f\"      Negative: {num_neg} ({percent_neg:.2f}%)\")\n",
    "            if percent_neg > 1:\n",
    "                DerivativeScore = (200 / (200 - percent_neg) - 1)\n",
    "            else:\n",
    "                DerivativeScore = (2 / (2 - percent_neg) - 1)\n",
    "            log_message(f\"      DerivativeScore={DerivativeScore:.4f}\")\n",
    "        else:\n",
    "            log_message(f\"      No derivatives computed\")\n",
    "    \n",
    "        # Final Score\n",
    "        if BandWidthScore is not None and DerivativeScore is not None:\n",
    "            score = BandWidthScore + DerivativeScore\n",
    "        elif BandWidthScore is not None:\n",
    "            log_message(f\"      Missing DerivativeScore in experiment {filename}\")\n",
    "            score = 2 * BandWidthScore\n",
    "        elif DerivativeScore is not None:\n",
    "            log_message(f\"      Missing BandWidthScore in experiment {filename}\")\n",
    "            score = 2 * DerivativeScore\n",
    "        else:\n",
    "            log_message(f\"      Missing BandWidthScore and DerivativeScore in experiment {filename}\")\n",
    "            score = 0\n",
    "    \n",
    "        log_message(f\"      Final score={score:.4f}\")\n",
    "    \n",
    "        # Combined Test\n",
    "        threshold = 1.4\n",
    "        is_good = score > threshold\n",
    "        was_force_accepted = False\n",
    "        was_force_rejected = False\n",
    "        if filename in force_accept_files:\n",
    "            log_message(f\"      File {filename} is in force_accept_files → passing filter even if score is low\")\n",
    "            is_good = True\n",
    "            was_force_accepted = True\n",
    "        elif filename in force_reject_files:\n",
    "            log_message(f\"      File {filename} is in force_reject_files → rejecting even if score is high\")\n",
    "            is_good = False\n",
    "            was_force_rejected = True\n",
    "        summary_path = output_folder / \"Crystalline_Reading_Summary.txt\"\n",
    "        with open(win_long_path(summary_path), 'a') as summary_file:\n",
    "            summary_file.write(\n",
    "                f\"{filename}: N={'No N found' if needed_N is None else f'{needed_N:.2e}'}, \"\n",
    "                f\"BandWidthScore={'None' if BandWidthScore is None else f'{BandWidthScore:.4f}'}, \"\n",
    "                f\"Negative derivatives={'No Deriv found' if percent_neg is None else f'{percent_neg:.2f}%'}, \"\n",
    "                f\"DerivativeScore={'None' if DerivativeScore is None else f'{DerivativeScore:.4f}'}, \"\n",
    "                f\"TotalScore={score:.4f}\"\n",
    "            )\n",
    "            if was_force_accepted:\n",
    "                summary_file.write(\" [FORCE ACCEPTED]\")\n",
    "            elif was_force_rejected:\n",
    "                summary_file.write(\" [FORCE Rejected]\")\n",
    "            elif not is_good:\n",
    "                summary_file.write(\" ***\")\n",
    "            summary_file.write(\"\\n\")\n",
    "\n",
    "        # Make and save plots in the correct folder\n",
    "        plot_folder = output_folder if is_good else failures_folder\n",
    "       \n",
    "        #Redundant but just in case there were unwanted changes in x, y and Err\n",
    "        x = df_filtered['DeltaTime'].values\n",
    "        y = df_filtered['SoftPolarizationD3'].values\n",
    "        Err = df_filtered['ErrPolarizationD3'].values if 'ErrPolarizationD3' in df_filtered.columns else np.zeros_like(y)\n",
    "        # === Linear Fit Plot ===\n",
    "        plt.figure(figsize=(8,5))\n",
    "\n",
    "        # Black points with error bars\n",
    "        plt.scatter(x_sorted, y_sorted, color='black', s=30, label='Data', marker='o')\n",
    "        if Err is not None:\n",
    "            plt.errorbar(x_sorted, y_sorted, yerr=Err, fmt='none', ecolor='black', alpha=0.6, capsize=2)\n",
    "\n",
    "        # Build a clean name for titles / saving\n",
    "        clean_name = (\n",
    "            filename.replace(\"PolarizationD3_\", \"\")\n",
    "                    .replace(\"MillerIndex_\", \"\")\n",
    "                    .replace(\"_Filtered.txt\", \"\"))\n",
    "        \n",
    "        # Blue linear fit\n",
    "        plt.plot(x_sorted, linear_func(x_sorted, m, n), '-', color='blue', label='Fit')\n",
    "        \n",
    "        # Light blue band around the fit\n",
    "        if needed_N is not None:\n",
    "            upper_band = linear_func(x_sorted, m, n) + needed_N\n",
    "            lower_band = linear_func(x_sorted, m, n) - needed_N\n",
    "            plt.fill_between(\n",
    "                x_sorted,\n",
    "                lower_band,\n",
    "                upper_band,\n",
    "                color='lightblue',\n",
    "                alpha=0.4,\n",
    "                label=f'Band ±{needed_N:.2e}'\n",
    "            )\n",
    "        \n",
    "        plt.xlabel(r'$\\Delta t$ (s)')\n",
    "        plt.ylabel(r'$P_{\\mathrm{soft}}$')\n",
    "        plt.title(f\"Linear fit for {clean_name}\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = clean_plot_filename(filename, needed_N, plot_folder)\n",
    "        plt.savefig(win_long_path(plot_path), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # === Derivative Plot ===\n",
    "        mid_x = (x_sorted[:-1] + x_sorted[1:]) / 2\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(mid_x, derivatives, marker='o', color='black')\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.xlabel(r'$\\Delta t$ (s)')\n",
    "        plt.ylabel('Derivative')\n",
    "        plt.title(f\"Derivatives of {clean_name}\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = derivative_plot_filename(filename, plot_folder)\n",
    "        plt.savefig(win_long_path(plot_path), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "        return is_good\n",
    "    \n",
    "    #From now on you have the failed methods. They are left so that anyone who tries anything new knows what things did not work.\n",
    "    \"\"\"\n",
    "    elif FilteringMethodInt == 0:\n",
    "        \n",
    "        #This method computes the area in the plot that satisfies mx+n-σ_n < y < mx+n+σ_n\n",
    "        #Then it finds the value of N so that mx+n-Nkσ_n < y < mx+n+k*σ_n contains 75% of the points\n",
    "        #This method seemed innefective as good files with lots of points needed aburd values of k despite being the only really good files\n",
    "        #The only good filtering capability is checking if the slope m is positive (if it is positive it is impossible to have a decreasing polarization evolution)\n",
    "        #The summary of all files and thier scores with the test are being saved in Sigmas.txt (if you can't find it is because this method has not been used (it is useless anyways)\n",
    "        #The method is saved only as a \"show of concept\"\n",
    "        \n",
    "        def linear_func(x, m, n):\n",
    "            return m*x + n\n",
    "        try:\n",
    "            popt, pcov = curve_fit(linear_func, x, y)\n",
    "            m, n = popt\n",
    "            sigma_n = np.sqrt(np.diag(pcov))[1]  # uncertainty in n\n",
    "    \n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error fitting data: {e}\")\n",
    "            return False\n",
    "    \n",
    "        # Step 4: check slope\n",
    "        if m > 0:\n",
    "            log_message(f\"      Overall slope is positive. Can't be polarization information. Skipping Combination\")\n",
    "            return False\n",
    "    \n",
    "        # Step 5–7: try sigma_n bands\n",
    "        num_points = len(x)\n",
    "        sorted_idx = np.argsort(x)\n",
    "        x_sorted = x[sorted_idx]\n",
    "        y_sorted = y[sorted_idx]\n",
    "    \n",
    "        needed_sigma = None\n",
    "        max_sigma = 20\n",
    "    \n",
    "        for k in range(1, max_sigma+1):\n",
    "            upper_band = linear_func(x_sorted, m, n) + k*sigma_n\n",
    "            lower_band = linear_func(x_sorted, m, n) - k*sigma_n\n",
    "    \n",
    "            inside = np.logical_and(y_sorted <= upper_band, y_sorted >= lower_band)\n",
    "            percent_inside = np.sum(inside) / num_points * 100\n",
    "    \n",
    "            if percent_inside >= 75:\n",
    "                needed_sigma = k\n",
    "                # Step 6: plot\n",
    "                plt.figure(figsize=(8,5))\n",
    "                plt.plot(x_sorted, y_sorted, 'o', label='Data')\n",
    "                plt.plot(x_sorted, linear_func(x_sorted, m, n), '-', label='Fit')\n",
    "                plt.fill_between(x_sorted, lower_band, upper_band, color='gray', alpha=0.3,\n",
    "                                 label=f'Band ±{k}·sigma_n')\n",
    "                plt.xlabel(r'$\\Delta t$ (s)')   # Delta time in seconds\n",
    "                plt.ylabel(r'$P_{\\mathrm{soft}}$')  # SoftPolarizationD3\n",
    "\n",
    "                plt.title(r'Fit: $y = %.2e \\cdot x + %.2e$, $\\sigma_n=%.2e$' % (m, n, sigma_n))\n",
    "\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                # Save plot\n",
    "                plot_filename = output_folder / f\"{filename}_AutomaticRange.png\"\n",
    "                plt.savefig(plot_filename)\n",
    "                log_message(f\"      Plot saved to {plot_filename}\")\n",
    "                plt.close()\n",
    "                break\n",
    "    \n",
    "        # Step 8: write txt file with needed sigma\n",
    "        # Common file in the same folder\n",
    "        sigmas_txt_path = output_folder / \"Sigmas.txt\"\n",
    "        \n",
    "        with open(sigmas_txt_path, 'a') as f:\n",
    "            if needed_sigma is not None:\n",
    "                f.write(f\"{filename}: needed sigma multiplier = {needed_sigma}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{filename}: needed sigma multiplier > {max_sigma}\\n\")\n",
    "        log_message(f\"      Sigma info saved to {sigmas_txt_path}\")\n",
    "    \n",
    "    \n",
    "        # Step 9: final decision\n",
    "        if needed_sigma is None or needed_sigma > max_sigma:\n",
    "            log_message(f\"      Needed > {max_sigma}·sigma_n → file considered bad\")\n",
    "            return False\n",
    "        else:\n",
    "            log_message(f\"      Approximately decreasing. Needed sigma multiplier: {needed_sigma}\")\n",
    "            return True\n",
    "\n",
    "\n",
    "    \n",
    "    elif FilteringMethodInt == 1:\n",
    "        \n",
    "        #This method computes the area in the plot that satisfies mx+n-N < y < mx+n+N\n",
    "        #Then it finds the value of N so that mx+n-N*σ_n < y < mx+n+N*σ_n contains 75% of the points\n",
    "        #This method seemed much more efficient as it didn't depend of the linearity of the fit but the dispersion of the points from a first order approxiamtion\n",
    "        #Still, it is not bullet-proof. Again, just a \"show of concept\"\n",
    "        #Saves everything in Bands.txt and the plots have in the title the value of N.\n",
    "        #Also does the m<0 test (really important)\n",
    "\n",
    "        def linear_func(x, m, n):\n",
    "            return m * x + n\n",
    "\n",
    "        try:\n",
    "            popt, _ = curve_fit(linear_func, x, y)\n",
    "            m, n = popt\n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error fitting data: {e}\")\n",
    "            return False\n",
    "\n",
    "        if m > 0:\n",
    "            log_message(f\"      Overall slope is positive. Can't be polarization information. Skipping Combination\")\n",
    "            return False\n",
    "\n",
    "        num_points = len(x)\n",
    "        sorted_idx = np.argsort(x)\n",
    "        x_sorted = x[sorted_idx]\n",
    "        y_sorted = y[sorted_idx]\n",
    "\n",
    "        needed_N = None\n",
    "        N_start = 0.0001\n",
    "        N_step = 0.0001\n",
    "        N_max = 0.4\n",
    "\n",
    "        N = N_start\n",
    "        while N <= N_max:\n",
    "            upper_band = linear_func(x_sorted, m, n) + N\n",
    "            lower_band = linear_func(x_sorted, m, n) - N\n",
    "\n",
    "            inside = np.logical_and(y_sorted <= upper_band, y_sorted >= lower_band)\n",
    "            percent_inside = np.sum(inside) / num_points * 100\n",
    "\n",
    "            if percent_inside >= 75:\n",
    "                needed_N = N\n",
    "                plt.figure(figsize=(8,5))\n",
    "                plt.plot(x_sorted, y_sorted, 'o', label='Data')\n",
    "                plt.plot(x_sorted, linear_func(x_sorted, m, n), '-', label='Fit')\n",
    "                plt.fill_between(x_sorted, lower_band, upper_band, color='gray', alpha=0.3,\n",
    "                                 label=f'Band ±{N:.2e}')\n",
    "                plt.xlabel(r'$\\Delta t$ (s)')   # Delta time in seconds\n",
    "                plt.ylabel(r'$P_{\\mathrm{soft}}$')  # SoftPolarizationD3\n",
    "\n",
    "                plt.title(r'Fit: $y = %.2e \\cdot x + %.2e$, $N=%.2e$' % (m, n, N))\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plot_filename = output_folder / f\"{filename}_N_{N:.2f}_ManualInterval.png\"\n",
    "                plt.savefig(plot_filename)\n",
    "                log_message(f\"      Plot saved to {plot_filename}\")\n",
    "                plt.close()\n",
    "                break\n",
    "\n",
    "            N += N_step\n",
    "\n",
    "        bands_txt_path = output_folder / \"Bands.txt\"\n",
    "        with open(bands_txt_path, 'a') as f:\n",
    "            if needed_N is not None:\n",
    "                f.write(f\"{filename}: needed band width = ±{needed_N:.2e}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{filename}: needed band width > ±{N_max:.2e}\\n\")\n",
    "        log_message(f\"      Band info saved to {bands_txt_path}\")\n",
    "\n",
    "        if needed_N is None:\n",
    "            log_message(f\"      Needed band width > ±{N_max:.2e} → file considered bad\")\n",
    "            return False\n",
    "        else:\n",
    "            log_message(f\"      Approximately decreasing. Needed band width: ±{needed_N:.2e}\")\n",
    "            return True\n",
    "\n",
    "    \n",
    "    elif FilteringMethodInt == 2:\n",
    "        \n",
    "        #This method computes the \"number of negative derivatives\".\n",
    "        #The defintion of derivative of a point is $f'(x_0) = lim_x\\righarrow {x_0}\\frac{f(x+x_0)-f(x_0)}{x-x_0}$.\n",
    "        #It is the division of the substraction of two images from two points infinitey close over their distance (metric space needed).\n",
    "        #If the data set is not continious we can obtain the slope between two points using a similar idea $f'(x_j):=\\frac{y_{j+1}-y_j}{x_{j+1}-x_j}$\n",
    "        #If 50% of slopes are negative we could say the overall curve is decreasing (which may not be true)\n",
    "        #A 100% can't be asked as noise in measurement can make some \"derivatives\" to be positive. \n",
    "        #Again, another show of concept\n",
    "        #Does not do the m<0 test\n",
    "\n",
    "        num_points = len(x)\n",
    "        derivatives = []\n",
    "    \n",
    "        # Compute derivative for sequential pairs\n",
    "        for i in range(num_points - 1):\n",
    "            x1, y1 = x[i], y[i]\n",
    "            x2, y2 = x[i+1], y[i+1]\n",
    "            if x2 != x1:\n",
    "                derivative = (y2 - y1) / (x2 - x1)\n",
    "                derivatives.append(derivative)\n",
    "            else:\n",
    "                log_message(f\"      Skipping derivative at index {i} due to zero delta x\")\n",
    "    \n",
    "        derivatives = np.array(derivatives)\n",
    "        num_derivatives = len(derivatives)\n",
    "    \n",
    "        num_neg = np.sum(derivatives < 0)\n",
    "        num_pos = np.sum(derivatives > 0)\n",
    "    \n",
    "        percent_neg = num_neg / num_derivatives * 100 if num_derivatives > 0 else 0\n",
    "        percent_pos = num_pos / num_derivatives * 100 if num_derivatives > 0 else 0\n",
    "    \n",
    "        # Log results\n",
    "        bands_txt_path = output_folder / \"Bands.txt\"\n",
    "        with open(bands_txt_path, 'a') as f:\n",
    "            f.write(f\"{filename}: Total derivatives: {num_derivatives}\\n\")\n",
    "            f.write(f\"{filename}: Negative derivatives: {num_neg} ({percent_neg:.2f}%)\\n\")\n",
    "            f.write(f\"{filename}: Positive derivatives: {num_pos} ({percent_pos:.2f}%)\\n\")\n",
    "    \n",
    "        log_message(f\"      Derivatives computed: {num_derivatives}\")\n",
    "        log_message(f\"      Negative: {num_neg} ({percent_neg:.2e}%)\")\n",
    "        log_message(f\"      Positive: {num_pos} ({percent_pos:.2e}%)\")\n",
    "        log_message(f\"      Band info saved to {bands_txt_path}\")\n",
    "        mid_x = (x[:-1] + x[1:]) / 2\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(mid_x, derivatives, marker='o')\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.xlabel(r'$\\Delta t$ (s)')\n",
    "        plt.ylabel(r'Derivative')\n",
    "        plt.title(f'Derivatives of {filename}')\n",
    "        plot_filename = output_folder / f\"{filename}_plot_Derivatives.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "\n",
    "        # Placeholder decision: if ≥ 50% negative, return True\n",
    "        if percent_neg >= 50:\n",
    "            log_message(f\"      Majority negative derivatives → file considered decreasing\")\n",
    "            return True\n",
    "        else:\n",
    "            log_message(f\"      Majority positive derivatives → file considered NOT decreasing\")\n",
    "            return False\n",
    "\n",
    "    elif FilteringMethodInt == 3:\n",
    "    \n",
    "        #The N bands with a sinh, cosh fit. Was useless. Saved so that no one loses their time with it\n",
    "        def fit_func(x, a, b, c, d):\n",
    "            return a * np.sinh(b * x) + c * np.cosh(d * x)\n",
    "    \n",
    "        try:\n",
    "            popt, _ = curve_fit(fit_func, x, y, maxfev=1000000)\n",
    "            a, b, c, d = popt\n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error fitting data: {e}\")\n",
    "            return False\n",
    "            \n",
    "        num_points = len(x)\n",
    "        sorted_idx = np.argsort(x)\n",
    "        x_sorted = x[sorted_idx]\n",
    "        y_sorted = y[sorted_idx]\n",
    "    \n",
    "        needed_N = None\n",
    "        N_start = 0.0001\n",
    "        N_step = 0.0001\n",
    "        N_max = 0.4\n",
    "    \n",
    "        N = N_start\n",
    "        while N <= N_max:\n",
    "            fitted_curve = fit_func(x_sorted, a, b, c, d)\n",
    "            upper_band = fitted_curve + N\n",
    "            lower_band = fitted_curve - N\n",
    "    \n",
    "            inside = np.logical_and(y_sorted <= upper_band, y_sorted >= lower_band)\n",
    "            percent_inside = np.sum(inside) / num_points * 100\n",
    "    \n",
    "            if percent_inside >= 75:\n",
    "                needed_N = N\n",
    "                plt.figure(figsize=(8,5))\n",
    "                plt.plot(x_sorted, y_sorted, 'o', label='Data')\n",
    "                plt.plot(x_sorted, fitted_curve, '-', label='Fit')\n",
    "                plt.fill_between(x_sorted, lower_band, upper_band, color='gray', alpha=0.3,\n",
    "                                 label=f'Band ±{N:.2e}')\n",
    "                plt.xlabel(r'$\\Delta t$ (s)')\n",
    "                plt.ylabel(r'$P_{\\mathrm{soft}}$')\n",
    "    \n",
    "                plt.title(\n",
    "                    r'Fit: $a \\cdot \\sinh(bx) + c \\cdot \\cosh(dx)$' + '\\n' +\n",
    "                    r'$a=%.2e$, $b=%.2e$, $c=%.2e$, $d=%.2e$, $N=%.2e$' % (a, b, c, d, N)\n",
    "                )\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plot_filename = output_folder / f\"FIT_{filename}_N_{N:.2f}_plot.png\"\n",
    "                plt.plot()\n",
    "                plt.savefig(plot_filename)\n",
    "                log_message(f\"      Plot saved to {plot_filename}\")\n",
    "                plt.close()\n",
    "                break\n",
    "    \n",
    "            N += N_step\n",
    "    \n",
    "        bands_txt_path = output_folder / \"Bands.txt\"\n",
    "        with open(bands_txt_path, 'a') as f:\n",
    "            if needed_N is not None:\n",
    "                f.write(f\"{filename}: needed band width = ±{needed_N:.2e}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{filename}: needed band width > ±{N_max:.2e}\\n\")\n",
    "        log_message(f\"      Band info saved to {bands_txt_path}\")\n",
    "    \n",
    "        if needed_N is None:\n",
    "            log_message(f\"      Needed band width > ±{N_max:.2e} → file considered bad\")\n",
    "            return False\n",
    "        else:\n",
    "            log_message(f\"      Approximately fits inside band. Needed band width: ±{needed_N:.2e}\")\n",
    "            return True\n",
    "\n",
    "    else:\n",
    "        # === Add other methods here as needed ===\n",
    "        log_message(f\"      Unknown FilteringMethodInt = {FilteringMethodInt}\")\n",
    "        return False\n",
    "    \"\"\"\n",
    "\n",
    "################################################\n",
    "\n",
    "def filter_best_combination(i, df):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. i (int): The chunk number a.k.a the (ordinal) number of the Miller index combination\n",
    "        2. df (pandas object): It is something like this: (NaNs are intended)\n",
    "                  1    2    3  PolarizationD3 ErrPolarizationD3  12   13   14  DeltaTime\n",
    "            0   3.0  3.0  3.0          0.5383            0.0021 NaN  NaN  NaN          0\n",
    "            1   3.0  3.0  3.0          0.5379            0.0021 NaN  NaN  NaN        147\n",
    "            2   3.0  3.0  3.0          0.5315            0.0022 NaN  NaN  NaN       3919\n",
    "            ...\n",
    "\n",
    "    Returns:\n",
    "        1. A filtered df object like this one:\n",
    "                  1    2    3  PolarizationD3 ErrPolarizationD3 DeltaTime\n",
    "            0   3.0  3.0  3.0          0.5383            0.0021         0\n",
    "            1   3.0  3.0  3.0          0.5379            0.0021       147\n",
    "            2   3.0  3.0  3.0          0.5315            0.0022      3919\n",
    "            ...\n",
    "        2. An object (int,int,int) with the adequate Miller index combination.\n",
    "        \n",
    "    Notes:    \n",
    "        First, it extracts the Miller index combination and converts it into a set of three integers (format_combination)\n",
    "        Then it tries a couple of tests to see if the data associated to them is valid\n",
    "            1. Check there is data\n",
    "            2. Check if the time array is present and convert all values to either floats or integers\n",
    "            3. Check if all polarization values are positive. If they are not, skip that Miller index combination\n",
    "            4. Check if there are more than three rows of data. If there are not, skip that Miller index combination\n",
    "            5. Check if the filtered df object passes the Overall_Decrease\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    filter_func=savgol_filter #Only tested for 'savgol_filter'\n",
    "    filter_params_func=savgol_params_func #Only tested for the previously defined function 'savgol_params_func'\n",
    "    min_points_required=3  #Minimum points needed for the filter to work (3 for Savitzky-Golay)\n",
    "    tolerance=1e-8 #Tolerance to decide if the filtered value is worth keeping\n",
    "    filter_column_idx=df.columns.get_loc('PolarizationD3')\n",
    "    time_column_idx=df.columns.get_loc('DeltaTime')\n",
    "    error_column_idx=df.columns.get_loc('ErrPolarizationD3')\n",
    "    new_column_name='SoftPolarizationD3'\n",
    "    folder_name = FileName.replace(\".fli\", \"\")    \n",
    "\n",
    "    \n",
    "    # Group by first three columns (Miller indices)\n",
    "    combination_counts = (\n",
    "        df.groupby([df.columns[0], df.columns[1], df.columns[2]])\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    log_message(f\"Analyzing combinations in file: {folder_name}_Array_{i}.fli\")\n",
    "    #Read the three numbers from the .fli file\n",
    "    for comb, count in combination_counts.items(): \n",
    "        log_message(f\"Combination {comb} occurs {count} times in file {folder_name}.fli. Trying this combination\")\n",
    "        mask = (\n",
    "            (df.iloc[:,0] == comb[0]) &\n",
    "            (df.iloc[:,1] == comb[1]) &\n",
    "            (df.iloc[:,2] == comb[2])\n",
    "        )\n",
    "        PrettyCombination = format_combination(comb)\n",
    "        \n",
    "\n",
    "        filtered_df = df.loc[mask].copy()\n",
    "        # Requisites for the Combination to be valid:\n",
    "        # Requisite 1: Have data in the data (duh)\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      {PrettyCombination} has no data\")\n",
    "            continue\n",
    "        \n",
    "        # Requisite 2: Check if data column exists\n",
    "        if filtered_df.shape[1] <= filter_column_idx:\n",
    "            log_message(f\"      Expected column index {filter_column_idx} not found. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert to numeric all columns (all columns are considered as object type)\n",
    "        filtered_df = filtered_df.apply(pd.to_numeric, errors='coerce')\n",
    "        filtered_df = filtered_df.dropna()  # drops any rows with NaNs introduced by coercion\n",
    "        \n",
    "        # Check dtypes\n",
    "        all_numeric = all(dtype.kind in ('f', 'i') for dtype in filtered_df.dtypes)\n",
    "        \n",
    "        if all_numeric:\n",
    "            log_message(f\"      All columns have been successfully converted to numbers.\")\n",
    "        else:\n",
    "            log_message(f\"      Not all columns are numbers. Current dtypes:\")\n",
    "            log_message(f\"      {filtered_df.dtypes}\")\n",
    "            log_message(f\"      Expect Error Message from Python. Perhaps removing this file might be wise unless all files have the same issue\")\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      All rows dropped after conversion to numeric. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Requisite 3: Polarization is ALWAYS positive. If any is negative, that is not a polarization. Immediately sent to the Bad Files Folder\n",
    "        if (filtered_df.iloc[:, filter_column_idx] < 0).any():\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "\n",
    "            # Ensure folder creation with long path support\n",
    "            os.makedirs(win_long_path(badfile_subfolder), exist_ok=True)\n",
    "\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "            filtered_df.to_csv(win_long_path(badfiles_txt_path), index=False, sep='\\t')\n",
    "\n",
    "            log_message(f\"      {PrettyCombination} has negative polarization values. Sent to BadFiles with name {filename}. Skipping to next Combination\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Requisite 4: Have at least three rows (otherwise we can't teach the ML algorythm nothing).\n",
    "        if len(filtered_df) < min_points_required:\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "\n",
    "            # Ensure folder creation with long path support\n",
    "            os.makedirs(win_long_path(badfile_subfolder), exist_ok=True)\n",
    "\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "            filtered_df.to_csv(win_long_path(badfiles_txt_path), index=False, sep='\\t')\n",
    "\n",
    "            log_message(f\"      {PrettyCombination} has only {len(filtered_df)} rows (< {min_points_required}). Sent to BadFiles with name {filename}. Skipping to next Combination\")\n",
    "            continue\n",
    "\n",
    "        # \"Requisite 5\": Be worthy of having the filter used.\n",
    "        y = filtered_df.iloc[:, filter_column_idx].values\n",
    "        filter_params = filter_params_func(len(y))\n",
    "        try:\n",
    "            y_filtered = filter_func(y, **filter_params)\n",
    "            diff = np.abs(y - y_filtered)\n",
    "            changed_count = np.sum(diff > tolerance)\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "            if changed_count > 0:\n",
    "                log_message(f\"      Filter changed {changed_count}/{len(y)} points. Adding column '{new_column_name}'.\")\n",
    "            else:\n",
    "                log_message(f\"      Filter applied but data unchanged. Adding '{new_column_name}' as duplicated values.\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error applying filter to combination {comb}: {e}\")\n",
    "            log_message(f\"      Adding '{new_column_name}' as duplicated values to proceed anyway.\")\n",
    "            # Just duplicate the original column\n",
    "            y_filtered = y.copy()\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "\n",
    "        # Requisite 6: Add the Derivative and BandWidth filtering logic\n",
    "        if Overall_Decrease(filtered_df, f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"):\n",
    "            log_message(f\"      {PrettyCombination} has surpassed all tests. Proceding with it.\")\n",
    "            return filtered_df, PrettyCombination\n",
    "        else:\n",
    "            log_message(f\"      {PrettyCombination} failed the Overall_Decrease test. Trying next combination.\")\n",
    "            continue\n",
    "    return None, None\n",
    "\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def RemoveOutcast_FixUncertainty(df_filtered, PrettyCombination, filename, AcceptableMultiplier=2.0):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. df_filtered (pandas object): It is something like this:\n",
    "                  1    2    3  PolarizationD3 ErrPolarizationD3 DeltaTime\n",
    "            0   3.0  3.0  3.0          0.5383            0.0021         0\n",
    "            1   3.0  3.0  3.0          0.5379            0.0021       147\n",
    "            2   3.0  3.0  3.0          0.5315            0.0022      3919\n",
    "            ...\n",
    "        2. PrettyCombination (object): A set of three integers. For example (3,3,3)\n",
    "        3. filename (str): The name used to save the data. For example: f\"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}\"\n",
    "        4. AcceptableMultiplier\n",
    "\n",
    "    Returns:\n",
    "        1. A filtered df object like this one:\n",
    "                  1    2    3  PolarizationD3 ErrPolarizationD3 DeltaTime\n",
    "            0   3.0  3.0  3.0          0.5383            0.0021         0\n",
    "            1   3.0  3.0  3.0          0.5379            0.0021       147\n",
    "            2   3.0  3.0  3.0          0.5315            0.0022      3919\n",
    "            ...\n",
    "        2. An object (int,int,int) with the adequate Miller index combination.\n",
    "        \n",
    "    Notes:    \n",
    "        The function does three different jobs.\n",
    "            1. It removes all points that are outliers.\n",
    "                First, it computes the best linear fit to the data P=m·t+n\n",
    "                Second, it finds the smallest value of N such that 75% of the points are in the region -N + n + m·t < P < m·t + n + N\n",
    "                Third, it erases all points outside this range: -N·AcceptableMultiplier + n + m·t < P < m·t + n + N·AcceptableMultiplier\n",
    "            2. It re-scales the uncertainties of the rest of the points\n",
    "                The reasoning behind this rescaling is because if we assume that the decay is a smooth curve, then, clearly, the uncertainties are underestimated\n",
    "            3. Plots the succesful polarization decay\n",
    "                Two plots are saved, one with the original data and another wit the filtered data (both with the corrected uncertainty)    \n",
    "    \"\"\"\n",
    "  \n",
    "    output_folder = Path.cwd() / \"CrystallinePlotResults\"\n",
    "    output_folder.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "\n",
    "    x = df_filtered['DeltaTime'].values\n",
    "    y_hard = df_filtered['PolarizationD3'].values\n",
    "    y_soft = df_filtered['SoftPolarizationD3'].values\n",
    "\n",
    "    #Linear fit\n",
    "    def linear_func(x, m, n):\n",
    "        return m * x + n\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(linear_func, x, y_hard)\n",
    "        m, n = popt\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error fitting data: {e}\")\n",
    "        return df_filtered  # Return original if error\n",
    "\n",
    "    #Find smallest N for 75% within band\n",
    "    num_points = len(x)\n",
    "    sorted_idx = np.argsort(x)\n",
    "    x_sorted = x[sorted_idx]\n",
    "    y_sorted = y_hard[sorted_idx]\n",
    "\n",
    "    N_start = 0.0001\n",
    "    N_step = 0.0001\n",
    "    N_max = 0.4\n",
    "    N = N_start\n",
    "    needed_N = None\n",
    "\n",
    "    while N <= N_max:\n",
    "        y_fit = linear_func(x_sorted, m, n)\n",
    "        upper = y_fit + N\n",
    "        lower = y_fit - N\n",
    "        inside = np.logical_and(y_sorted <= upper, y_sorted >= lower)\n",
    "        percent_inside = np.sum(inside) / num_points * 100\n",
    "\n",
    "        if percent_inside >= 75:\n",
    "            needed_N = N\n",
    "            break\n",
    "        N += N_step\n",
    "\n",
    "    if needed_N is None:\n",
    "        log_message(f\"[{filename}] No N found to contain 75% within ±{N_max}\")\n",
    "        return df_filtered  # Return original if no good N\n",
    "\n",
    "    # Compute extended band and filter out the points outside the extended band\n",
    "    y_fit_full = linear_func(x, m, n)\n",
    "    upper_band = y_fit_full + needed_N * AcceptableMultiplier\n",
    "    lower_band = y_fit_full - needed_N * AcceptableMultiplier\n",
    "\n",
    "    mask = np.logical_and(y_hard <= upper_band, y_hard >= lower_band)\n",
    "    df_cleaned = df_filtered[mask].copy()\n",
    "\n",
    "    log_message(f\"[{filename}] Filtering kept {np.sum(mask)} of {len(mask)} rows (±{needed_N * AcceptableMultiplier:.2e})\")\n",
    "\n",
    "    \n",
    "    # Rescale uncertainties using reduced chi-squared \n",
    "\n",
    "    sigma = df_filtered['ErrPolarizationD3'].values\n",
    "    \n",
    "    # Fit using curve_fit with uncertainties\n",
    "    popt, pcov = curve_fit(linear_func, x, y_hard, sigma=sigma, absolute_sigma=True)\n",
    "    \n",
    "    # Extract best-fit parameters and their uncertainties\n",
    "    m_fit, n_fit = popt\n",
    "    m_err, n_err = np.sqrt(np.diag(pcov))\n",
    "    \n",
    "    # Recalculate the reduced chi-squared \n",
    "    residuals = (y_hard - linear_func(x, *popt)) / sigma\n",
    "    dof = len(x) - len(popt)\n",
    "    chi_squared_red = np.sum(residuals**2) / dof #Maybe the reduced chi-squaed is better\n",
    "    correction_factor = np.sqrt(chi_squared_red)\n",
    "    \n",
    "    # Automatically apply correction if needed\n",
    "    if correction_factor > 1:\n",
    "        df_cleaned['ErrPolarizationD3'] *= correction_factor\n",
    "        log_message(f\"[{filename}] Applied uncertainty correction factor: √(χ²) = {correction_factor:.2f}\")\n",
    "    else:\n",
    "        log_message(f\"[{filename}] No correction applied: √(χ²) = {correction_factor:.2f}\")\n",
    "    \n",
    "    log_message(f\"[{filename}] Fit results: m = {m_fit:.4e} ± {m_err:.4e}, n = {n_fit:.4e} ± {n_err:.4e}\")\n",
    "\n",
    "    # Extract values\n",
    "    T = df_filtered['DeltaTime'].values\n",
    "    P_soft = df_filtered['SoftPolarizationD3'].values\n",
    "    P_hard = df_filtered['PolarizationD3'].values\n",
    "    Err = df_filtered['ErrPolarizationD3'].values if 'ErrPolarizationD3' in df_filtered.columns else np.zeros_like(P_soft)\n",
    "    \n",
    "    # Clean title + filename\n",
    "    clean = make_clean_name(filename)\n",
    "    save_name = extended_area_plot_filename(filename)  # ends with _ExtendedArea.png\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Black points with error bars\n",
    "    plt.scatter(T, P_hard, s=30, color=\"black\", label=\"PolarizationD3\", marker='o')\n",
    "    if Err is not None:\n",
    "        plt.errorbar(T, P_hard, yerr=Err, fmt='none', ecolor='black', alpha=0.6, capsize=2)\n",
    "    \n",
    "    # Blue linear fit\n",
    "    fit = linear_func(T, m, n)\n",
    "    plt.plot(T, fit, '-', color='blue', label=\"Linear Fit\")\n",
    "    \n",
    "    # Bands: light blue (±N) and translucent green (±N*AcceptableMultiplier)\n",
    "    if needed_N is not None:\n",
    "        upper_narrow = fit + needed_N\n",
    "        lower_narrow = fit - needed_N\n",
    "        upper_wide   = fit + needed_N * AcceptableMultiplier\n",
    "        lower_wide   = fit - needed_N * AcceptableMultiplier\n",
    "    \n",
    "        plt.fill_between(T, lower_narrow, upper_narrow, color='lightblue', alpha=0.35, label=f'Band ±{needed_N:.2e}')\n",
    "        plt.fill_between(T, lower_wide,   upper_wide,   color='green',     alpha=0.18, label=f'Filter Band ±{(needed_N*AcceptableMultiplier):.2e}')\n",
    "    \n",
    "    # Labels\n",
    "    plt.xlabel(\"DeltaTime\")\n",
    "    plt.ylabel(\"PolarizationD3\")\n",
    "    plt.title(f\"{clean}_ExtendedArea\")  # title matches saved name (sans .png)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # --- Y limits that include error bars and bands ---\n",
    "    vals_min = [np.nanmin(P_hard - Err), np.nanmin(P_hard + Err)]\n",
    "    vals_max = [np.nanmax(P_hard - Err), np.nanmax(P_hard + Err)]\n",
    "    if needed_N is not None:\n",
    "        vals_min += [np.nanmin(lower_narrow), np.nanmin(lower_wide)]\n",
    "        vals_max += [np.nanmax(upper_narrow), np.nanmax(upper_wide)]\n",
    "    # Small margins\n",
    "    ymin = np.nanmin(vals_min)\n",
    "    ymax = np.nanmax(vals_max)\n",
    "    pad = 0.02 * (ymax - ymin if np.isfinite(ymax - ymin) and (ymax - ymin) > 0 else 1.0)\n",
    "    plt.ylim(ymin - pad, ymax + pad)\n",
    "    \n",
    "    # Save\n",
    "    plot_path_hard = output_folder / save_name\n",
    "    plot_path_hard.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(win_long_path(plot_path_hard), dpi=300, bbox_inches='tight')\n",
    "    if ShowPlot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d77831-99f6-4f71-b740-680c8da916be",
   "metadata": {},
   "source": [
    "## 4. Clean old files and force certain experiments\n",
    "\n",
    "Some parts of the code might use data from different sessions. It is safer to erase them and create all files from scratch everytime. This is not a big deal because this code file should only be run once unless the data base changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0b510-d3f3-4fa6-9868-f87e3e3e1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_erase = [\n",
    "    \"CrystallineLog_Testing_Creation.txt\",\n",
    "    \"Crystalline_CellID.txt\",\n",
    "    \"CrystallineSeparatedFolder\",\n",
    "    \"CrystallinePlotResults\",\n",
    "    \"CrystallineMLDataBase\",\n",
    "    \"CrystallineFailuresTest\",\n",
    "    \"CrystallineDataBase\",\n",
    "    \"CrystallineBadFiles\"]\n",
    "for item in to_erase:\n",
    "    path = os.path.abspath(item)  # full path\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                log_message(f\"Deleted file: {path}\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                log_message(f\"Deleted folder: {path}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\" Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"Not found (skipped): {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89a1ee-4712-411a-9ffb-17c8a2f986ed",
   "metadata": {},
   "source": [
    "## 5. ZIP Folder Treatment and .fli data extraction\n",
    "\n",
    "The code will take all zipped folders from the folder _D3Files_ and prepare them to get their .fli files extracted.\n",
    "\n",
    "First, it will check if there are duplicate zip folders. To check it it will compare the folder name and the hash sha256. Duplicate folders will be erased. For more information about hash sha256 check for example:\n",
    ">Wikipedia contributors. (2026, January 2). SHA-2. In Wikipedia, The Free Encyclopedia. Retrieved 10:49, January 17, 2026, from https://en.wikipedia.org/w/index.php?title=SHA-2&oldid=1330753870\n",
    "\n",
    "Second, it will copy the contents of the zipped folders and create a new folder with the name of the experiment inside _D3Files_. No more zipped folders are erased and information gets duplicated.\n",
    "\n",
    "Third, it will try to find all .fli files inside all the unzipped folders whether if they come from a zipped file or not. It will then send them to the newly created folder _CrystalineDataBase_. If, for each experiment proposal there are more than one .fli files, they get a numeric suffix ('_1', '_2',...) to distinguish them. Afterwards, all unzipped folders get erased leaving behind only the non-duplicated zipped folders.\n",
    "\n",
    "Note: All unzipped folders in D3Files will be explored, however they will get erased at the end of the pipeline. If you want them to persist for future runs of the code, they should be zipped first. For ILL users, when navigating the ILL Cloud, the easiest way to prepare the zip files is to download the _processed_ folder for each experiment proposal. The code is \"smart\" enough to only process .fli files with polarization information. Therefore, there is no need to manually prepare anything.\n",
    "To be precise, this cell of code will take all the zip files, extract them nd remove duplicates using the file name And the hash sha256. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc618c-c148-406a-abc3-8f06ef00ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_hash(filepath, algo=\"sha256\", block_size=65536):\n",
    "    \"\"\"Compute hash of a file (default SHA256).\"\"\"\n",
    "    h = hashlib.new(algo)\n",
    "    with open(win_long_path(filepath), \"rb\") as f:\n",
    "        for block in iter(lambda: f.read(block_size), b\"\"):\n",
    "            h.update(block)\n",
    "    return h.hexdigest()\n",
    "\n",
    "folder = Path(\"D3Files\")  # Folder where all the raw data folders reside\n",
    "zip_files = [f.name for f in folder.glob(\"*.zip\")]  # List all zip files\n",
    "\n",
    "log_message(f\"Reading ZIP files. Checking for true duplicates by content...\")\n",
    "base_names = set()\n",
    "seen_hashes = {}\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    zip_path = folder / zip_file\n",
    "    name, ext = os.path.splitext(zip_file)\n",
    "\n",
    "    # Compute hash of this file\n",
    "    filehash = file_hash(zip_path)\n",
    "\n",
    "    if filehash in seen_hashes:\n",
    "        log_message(f\"Duplicate confirmed by hash! Removing: {zip_file} (same as {seen_hashes[filehash]})\")\n",
    "        os.remove(win_long_path(zip_path))   # Safe long path\n",
    "    else:\n",
    "        seen_hashes[filehash] = zip_file\n",
    "        base_names.add(name)\n",
    "\n",
    "log_message(f\"\\nAll duplicates (by content) removed. Begin unzipping...\\n\")\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\"\"\" UNZIPPING \"\"\"\n",
    "\n",
    "log_message(f\"Begin unzipping...\\n\")\n",
    "# Refresh zip_files list after removals\n",
    "zip_files = [f.name for f in folder.glob(\"*.zip\")]\n",
    "\n",
    "# Unzip and remove original zip files\n",
    "for zip_file in zip_files:\n",
    "    zip_path = folder / zip_file\n",
    "    if zipfile.is_zipfile(win_long_path(zip_path)):\n",
    "        folder_name = sanitize(zip_file.stem if isinstance(zip_file, Path) else os.path.splitext(zip_file)[0])\n",
    "        extract_dir = folder / folder_name\n",
    "        log_message(f\"Unzipping: {zip_file} -> {extract_dir}\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(win_long_path(zip_path), 'r') as zip_ref:\n",
    "                zip_ref.extractall(win_long_path(extract_dir))\n",
    "        except Exception as e:\n",
    "            log_message(f\"WARNING: Error extracting {zip_file}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"WARNING: Skipping invalid zip file: {zip_file}\")\n",
    "\n",
    "log_message(f\"\\nFinished Unzipping. Experiments stored in individual folders substituting the zip files\\n\")\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\"\"\" .fli FILE EXTRACTION \"\"\"\n",
    "source_folder = folder\n",
    "database_folder = Path(\"CrystallineDataBase\")\n",
    "database_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_message(f\"\\n\\n\\n Scanning all folders for .fli files...\\n \")\n",
    "for item in source_folder.iterdir():\n",
    "    if item.is_dir():\n",
    "        log_message(f\"    Processing folder: {item.name}\")\n",
    "        for root, dirs, files in os.walk(win_long_path(item)):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(\".fli\"):\n",
    "                    src_file = Path(root) / file\n",
    "                    dest_file = database_folder / file\n",
    "\n",
    "                    # Handle duplicate names\n",
    "                    counter = 1\n",
    "                    base_name, ext = os.path.splitext(file)\n",
    "                    while dest_file.exists():\n",
    "                        dest_file = database_folder / f\"{base_name}_{counter}{ext}\"\n",
    "                        counter += 1\n",
    "\n",
    "                    log_message(f\"Copying: {src_file} -> {dest_file}\")\n",
    "                    shutil.copy2(win_long_path(src_file), win_long_path(dest_file))\n",
    "        \n",
    "        # After processing all .fli files, delete the original folder\n",
    "        log_message(f\"Deleting folder: {item}\")\n",
    "        shutil.rmtree(win_long_path(item))\n",
    "\n",
    "log_message(f\"\\nAll .fli files collected, sent from folder {source_folder} to folder {database_folder}.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8584a-9866-4d21-af3c-becf0f266cf0",
   "metadata": {},
   "source": [
    "## 6. Separation of .fli files according to experiments\n",
    "\n",
    "Some fli files have the wrong structure which means that they are not polarization measurements and if they are polarization files they may have used more than one polarization cell. Therefore, we need to remove all the non-polarization sets of data and separate the good fli files depending of the type of polarization cell they used.\n",
    "\n",
    "For evey fli file we will read the contents and try to find the header. This symbolizes the installation of a new polarizer cell. If there are numerical values before the first header, that means that the process of saving the file occured before swapping the cell. Those data rows will be skipped. A correct fli file will have the following structure:\n",
    "\n",
    "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
    "|--|--|--|--|--|--|--|--|--|--|--|--|--|--|\n",
    "| polariser cell info | ge18004 | pressure/init. polar | 2.29 | 0.79 | initial date/time | 17/09/23 | @ | 10:39 |  |  |\n",
    "| 47391 | 4.000 | 0.000 | 1.000 | 18/09/23 | 06:20:44 | 155.03 | +z | +z | 0.8391 | 0.0156 | 11.4270 | 1.2031 | 120.00 |\n",
    "| 37392 | 4.000 | 0.000 | 1.000 | 18/09/23 | 06:26:49 | 155.05 | +x | +x | 0.8255 | 0.0110 | 10.4610 | 0.7211 | 300.00 |\n",
    "|  ...  |       |       |       |          |          |      |        |        |        |        |        | | |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Which corresponds to the following information for the first two rows:\n",
    "1. 'polariser cell info' (str): Log of the installation of the polariser cells\n",
    "2. 'PolariserID' (str): A string with the type of cell used\n",
    "3. 'pressure/init. polar' (str): A string to introduce the $^\\mathrm{3}$He gas pressure and the polarization measured at the creation lab.\n",
    "4. 'PolariserPressure' (float): $^\\mathrm{3}$He gas pressure in some units\n",
    "5. 'InitialLabPolarization' (float): Polarization measured at the creation lab\n",
    "6. 'initial date/time' (str): A string that introduces the day, month and year and the hour and minutes.\n",
    "7. 'Date' (str): A string with the information DD/MM/YY\n",
    "8. '@' (str): A string to separate date and time\n",
    "9. 'time' (str): A string with the information HH:MM\n",
    "\n",
    "And for the rest of the rows:\n",
    "1. 'Measurement number' (int): The number index of the measurement.\n",
    "2. 'First_Miller_Index' (float): The first Miller index of the crystal. Polarization is measured using a known Si Bragg crystal. For the source of the origin of the Si crystal see:\n",
    ">Stunault, Anne & Vial, S & Pusztai, Laszlo & Cuello, Gabriel & Temleitner, László. (2016). Structure of hydrogenous liquids: separation of coherent and incoherent cross sections using polarised neutrons. Journal of Physics: Conference Series. 711. 012003. 10.1088/1742-6596/711/1/012003. \n",
    "3. 'Second_Miller_Index' (float)\n",
    "4. 'Third_Miller_Index' (float):\n",
    "5. 'Date' (str): A string with the information DD/MM/YY of that measurement\n",
    "6. 'time' (str): A string with the information HH:MM:SS of that measurement\n",
    "7. Temperature \n",
    "8. 'Direction_1' (str): It is the direction of the polarization after the monochromator.The direction +z corresponds to the orthogonal with respect to the floor pointing away from it. +x is the direction of the beam (variable) and +y is the orthogonal (positive orthonormal basis) direction to +z and +x. \n",
    "8. 'Direction_2' (str): The direction of the polarization at the sensors. True polarization measurements are done **only** on the (+z,+z) direction.\n",
    "\n",
    "8. 'D3Polarization' (float): A float with the polarization measurement\n",
    "9. 'ErrD3Polarization' (float): A float with the uncertainty of that polarization measurement\n",
    "10. 'FlippingRatio' (float): The flipping ratio. Given either the flipping ratio or the polarization value, the other one is fully determined. Therefore, only one is needed and that is why we don´t work with the flipping ratio\n",
    "11. 'ErrFlippingRatio' (float): The uncertainty of the flipping ratio\n",
    "12. 'Elapsed time' (float): It is the time used to obtain the measurement (integration of the beam over that number of seconds)\n",
    "\n",
    "Temperature did not seem to have an effect on the decay. Therefore, it has been eliminated in this code cell. Here is a summary of what the code does:\n",
    "\n",
    "1. The code will go through the .fli files and find all rows with 'polariser cell info'. A cell change is considered once a new 'polariser cell info'. At the moment it ignores the experiments that use the 'magical box' as we are not sure if they are experiments compatible with the ones studied here.\n",
    "2. For evey cell change, a new .fli file is created storing all the information including the header row and the measured data rows. Also, all cell IDs are recorded\n",
    "3. For all .fli files the code now will:   \n",
    "\n",
    "    3.1 Remove unwanted rows\n",
    "    \n",
    "    3.2 It removes any rows that don´t have polarization directions (+z,+z)\n",
    "    \n",
    "    3.3 Extract data form the header row \n",
    "    \n",
    "    3.4 Remove unwanted columns (temperature, flipping ratio, counts, elapsed time,...)\n",
    "    \n",
    "    3.5 Set a time reference with the first measurement row. All other time values get referenced with respect to this moment in time and converted into seconds.\n",
    "    \n",
    "    3.6 Ignore all Miller index combinations that are not integers.\n",
    "    \n",
    "    3.7 Run through all Miller index combinations until one passes all the filters defined in previous code cells. When one is found, any other combinations are voided as well.\n",
    "    \n",
    "    3.8 Plot the succesful experiments.\n",
    "    \n",
    "    3.9 Save two files for each experiment. One with the header rows and another one with just the numeric rows (with a new header that explains what each column has)\n",
    "\n",
    "For every succesful experiment we will output:\n",
    "1. Image:  **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Multiplier={Multiplier}.png\"** in PlotResults. Shows the plot with the extended area with the raw data\n",
    "2. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Multiplier={Multiplier}\\_Soft.png\"** in PlotResults. Shows the plot with the extended area with the filtered data\n",
    "3. Image: **\"PolarizationD3\\_{folder_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Filtered.txt\\_plot\\_Derivatives.png\"** in PlotResults. Shows the evolution of the \"derivatives\". I apologize for the hideous names. Unless it results in a fatal error, I am scared to change the code.\n",
    "4. Image: **\"PolarizationD3\\_{folder_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex_{PrettyCombination}\\_Filtered.txt\\_N\\_{N}\\_ManualInterval.png\"** in PlotResults. Shows the plot with the non-exteded area\n",
    "5. Txt: **\"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}.txt\"** in MLDataBase. It contains the four data columns (DeltaTime, PolarizationD3, SoftPolarizationD3, ErrPolarizationD3)\n",
    "6. Txt: **\"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\"** in MLDataBase. It contains the parameters (CellID, Pressure, LabPolarization, LabTime)\n",
    "\n",
    "\n",
    "The plots are not necessary but are saved for the user to know what all the files look like. The txt files are fundamental for the rest of the pipeline. \n",
    "The files that are wrong or useless when all is done are the folowing. They are kept for  debug purposes (to see files with differents structures, why they fail,etc).\n",
    "\n",
    "1. Txt: **\"{folder\\_name}\\_Arrays\\_{i}.txt\"** in SeparatedFolder/{folder_name}. It still has the header and useless columns. It is the fli file of evey chunk, of every recorded experiment (correct or incorrect)\n",
    "2. Txt: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}.txt\"** in SeparatedFolder/{folder_name}. It is the same as the one in MLDataBase (a duplicate)\n",
    "3. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}.png\"** in SeparatedFolder/{folder_name}. It plots (with error bars) PolarizationD3\n",
    "4. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Combined.png\"** in SeparatedFolder/{folder_name}. It plots (with error bars) both PolarizationD3 and SoftPolarizationD3\n",
    "5. Image: **\"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Softened.png\"** in SeparatedFolder/{folder_name}. It plots (with error bars) SoftPolarizationD3\n",
    "6. Folder: **\"FailuresTest\"** contains all the graphs of the data sets that were considered not worthy but had more points that the ones saved. Check them if your experiment was not properly added\n",
    "7. Folder: **\"DataBase\"** has the raw fli files. Once the code has been used they are no longer important (if you don't find the folder I may have added a line of code to erase it. Sorry in advance for any inconveniences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdefe238-9d88-4918-8fde-ad944bf26e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the original folder and the final folder\n",
    "DataBase = Path('CrystallineDataBase')\n",
    "output_base = Path('CrystallineSeparatedFolder')\n",
    "\n",
    "# List all .fli files in that folder, prepare folders\n",
    "FileNameList = [f.name for f in DataBase.glob('*.fli')]\n",
    "polyorder = 2\n",
    "default_window_length = 5\n",
    "SeparatedFolder = Path(\"CrystallineSeparatedFolder\")\n",
    "BadFilesFolder = Path(\"CrystallineBadFiles\")\n",
    "MLDataBaseFolder = Path(\"CrystallineMLDataBase\")\n",
    "BadFilesFolder.mkdir(exist_ok=True, parents=True)\n",
    "MLDataBaseFolder.mkdir(exist_ok=True, parents=True)\n",
    "log_message(f\"\\n\\n Files in the data base that will be (tried) to be used\\n {FileNameList}\\n\")\n",
    "\n",
    "for FileName in FileNameList:\n",
    "    \"\"\"READ THE FILE AND SEPRATE IT INTO EACH EXPERIMENT USING THE POLARIZATION CELL\"\"\"\n",
    "    #1.1- Open file\n",
    "    folder_name = FileName.replace(\".fli\", \"\")\n",
    "    output_folder = output_base / folder_name\n",
    "    file_path = DataBase / FileName\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(win_long_path(file_path), \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    \n",
    "    #1.2- Locate the header with CellID, Pressure, etc. Chunks are the data rows sandwiched between two 'polariser cell info' strings\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    started = False  # flag to know when we found first header\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"polariser cell info\"):  # All before polariser cell info will be forgotten\n",
    "            if started and current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = [line]\n",
    "            started = True\n",
    "        else:\n",
    "            if started:\n",
    "                current_chunk.append(line)\n",
    "            # else: we are before the first header, so ignore these lines\n",
    "\n",
    "    if not started:\n",
    "        log_message(f\" File '{FileName}' does NOT contain any 'polariser cell info' header. Skipping.\\n\")\n",
    "        continue\n",
    "    else:\n",
    "        log_message(f\" File '{FileName}' contains at least one 'polariser cell info' header.\")\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    #1.3- Save .fli files for every correct chunk\n",
    "    base_name = FileName.replace(\".fli\", \"\")  # remove .fli for clean filenames\n",
    "    log_message(f\" Creating all the Array files \\n\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        fli_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "        fli_path = output_folder / fli_filename  # full path\n",
    "        with open(win_long_path(fli_path), \"w\", encoding='utf-8') as f_out:\n",
    "            f_out.writelines(chunk)\n",
    "    # 1.4- As CellID can be exchanged with real parameters, it is written in an independent file\n",
    "    cell_id_file = Path.cwd() / \"Crystalline_CellID.txt\"\n",
    "    try:\n",
    "        with open(win_long_path(cell_id_file), 'r', encoding='utf-8') as file:\n",
    "            seen_strings = set(line.strip() for line in file)\n",
    "    except FileNotFoundError:\n",
    "        seen_strings = set()\n",
    "    \n",
    "    # 1.5- Open each Array file and work with it (The Array file still has the header)\n",
    "    with open(win_long_path(cell_id_file), 'a', encoding='utf-8') as file:\n",
    "        for i in range(len(chunks)):\n",
    "            FLI_filename = f\"{base_name}_Arrays_{i}.fli\"  # Name of the Array file\n",
    "            FLI_path = output_folder / FLI_filename  # Full path\n",
    "            if not FLI_path.exists():\n",
    "                log_message(f\"   WARNING: Array file does not exist: {FLI_path}\")\n",
    "                continue\n",
    "            df = pd.read_csv(win_long_path(FLI_path), sep=r'\\s+', header=None, on_bad_lines='skip')  # Read file\n",
    "            log_message(f\" Reading {FLI_path}, removing ***WARNING No centering scan found \")\n",
    "            warning_str = \"***WARNING No centering scan found\"\n",
    "\n",
    "            #5.1 Combine first 4 columns as strings, join them with space, and filter rows containing this phrase (it is not important for us)\n",
    "            df = df[~df.iloc[:, :5].astype(str).agg(' '.join, axis=1).str.contains('No centering scan found', regex=False)] \n",
    "            \n",
    "            #5.2 Extract useful information from the header. Hopefully, CellID, Pressure, LabPolarization, Year, Month, Day, time of lab measurement before first experiment measurement (negative time) will be stored locally\n",
    "            log_message(f\" Header Information Extraction...\")\n",
    "            CellID =          df.iloc[0].tolist()[3]\n",
    "            Pressure =        df.iloc[0].tolist()[6]\n",
    "            LabPolarization = df.iloc[0].tolist()[7]\n",
    "\n",
    "            try:\n",
    "                HM, DD, MM, YY = df.iloc[0].tolist()[14], int(df.iloc[0].tolist()[10]), int(df.iloc[0].tolist()[11]), int(df.iloc[0].tolist()[12])\n",
    "                Day_Ref = f\"{DD:02d}/{MM:02d}/{YY:02d}\"\n",
    "                dt = Time(Day_Ref, HM)\n",
    "            except Exception as e:\n",
    "                log_message(f\"    Skipping file {file_path} because of invalid header data: {e}\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            #5.3 All redundant/useless information is removed\n",
    "            log_message(f\" Removing Measurement Index, Temperature, Flipping Ratio, Uncertainty of Flipping Ratio and Time between measurements,...\")\n",
    "            df = df.iloc[1:].reset_index(drop=True)\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "            df = df.drop(df.columns[5], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            df = df.drop(df.columns[9], axis=1)\n",
    "            log_message(f\" Saving only polarization values for the Spin Directions wanted in both Polarizer Cells, i.e. (+z,+z)\")\n",
    "\n",
    "            #5.4 Keep only rows where both are +z\n",
    "            df = df[(df[7] == '+z') & (df[8] == '+z')].copy()\n",
    "            if df.empty:\n",
    "                log_message(f\"   No valid '+z' rows in file {FileName}_Arrays_{i}.fli, skipping\")\n",
    "                continue  # skip to next file in your loop\n",
    "            df = df.drop(df.columns[[5,6]], axis=1, errors='ignore')\n",
    "            #5.5 Convert Miller index columns into integers. From string or object to float and if the float is close to an integer (tolerance is 1e-8) then save as integer. Otherwise remove row\n",
    "            cols_to_convert = [1, 2, 3]\n",
    "            df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce').astype(float)            \n",
    "            mask = np.isclose(df[cols_to_convert], np.round(df[cols_to_convert]), atol=1e-8)\n",
    "            df = df[mask.all(axis=1)].copy()\n",
    "            log_message(f\" All Spin directions removed. All irrational Miller Indices removed. Adding DeltaTime\")\n",
    "            \n",
    "            #5.5 The time columns are converted into difference of time being the referenced time the first +z,+z measurement that has survived at this point\n",
    "            if df.shape[0] < 2:\n",
    "                log_message(f\"   Not enough valid rows after filtering, skipping chunk\")\n",
    "                continue\n",
    "            df['DeltaTime'] = df.apply(\n",
    "                lambda row: deltatime(df[4].iloc[0], df[5].iloc[0], row[4], row[5]), axis=1 )\n",
    "            ref_dt = Time(df[4].iloc[0], df[5].iloc[0])\n",
    "            LabTime = int((dt - ref_dt).total_seconds())\n",
    "\n",
    "            #5.6 Rename the columns PolarizationD3, ErrPolarizationD3 (the polarization column and its uncertainty). The other one with name is DeltaTime. The rest are numbers (will be erased).\n",
    "            #Also we remove the time strings (with DeltaTime they have no new information)\n",
    "            log_message(f\"   Renaming PolarizationD3 and ErrPolarizationD3\")\n",
    "            df.rename(columns={\n",
    "                df.columns[5]: 'PolarizationD3',\n",
    "                df.columns[6]: 'ErrPolarizationD3'\n",
    "            }, inplace=True)\n",
    "            df.drop(columns=[df.columns[3], df.columns[4]], inplace=True)\n",
    "            log_message(f\"   Dropped Time Strings\")\n",
    "\n",
    "            \n",
    "            #5.7 Begin filtering and softening with previous functions\n",
    "            log_message(f\"   Begin removal of Bad files and softening with Savitzky-Golay filter\")\n",
    "            filtered_df, PrettyCombination = filter_best_combination(i, df)\n",
    "            if filtered_df is None and PrettyCombination is None:\n",
    "                log_message(f\"    Chunk {i}: No suitable combination found. Skipping to next chunk or file.\")\n",
    "                log_message(f\"_______________________________________________________________\\n\")\n",
    "                continue  # skip to next chunk\n",
    "            \n",
    "            #5.8 Removal of Miller indices (we have all the information they could give us)\n",
    "            log_message(f\" Removing Miller Indices columns\")\n",
    "            #log_message(filtered_df)\n",
    "            filtered_df = filtered_df.iloc[:, 3:]\n",
    "            desired_order = [\"DeltaTime\", \"PolarizationD3\", \"SoftPolarizationD3\", \"ErrPolarizationD3\"]\n",
    "\n",
    "            \n",
    "            # 5.9 Remove the points that won't be useful for the ML algorithm\n",
    "            columns_to_save = [col for col in desired_order if col in filtered_df.columns]  # Keep only the columns that exist\n",
    "            df_SEMIFINAL = filtered_df[columns_to_save].copy()\n",
    "            df_FINAL = filtered_df = RemoveOutcast_FixUncertainty(\n",
    "                df_SEMIFINAL,\n",
    "                PrettyCombination,\n",
    "                filename=f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}\",\n",
    "                AcceptableMultiplier=1.3)\n",
    "            \n",
    "            # 5.10 Plot the successful experiments\n",
    "            log_message(f\" Plot of Data. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            T = pd.to_numeric(df_FINAL[\"DeltaTime\"], errors='coerce')\n",
    "            P = pd.to_numeric(df_FINAL[\"PolarizationD3\"], errors='coerce')\n",
    "            Err = pd.to_numeric(df_FINAL[\"ErrPolarizationD3\"], errors='coerce')\n",
    "            P_soft = pd.to_numeric(df_FINAL[\"SoftPolarizationD3\"], errors='coerce')\n",
    "            \n",
    "            # Scatter plot\n",
    "            plt.scatter(T, P, linewidth=1, label='Original') \n",
    "            plt.plot(T, P, linestyle='--', color='blue', alpha=0.7)\n",
    "            plt.errorbar(T, P, yerr=Err, fmt='none', ecolor='gray', alpha=0.5)\n",
    "            \n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"PolarizationD3\")\n",
    "            plot_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}.png\"\n",
    "            plt.title(plot_filename)\n",
    "            plt.ylim(np.min(P - Err), np.max(P + Err))\n",
    "            plt.yticks(np.linspace(np.min(P - Err), np.max(P + Err), 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path = win_long_path(output_folder / plot_filename)\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            if ShowPlot:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            # Softened data plot\n",
    "            log_message(f\" Plot of Filtered Data. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Softened\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.scatter(T, P_soft, linewidth=1, label='Filtered')\n",
    "            plt.plot(T, P_soft, linestyle='--', color='green', alpha=0.7)\n",
    "            plt.errorbar(T, P_soft, yerr=Err, fmt='none', ecolor='gray', alpha=0.5)\n",
    "            \n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"SoftPolarizationD3\")\n",
    "            plot_filename_soft = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Softened.png\"\n",
    "            plt.title(plot_filename_soft)\n",
    "            plt.ylim(np.min(P_soft - Err), np.max(P_soft + Err))\n",
    "            plt.yticks(np.linspace(np.min(P_soft - Err), np.max(P_soft + Err), 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path_soft = win_long_path(output_folder / plot_filename_soft)\n",
    "            plt.savefig(plot_path_soft, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            if ShowPlot:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            # Comparison plot\n",
    "            log_message(f\" Comparison Plot. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Comparison\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.scatter(T, P, linewidth=1, color='blue', alpha=0.6, label='Original')\n",
    "            plt.plot(T, P, linestyle='--', color='blue', alpha=0.5)\n",
    "            plt.scatter(T, P_soft, linewidth=1, color='green', alpha=0.6, label='Filtered')\n",
    "            plt.plot(T, P_soft, linestyle='--', color='green', alpha=0.5)\n",
    "            plt.errorbar(T, P, yerr=Err, fmt='none', ecolor='gray', alpha=0.3)\n",
    "            plt.errorbar(T, P_soft, yerr=Err, fmt='none', ecolor='gray', alpha=0.3)\n",
    "            \n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"Polarization\")\n",
    "            plot_filename_combined = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Combined.png\"\n",
    "            plt.title(plot_filename_combined)\n",
    "            min_y = min(np.min(P - Err), np.min(P_soft - Err))\n",
    "            max_y = max(np.max(P + Err), np.max(P_soft + Err))\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(min_y, max_y)\n",
    "            plt.yticks(np.linspace(min_y, max_y, 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path_combined = win_long_path(output_folder / plot_filename_combined)\n",
    "            plt.savefig(plot_path_combined, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            if ShowPlot:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            # 5.11 Save the files\n",
    "            log_message(f\" Finally we save the chunk\")\n",
    "            csv_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}.txt\"\n",
    "            Parameter_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\"\n",
    "            csv_path = output_folder / csv_filename\n",
    "            \n",
    "            # Save CSV\n",
    "            df_FINAL['DeltaTime'] = df_FINAL['DeltaTime'] - df_FINAL['DeltaTime'].iloc[0]\n",
    "\n",
    "            df_FINAL.to_csv(win_long_path(csv_path), index=False, sep=',')\n",
    "            \n",
    "            # Save a copy to ML database\n",
    "            ml_txt_path = MLDataBaseFolder / csv_filename\n",
    "            df_FINAL.to_csv(win_long_path(ml_txt_path), index=False, sep=',')\n",
    "            log_message(f\"Saved: {csv_filename}\")\n",
    "            \n",
    "            # Parameter file\n",
    "            ml_param_path = MLDataBaseFolder / Parameter_filename\n",
    "            with open(win_long_path(ml_param_path), 'w', encoding='utf-8') as f:\n",
    "                f.write(\"CellID,Pressure,LabPolarization,LabTime\\n\")\n",
    "                f.write(f\"{CellID},{Pressure},{LabPolarization},{LabTime}\")\n",
    "            log_message(f\" Saved: {Parameter_filename}\")\n",
    "            log_message(f\" Parameter and Array files saved to ML database: {MLDataBaseFolder}\\n_______________________________________________________________\\n\\n\")\n",
    "            \n",
    "            # 5.12 Remove unwanted array files\n",
    "            for i in range(len(chunks)):\n",
    "                temp_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "                temp_path = output_folder / temp_filename\n",
    "                try:\n",
    "                    temp_path.unlink()  # delete the file\n",
    "                except FileNotFoundError:\n",
    "                    pass  # skip if missing\n",
    "            \n",
    "            log_message(f\" Created and saved {len(chunks)} CSV files from file called {FileName}.\")\n",
    "            \n",
    "            # Remove empty folder\n",
    "            if output_folder.exists() and not any(output_folder.iterdir()):\n",
    "                output_folder.rmdir()\n",
    "                log_message(f\"Removed empty folder: {output_folder}\")\n",
    "            \n",
    "            log_message('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff6b81-629d-4e65-9d02-f245a770a0d7",
   "metadata": {},
   "source": [
    "## 7. CellID File processing\n",
    "\n",
    "We need to save the CellID types without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186bd677-be38-4a52-a82f-bc7862e3bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_database_folder = Path(\"CrystallineMLDataBase\")\n",
    "\n",
    "# Find all txt files whose names end with Parameters.txt (case insensitive)\n",
    "parameter_files = list(ml_database_folder.glob('*Parameters.txt'))\n",
    "\n",
    "log_message(f\"Found {len(parameter_files)} parameter files.\")\n",
    "\n",
    "unique_cell_ids = []\n",
    "seen = set()\n",
    "\n",
    "for filepath in parameter_files:\n",
    "    try:\n",
    "        with open(win_long_path(filepath), 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 2:\n",
    "                second_row = lines[1].strip()\n",
    "                parts = second_row.split(',')\n",
    "                if parts:\n",
    "                    cell_id = parts[0]\n",
    "                    if cell_id not in seen:\n",
    "                        seen.add(cell_id)\n",
    "                        unique_cell_ids.append(cell_id)\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to read {filepath}: {e}\")\n",
    "\n",
    "# Write to Crystalline_CellID.txt\n",
    "cellid_file = Path.cwd() / \"Crystalline_CellID.txt\"\n",
    "with open(win_long_path(cellid_file), \"w\", encoding='utf-8') as f:\n",
    "    for cell_id in unique_cell_ids:\n",
    "        f.write(f\"{cell_id}\\n\")\n",
    "\n",
    "log_message(f\"Saved {len(unique_cell_ids)} unique cell IDs to {cellid_file.name}.\")\n",
    "\n",
    "# Remove the separated folder\n",
    "folder_to_delete = Path.cwd() / \"CrystallineSeparatedFolder\"\n",
    "if folder_to_delete.exists():\n",
    "    shutil.rmtree(win_long_path(folder_to_delete))\n",
    "    log_message(f\"Folder '{folder_to_delete}' has been deleted.\")\n",
    "else:\n",
    "    log_message(f\"Folder '{folder_to_delete}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d336e1-69c6-4b03-8016-2dc2a52416e8",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "It erases all intermediate files and prepares the remaining ones for the ML pipeline\n",
    "\n",
    "1. Removes all .fli files that have been created.\n",
    "2. Removes empty folders\n",
    "3. Collects all unique polariser–analyser ID pairs\n",
    " \n",
    "As a result, the only useful files are _Crystalline_CellID.txt_ and the folder _CrystallineMLDataBase_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e57e9fe-6d24-4ba8-ab0d-3d73119eece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_map = defaultdict(list)\n",
    "\n",
    "def file_sha256(filepath, block_size=65536):\n",
    "    \"\"\"Compute SHA256 hash of a file (safe for large files).\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(win_long_path(filepath), \"rb\") as f:\n",
    "        while chunk := f.read(block_size):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "# Scan all .txt files (only base files without '_Parameters')\n",
    "for root, _, files in os.walk(win_long_path(ml_database_folder)):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".txt\") and \"_parameters\" not in file.lower():\n",
    "            path = Path(root) / file\n",
    "            file_hash = file_sha256(path)\n",
    "            hash_map[file_hash].append(path)\n",
    "\n",
    "# Report & delete duplicates\n",
    "duplicates_found = False\n",
    "for file_hash, paths in hash_map.items():\n",
    "    if len(paths) > 1:\n",
    "        duplicates_found = True\n",
    "        log_message(f\"\\nDuplicate group (hash={file_hash}):\")\n",
    "        log_message(f\"   Keeping: {paths[0]}\")\n",
    "\n",
    "        # All but the first are duplicates\n",
    "        for p in paths[1:]:\n",
    "            base_name, ext = os.path.splitext(p)\n",
    "            param_file = Path(f\"{base_name}_Parameters{ext}\")\n",
    "\n",
    "            try:\n",
    "                os.remove(win_long_path(p))\n",
    "                log_message(f\"   Deleted duplicate base file: {p}\")\n",
    "            except Exception as e:\n",
    "                log_message(f\"   Could not delete base file {p}: {e}\")\n",
    "\n",
    "            # Also try deleting the corresponding parameter file\n",
    "            if param_file.exists():\n",
    "                try:\n",
    "                    os.remove(win_long_path(param_file))\n",
    "                    log_message(f\"   Deleted parameter file: {param_file}\")\n",
    "                except Exception as e:\n",
    "                    log_message(f\"   Could not delete parameter file {param_file}: {e}\")\n",
    "\n",
    "if not duplicates_found:\n",
    "    log_message(\"No duplicates found in MLDataBase!\")\n",
    "else:\n",
    "    log_message(\"\\n Duplicate cleanup complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
