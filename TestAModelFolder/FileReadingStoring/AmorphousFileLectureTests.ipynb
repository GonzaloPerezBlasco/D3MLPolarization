{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9710df03-5c41-41e3-9542-2fa76742c47d",
   "metadata": {},
   "source": [
    "<h1>AmorphousFileLectureCreate.ipynb</h1> \n",
    "\n",
    "Reads from D3Files all the files it is going to process\n",
    "\n",
    "Outputs the following folders and files:\n",
    "\n",
    "1. **AmorphousLog\\_Reading\\_Creation.txt**\n",
    "Logs all the prints and every step the code does. If you trust the code, it is irrelevant. If you don't trust it or want to change it then this txt file will tell you how each experiment file has been processed and where there might have been issues.\n",
    "\n",
    "\n",
    "2. **Amorphous\\_CellID**\n",
    "Contains the Cell IDs found on all the files. Needed for the ML code.\n",
    "\n",
    "\n",
    "3. **AmorphousPlotResults**\n",
    "This folder will store the graphs of all the experiments that were accepted. Not needed for anything but it is nice to see the files that will be fed to the model. For each experiment you can find the following files:\n",
    "\n",
    "    3.1 **{base\\_name}\\_ExtendedArea.png**\n",
    "It shows the same pot and also the range y_min=mx+n-1.3*N<y<mx+n+1.3*N<y_max. The points outside the green area will be discarded as they are considered to be too off to be considered correct.\n",
    "\n",
    "\n",
    "4. **AmorphousMLDataBase**\n",
    "Contains the .txt files NECESSARY for the ML algorithm. There are two per experiment\n",
    "\n",
    "    4.1 **{base\\_name}.txt** \n",
    "Contains DeltaTime (the time of the measurement measured from the first VALID polarization measurement), PolarizationD3, SoftPolarizationD3 (the polarization after using a Savitzky-Golay filter) and ErrPolarizationD3 (the uncertainty)\n",
    "\n",
    "    4.2 **{base\\_name}\\_Parameters.txt**\n",
    "Contains the CellID, Pressure, LabPolarization (the polarization measured at the lab) and LabTimeCellID (the time when it was measured)\n",
    "\n",
    "5. **AmorphousDataBase**\n",
    "Contains all the .fli files that were attempted to be read\n",
    "\n",
    "\n",
    "6. **AmorphousBadFiles**\n",
    "Contains all the .fli separated in experiment sets folders that were rejected (not enough points, negative polarizations, etc.)\n",
    "\n",
    "\n",
    "_________________________________________________________________________________________\n",
    "\n",
    "## Explanations\n",
    "\n",
    "Some parts of the code might use data from different sessions. It is safer to erase them and create all files from scratch everytime. This is not a big deal because this code file should only be run once unless the data base changes.\n",
    "\n",
    "Some experiments did not pass the filtering methods of the previous functions despite looking very promising. Also, some experiments were not adequate yet they passed all of the filtering process. That is why we will store the names of those files manually.\n",
    "The code will take all zipped folders from the folder _D3Files_ and prepare them to get their .fli files extracted.\n",
    "\n",
    "First, it will check if there are duplicate zip folders. To check it it will compare the folder name and the hash sha256. Duplicate folders will be erased. For more information about hash sha256 check for example:\n",
    ">Wikipedia contributors. (2026, January 2). SHA-2. In Wikipedia, The Free Encyclopedia. Retrieved 10:49, January 17, 2026, from https://en.wikipedia.org/w/index.php?title=SHA-2&oldid=1330753870\n",
    "\n",
    "Second, it will copy the contents of the zipped folders and create a new folder with the name of the experiment inside _D3Files_. No more zipped folders are erased and information gets duplicated.\n",
    "\n",
    "Third, it will try to find all .fli files inside all the unzipped folders whether if they come from a zipped file or not. It will then send them to the newly created folder _AmorphousDataBase_. If, for each experiment proposal there are more than one .fli files, they get a numeric suffix (\\_1, \\_2,...) to distinguish them. Afterwards, all unzipped folders get erased leaving behind only the non-duplicated zipped folders.\n",
    "\n",
    "Note: All unzipped folders in D3Files will be explored, however they will get erased at the end of the pipeline. If you want them to persist for future runs of the code, they should be zipped first. **For ILL users, when navigating the ILL Cloud, the easiest way to prepare the zip files is to download the _processed_ folder for each experiment proposal. The code is \"smart\" enough to only process .fli files with polarization information. Therefore, there is no need to manually prepare anything.**\n",
    "\n",
    "\n",
    "Some fli files have the wrong structure which means that they are not polarization measurements and if they are polarization files they may have used more than one polarization cell. Therefore, we need to remove all the non-polarization sets of data and separate the good fli files depending of the type of polarization cell they used.\n",
    "\n",
    "For evey fli file we will read the contents and try to find the header (two strings in two consecutive lines). This symbolizes the installation of a new polarizer cell. If there are numerical values before the first header, that means that the process of saving the file occured before swapping the cell. Those data rows will be skipped. A correct fli file will have the following structure:\n",
    "\n",
    "|  |  |  |  |  |  |  |  |  |  |  |  |\n",
    "|--|--|--|--|--|--|--|--|--|--|--|--|\n",
    "| polariser cell info | ge18004 | pressure/init. polar | 2.30 | 0.79 | initial date/time | 21/11/23 | @ | 12:45 |\n",
    "| analyser cell info  | sic1402 | pressure/init. polar | 2.00 | 0.79 | initial date/time | 21/11/23 | @ | 12:45 |\n",
    "| 40661 | 3.000 | 3.000 | 3.000 | 21/11/23 | 12:50:35 | 0.00 | 0.7890 | 0.0020 | 8.4795 | 0.0897 | 120.00 |\n",
    "| 40662 | 3.000 | 3.000 | 3.000 | 21/11/23 | 12:54:43 | 0.00 | 0.7851 | 0.0020 | 8.3048 | 0.0867 | 120.00 |\n",
    "|  ...  |       |       |       |          |          |      |        |        |        |        |        |\n",
    "\n",
    "Which corresponds to the following information for the first two rows:\n",
    "1. 'polariser cell info'/'analyser cell info' (str): Log of the installation of the first and second polariser cells\n",
    "2. 'PolariserID' (str): A string with the type of cell used\n",
    "3. 'pressure/init. polar' (str): A string to introduce the $^\\mathrm{3}$He gas pressure and the polarization measured at the creation lab.\n",
    "4. 'PolariserPressure' (float): $^\\mathrm{3}$He gas pressure in some units\n",
    "5. 'InitialLabPolarization' (float): Polarization measured at the creation lab\n",
    "6. 'initial date/time' (str): A string that introduces the day, month and year and the hour and minutes.\n",
    "7. 'Date' (str): A string with the information DD/MM/YY\n",
    "8. '@' (str): A string to separate date and time\n",
    "9. 'time' (str): A string with the information HH:MM\n",
    "\n",
    "And for the rest of the rows:\n",
    "1. 'Measurement number' (int): The number index of the measurement.\n",
    "2. 'First\\_Miller\\_Index' (float): The first Miller index of the crystal. Polarization is measured using a known Si Bragg crystal. For the source of the origin of the Si crystal see:\n",
    ">Stunault, Anne & Vial, S & Pusztai, Laszlo & Cuello, Gabriel & Temleitner, László. (2016). Structure of hydrogenous liquids: separation of coherent and incoherent cross sections using polarised neutrons. Journal of Physics: Conference Series. 711. 012003. 10.1088/1742-6596/711/1/012003. \n",
    "3. 'Second\\_Miller\\_Index' (float)\n",
    "4. 'Third\\_Miller\\_Index' (float):\n",
    "5. 'Date' (str): A string with the information DD/MM/YY of that measurement\n",
    "6. 'time' (str): A string with the information HH:MM:SS of that measurement\n",
    "7. Unknown float, maybe Temperature\n",
    "8. 'D3Polarization' (float): A float with the polarization measurement\n",
    "9. 'ErrD3Polarization' (float): A float with the uncertainty of that polarization measurement\n",
    "10. 'FlippingRatio' (float): The flipping ratio. Given either the flipping ratio or the polarization value, the other one is fully determined. Therefore, only one is needed and that is why we don´t work with the flipping ratio\n",
    "11. 'ErrFlippingRatio' (float): The uncertainty of the flipping ratio\n",
    "12. 'Elapsed time' (float): It is the time used to obtain the measurement (integration of the beam over that number of seconds)\n",
    "\n",
    "Temperature did not seem to have an effect on the decay. Therefore, it has been eliminated in this code cell. Here is a summary of what the code does:\n",
    "\n",
    "1. The code will go through the .fli files and find all combinations of consecutive rows with 'polariser cell info' and 'analyser cell info'. It doesn´t care about the order which makes the code more robust. We will consider that a polariser cell has been properly installed whrn both of these rows are present and that the cell has been changed once a new set of polariser and analyser rows are encountered. At the moment it ignores the experiments that use the 'magical box' as we are not sure if they are experiments compatible with the ones studied here\n",
    "2. For evey cell change, a new .fli file is created storing all the information including both polariser and analyser rows and the measured data rows. Also, all cell IDs are recorded\n",
    "3. For all .fli files the code now will:\n",
    "    \n",
    "- Remove unwanted rows\n",
    "- Extract data form the header rows (polariser row and analyser row)\n",
    "- Remove unwanted columns\n",
    "- Set a time reference with the first measurement row. All other time values get referenced with respect to this moment in time and converted into seconds.\n",
    "- Ignore all Miller index combinations that are not integers\n",
    "- Run through all Miller index combinations until one passes all the filters defined in previous code cells\n",
    "- Plot the succesful experiments\n",
    "- Save two files for each experiment. One with the header rows and another one with just the numeric rows (with a new header that explains what each column has)\n",
    "\n",
    "For every succesful experiment we will output:\n",
    "1. Image:  \"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_ExtendedArea.png\" in AmorphousPlotResults. Shows the plot with the extended area with the raw data\n",
    "2. Txt:    \"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}.txt\" in AmorphousMLDataBase. It contains the four data columns (DeltaTime, PolarizationD3, SoftPolarizationD3, ErrPolarizationD3)\n",
    "3. Txt:    \"PolarizationD3\\_{folder\\_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Parameters.txt\" in AmorphousMLDataBase. It contains the parameters (CellID, Pressure, LabPolarization, LabTime)\n",
    "\n",
    "These plots are not necessary but are saved for the user to know what all the files look like.\n",
    "The files that are wrong or useless when all is done are the folowing:\n",
    "1. Txt:    \"{folder\\_name}\\_Arrays\\_{i}.txt\" in SeparatedFolder/{folder\\_name}. It still has the header and useless columns. It is the fli file of evey chunk, of every recorded experiment (correct or incorrect)\n",
    "2. Folder: \"BadTest\" contains all the graphs of the data sets that were considered not worthy but had more points that the ones saved. Check them if your experiment was not properly added\n",
    "\n",
    "Finally, it erases all intermediate files and prepares the remaining ones for the ML pipeline\n",
    "\n",
    "1. Removes all .fli files that have been created.\n",
    "2. Removes empty folders\n",
    "3. Collects all unique polariser–analyser ID pairs\n",
    " \n",
    "As a result, the only useful files are _AmorphousPolariserAndAnalyser\\_IDs.txt_ and the folder _AmorphousMLDataBase_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3323bdf-2869-405f-b9c1-319d46ebc15a",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14e0ebb-d050-4789-9f86-2d89d7cfc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import curve_fit\n",
    "from collections import defaultdict\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69b0c9-b0ac-4808-ae61-c6ccfabe1747",
   "metadata": {},
   "source": [
    "## 2. Auxiliary Functions and log file creation\n",
    "\n",
    "1. _PrintDebug_ is a flag that allows the code to output on screen all the steps. If it is set to false, it won´t show anything. However, all information will be properly logged whether this flag is set to true or false. The name of the log is determined by the variable *log_file_path*. The code runs faster if it is set to False.\n",
    "\n",
    "2. _ShowPlot_ is a similar flag that allows the code to show on screen all plots that are being produced. They are all stored independently of whether this flag is True or False. The code runs faster if it is set to False.\n",
    "\n",
    "3. **log_message** is a function used for writting on the log file\n",
    "\n",
    "4. **long_path** is a function that \"fixes\" directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f65c38-5426-43c4-bd7b-afcd9aaefe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintDebug = False \n",
    "ShowPlot = False \n",
    "# Initialize log file at the start of the script\n",
    "log_file_path = os.path.join(\".\", \"AmorphousLog_Testing_Creation.txt\")\n",
    "with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(\"=== Log started ===\\n\")\n",
    "\n",
    "def log_message(message):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        message (string): The text that will be logged\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    Notes:\n",
    "        It will write the string \"message\" in the log file.\n",
    "        If PrintDebug==True then it will also print the string\n",
    "    \"\"\"\n",
    "    message = str(message)\n",
    "    if PrintDebug:\n",
    "        print(message)\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "        log_file.write(str(message) + \"\\n\")\n",
    "\n",
    "################################################################\n",
    "\n",
    "def long_path(path):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        path (path): The path that needs to be converted\n",
    "\n",
    "    Returns:\n",
    "        The updated path string or path depending on the platform used\n",
    "\n",
    "    Notes:\n",
    "        To avoid Windows 260 character limit for Windows paths, a special \"prefix\" is added.\n",
    "        It also unifies how directories are managed.\n",
    "        Also works with Linux and Mac\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert to Path and resolve to absolute\n",
    "    path = Path(path).resolve()\n",
    "\n",
    "    #Windows only:\n",
    "    if os.name == \"nt\":\n",
    "        path_str = str(path)\n",
    "        if not path_str.startswith(\"\\\\\\\\?\\\\\"):\n",
    "            if path_str.startswith(\"\\\\\\\\\"):\n",
    "                path_str = \"\\\\\\\\?\\\\UNC\\\\\" + path_str[2:]\n",
    "            else:\n",
    "                path_str = \"\\\\\\\\?\\\\\" + path_str\n",
    "            return path_str\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca18716-28be-414e-b5eb-80ae8fbabbd4",
   "metadata": {},
   "source": [
    "## 3. Functions\n",
    "\n",
    "1. **Time** is a function that converts time to a universal format\n",
    "\n",
    "2. **deltatime** is a funtions that computes the difference in time between two sets of time, in seconds \n",
    "\n",
    "3. **format_combination** is just an aesthetic change in the Miller index combination variable\n",
    "\n",
    "4. **sanitize** is a funtion that fixes any directory path with \"illegal\" variables\n",
    "\n",
    "5. **savgol_params_func** is a function that ensures that the window length is odd and large enough for the polynomial order.\n",
    "\n",
    "6.  **Overall_Decrease** is a function that checks if the linear approximation of the data set has a negative slope\n",
    "\n",
    "7. **filter_best_combination**  is a function that discards all problematic data sets (negative polarization, small sets of data and also uses **Overall_Decrease**  \n",
    "            \n",
    "8. **RemoveOutcast_FixUncertainty** is a funtion that removes points that are clear outliers, corrects the underestimation of experimental uncertainty and plots succesful experiments\n",
    "\n",
    "9. **ensure path** is a function that makes sure a file path is Windows-safe, its folders exist, and the file exists—without crashing due to long paths or missing directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfcb2cd-92f2-42f2-83fe-6690d1b51d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Time(Day_Ref, Hour_Ref):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        Day_Ref (str): 'DD/MM/YY' a.k.a Day/Month/Year\n",
    "        Hour_Ref (str): = 'HH:MM' or 'HH:MM:SS' a.k.a Hour:Month:Second\n",
    "        \n",
    "    Returns:\n",
    "        A 'datetime' object with format (year, month, day, hour, minute, second).\n",
    "        \n",
    "    Notes:\n",
    "        If there is no information about the seconds, they will be considered 0\n",
    "    \"\"\"\n",
    "    \n",
    "    match = re.match(r\"(\\d+)/(\\d+)/(\\d+)\", Day_Ref)\n",
    "    if match:\n",
    "        DD = int(match.group(1))\n",
    "        MM = int(match.group(2))\n",
    "        YY = int(match.group(3))\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid date format: {Day_Ref}\")\n",
    "\n",
    "    match = re.match(r\"(\\d+):(\\d+):(\\d+)\", Hour_Ref)\n",
    "    if match:\n",
    "        Hour = int(match.group(1))\n",
    "        Minute = int(match.group(2))\n",
    "        Second = int(match.group(3))\n",
    "    else:\n",
    "        # If seconds are missing, try HH:MM\n",
    "        match = re.match(r\"(\\d+):(\\d+)\", Hour_Ref)\n",
    "        if match:\n",
    "            Hour = int(match.group(1))\n",
    "            Minute = int(match.group(2))\n",
    "            Second = 0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time format: {Hour_Ref}\")\n",
    "\n",
    "    return datetime(YY + 2000 if YY < 100 else YY, MM, DD, Hour, Minute, Second)\n",
    "\n",
    "################################################################\n",
    "\n",
    "\n",
    "def deltatime(AIni,BIni, AFin,BFin):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        AIni (str): 'DD/MM/YY' a.k.a Day/Month/Year for the initial time\n",
    "        BIni (str): = 'HH:MM' or 'HH:MM:SS' a.k.a Hour:Month:Second for the initial time\n",
    "        AFin (str): 'DD/MM/YY' a.k.a Day/Month/Year for the final time\n",
    "        BFin (str): = 'HH:MM' or 'HH:MM:SS' a.k.a Hour:Month:Second for the final time\n",
    "    Returns:\n",
    "        The time variation in seconds\n",
    "        \n",
    "    Notes:\n",
    "        Requires the function \"Time\"\n",
    "    \"\"\"\n",
    "\n",
    "    time1 = Time(AIni, BIni)\n",
    "    time2 = Time(AFin, BFin)\n",
    "    return( int((time2 - time1).total_seconds()))\n",
    "\n",
    "#################################################################\n",
    "\n",
    "def format_combination(comb):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        comb (float, float, float): A set of three floats characterizing the Miller indices.\n",
    "\n",
    "    Returns:\n",
    "        An object (int,int,int): With the floor integer of those float variables. (3.0 -> 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    if comb is None:\n",
    "        return \"(None)\"\n",
    "    ints = tuple(int(float(x)) for x in comb)\n",
    "    return f\"({','.join(map(str, ints))})\"\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def sanitize(name):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        name (str): Directory string \n",
    "    Returns:\n",
    "        The same string but with symbols [,<,>,:,\",/,\\\\,|,?,*,] converted to _\n",
    "    \"\"\"\n",
    "    \n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', name)\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def savgol_params_func(n_points):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        n_points (int): Number of points where the filtered will be used\n",
    "    Returns:\n",
    "        A dictionary containing valid parameters for a Savitzky–Golay filter.\n",
    "    Notes:    \n",
    "        It ensures that the window length is odd and large enough for the polynomial order.\n",
    "    \"\"\"\n",
    "    window_length = min(default_window_length, n_points)\n",
    "    if window_length % 2 == 0:\n",
    "        window_length -= 1\n",
    "    if window_length < polyorder + 2:\n",
    "        window_length = polyorder + 2\n",
    "        if window_length % 2 == 0:\n",
    "            window_length += 1\n",
    "    return {'window_length': window_length, 'polyorder': polyorder}\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def Overall_Decrease(df_filtered):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        df_filtered (pandas object): It is something like this:\n",
    "                1    2    3  PolarizationD3  ErrPolarizationD3  DeltaTime  \n",
    "            0  3.0  3.0  3.0          0.5552             0.0022          0   \n",
    "            1  3.0  3.0  3.0          0.5522             0.0021        279   \n",
    "            2  3.0  3.0  3.0          0.5464             0.0022       4148\n",
    "            ...\n",
    "        \n",
    "    Returns:\n",
    "        True if df_filtered shows a decay and False otherwise\n",
    "        \n",
    "    Notes:    \n",
    "        First, it extracts polarization and time arrays. \n",
    "        Second, it obtains the best linear fit.\n",
    "        If the slope is positive then they get discarded to herwise they get accepted.\n",
    "        Amorphous experiments are more stable than the crystaline ones. Therefore, less checks are needed.\n",
    "    \"\"\"\n",
    "    def linear_func(x, m, n):\n",
    "        return m * x + n\n",
    "\n",
    "\n",
    "    x = df_filtered[\"DeltaTime\"].values\n",
    "    y = df_filtered[\"SoftPolarizationD3\"].values\n",
    "    \n",
    "    try:\n",
    "        popt, _ = curve_fit(linear_func, x, y)\n",
    "        m, n = popt\n",
    "        log_message(f\"      Linear fit slope m={m:.4e}, intercept n={n:.4f}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"      Error fitting data: {e}\")\n",
    "        return False\n",
    "\n",
    "    if m > 0:\n",
    "        log_message(f\"      Overall slope is positive. Can't be polarization information. Skipping Combination\")\n",
    "        return False\n",
    "    else:\n",
    "        return True  \n",
    "\n",
    "###############################################################\n",
    "\n",
    "def filter_best_combination(i, df):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. i (int): The chunk number a.k.a the (ordinal) number of the Miller index combination\n",
    "        2. df (pandas object): It is something like this: (NaNs are intended)\n",
    "                  1    2    3  PolarizationD3 ErrPolarizationD3  12   13   14  DeltaTime\n",
    "            0   3.0  3.0  3.0          0.5383            0.0021 NaN  NaN  NaN          0\n",
    "            1   3.0  3.0  3.0          0.5379            0.0021 NaN  NaN  NaN        147\n",
    "            2   3.0  3.0  3.0          0.5315            0.0022 NaN  NaN  NaN       3919\n",
    "            ...\n",
    "\n",
    "    Returns:\n",
    "        1. A filtered df object like this one:\n",
    "                  1    2    3  PolarizationD3 ErrPolarizationD3 DeltaTime\n",
    "            0   3.0  3.0  3.0          0.5383            0.0021         0\n",
    "            1   3.0  3.0  3.0          0.5379            0.0021       147\n",
    "            2   3.0  3.0  3.0          0.5315            0.0022      3919\n",
    "            ...\n",
    "        2. An object (int,int,int) with the adequate Miller index combination.\n",
    "        \n",
    "    Notes:    \n",
    "        First, it extracts the Miller index combination and converts it into a set of three integers (format_combination)\n",
    "        Then it tries a couple of tests to see if the data associated to them is valid\n",
    "            1. Check there is data\n",
    "            2. Check if the time array is present and convert all values to either floats or integers\n",
    "            3. Check if all polarization values are positive. If they are not, skip that Miller index combination\n",
    "            4. Check if there are more than three rows of data. If there are not, skip that Miller index combination\n",
    "            5. Check if the filtered df object passes the Overall_Decrease\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    filter_func=savgol_filter #Only tested for 'savgol_filter'\n",
    "    filter_params_func=savgol_params_func #Only tested for the previously defined function 'savgol_params_func'\n",
    "    min_points_required=3  #Minimum points needed for the filter to work (3 for Savitzky-Golay)\n",
    "    tolerance=1e-8 #Tolerance to decide if the filtered value is worth keeping\n",
    "    filter_column_idx=df.columns.get_loc('PolarizationD3')\n",
    "    time_column_idx=df.columns.get_loc('DeltaTime')\n",
    "    error_column_idx=df.columns.get_loc('ErrPolarizationD3')\n",
    "    new_column_name='SoftPolarizationD3'\n",
    "    folder_name = FileName.replace(\".fli\", \"\")\n",
    "\n",
    "    # Group by first three columns (Miller indices)\n",
    "    combination_counts = (\n",
    "        df.groupby([df.columns[0], df.columns[1], df.columns[2]])\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    log_message(f\"Analyzing combinations in file: {folder_name}_Array_{i}.fli\")\n",
    "    \n",
    "    #Read those three numbers from the .fli file\n",
    "    for comb, count in combination_counts.items(): \n",
    "        log_message(f\"Combination {comb} occurs {count} times in file {folder_name}.fli. Trying this combination\")\n",
    "        mask = (\n",
    "            (df.iloc[:,0] == comb[0]) &\n",
    "            (df.iloc[:,1] == comb[1]) &\n",
    "            (df.iloc[:,2] == comb[2])\n",
    "        )\n",
    "        PrettyCombination = format_combination(comb)\n",
    "        filtered_df = df.loc[mask].copy()\n",
    "        \n",
    "        \n",
    "        # Requisites for the Combination to be valid:\n",
    "        # Requisite 1: Have data in the data\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      {PrettyCombination} has no data\")\n",
    "            continue\n",
    "        log_message(filtered_df)\n",
    "        \n",
    "        filtered_df = filtered_df.drop(filtered_df.columns[5], axis=1) #NaNs are eliminated\n",
    "        filtered_df = filtered_df.drop(filtered_df.columns[5], axis=1)\n",
    "        filtered_df = filtered_df.drop(filtered_df.columns[5], axis=1)\n",
    "        \n",
    "        \n",
    "        # Requisite 2: Check if data column exists\n",
    "        if filtered_df.shape[1] <= filter_column_idx:\n",
    "            log_message(f\"      Expected column index {filter_column_idx} not found. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert to numeric all columns (all columns are considered as object type)\n",
    "        filtered_df = filtered_df.apply(pd.to_numeric, errors='coerce')\n",
    "        filtered_df = filtered_df.dropna()  # drops any rows with NaNs introduced by coercion (last line)\n",
    "\n",
    "        # Check dtypes\n",
    "        all_numeric = all(dtype.kind in ('f', 'i') for dtype in filtered_df.dtypes)\n",
    "        \n",
    "        if all_numeric:\n",
    "            log_message(f\"      All columns have been successfully converted to numbers.\")\n",
    "        else:\n",
    "            log_message(f\"      Not all columns are numbers. Current dtypes:\")\n",
    "            log_message(f\"      {filtered_df.dtypes}\")\n",
    "            log_message(f\"      Expect Error Message from Python. Perhaps removing this file might be wise unless all files have the same issue\")\n",
    "        if filtered_df.empty:\n",
    "            log_message(f\"      All rows dropped after conversion to numeric. Skipping combination {PrettyCombination}\")\n",
    "            continue\n",
    "        \n",
    "        # Requisite 3: Polarization is ALWAYS positive. If any is negative, that is not a polarization. Immediately sent to the Bad Files Folder\n",
    "        if (filtered_df.iloc[:, filter_column_idx] < 0).any():\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "            badfile_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "            filtered_df.to_csv(badfiles_txt_path, index=False, sep='\\t')\n",
    "            log_message(f\"      {PrettyCombination} has negative polarization values. Sent to BadFiles with name {filename}. Skipping to next Combination\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Requisite 4: Have at least three rows (otherwise we can't teach the ML algorithm nothing although three is almost useless too).\n",
    "        if len(filtered_df) < min_points_required:\n",
    "            filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Filtered.txt\"\n",
    "            badfile_subfolder = BadFilesFolder / folder_name\n",
    "            badfile_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "            badfiles_txt_path = badfile_subfolder / filename\n",
    "\n",
    "            filtered_df.to_csv(long_path(badfiles_txt_path), index=False, sep=\"\\t\")\n",
    "\n",
    "            log_message(f\"      {PrettyCombination} has only {len(filtered_df)} rows (< {min_points_required}). Sent to BadFiles with name {filename}. Skipping to next Combination\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # \"Requisite 5\": Be worthy of having the filter used.\n",
    "        # If the filter can´t be used, the \"soft\" correction will have the same data as the original\n",
    "        y = filtered_df.iloc[:, filter_column_idx].values\n",
    "        filter_params = filter_params_func(len(y))\n",
    "        try:\n",
    "            y_filtered = filter_func(y, **filter_params)\n",
    "            diff = np.abs(y - y_filtered)\n",
    "            changed_count = np.sum(diff > tolerance)\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "            if changed_count > 0:\n",
    "                log_message(f\"      Filter changed {changed_count}/{len(y)} points. Adding column '{new_column_name}'.\")\n",
    "            else:\n",
    "                log_message(f\"      Filter applied but data unchanged. Adding '{new_column_name}' as duplicated values.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_message(f\"      Error applying filter to combination {comb}: {e}\")\n",
    "            log_message(f\"      Adding '{new_column_name}' as duplicated values to proceed anyway.\")\n",
    "            # Just duplicate the original column\n",
    "            y_filtered = y.copy()\n",
    "            filtered_df[new_column_name] = y_filtered\n",
    "\n",
    "            \n",
    "            \n",
    "        # Requisite 6: Use Overall_Decrease to test the data set\n",
    "        if Overall_Decrease(filtered_df):\n",
    "            log_message(f\"      {PrettyCombination} has surpassed all tests. Proceding with it.\")\n",
    "            return filtered_df, PrettyCombination\n",
    "        else:\n",
    "            log_message(f\"      {PrettyCombination} failed the Overall_Decrease test. Trying next combination.\")\n",
    "            continue\n",
    "            \n",
    "    return None, None\n",
    "\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def RemoveOutcast_FixUncertainty(df_filtered, PrettyCombination, filename, AcceptableMultiplier=2.0):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. df_filtered (pandas object): It is something like this:\n",
    "                  1    2    3  PolarizationD3 ErrPolarizationD3 DeltaTime\n",
    "            0   3.0  3.0  3.0          0.5383            0.0021         0\n",
    "            1   3.0  3.0  3.0          0.5379            0.0021       147\n",
    "            2   3.0  3.0  3.0          0.5315            0.0022      3919\n",
    "            ...\n",
    "        2. PrettyCombination (object): A set of three integers. For example (3,3,3)\n",
    "        3. filename (str): The name used to save the data. For example: f\"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}\"\n",
    "        4. AcceptableMultiplier\n",
    "\n",
    "    Returns:\n",
    "        1. A filtered df object like this one:\n",
    "                  1    2    3  PolarizationD3 ErrPolarizationD3 DeltaTime\n",
    "            0   3.0  3.0  3.0          0.5383            0.0021         0\n",
    "            1   3.0  3.0  3.0          0.5379            0.0021       147\n",
    "            2   3.0  3.0  3.0          0.5315            0.0022      3919\n",
    "            ...\n",
    "        2. An object (int,int,int) with the adequate Miller index combination.\n",
    "        \n",
    "    Notes:    \n",
    "        The function does three different jobs.\n",
    "            1. It removes all points that are outliers.\n",
    "                First, it computes the best linear fit to the data P=m·t+n\n",
    "                Second, it finds the smallest value of N such that 75% of the points are in the region -N + n + m·t < P < m·t + n + N\n",
    "                Third, it erases all points outside this range: -N·AcceptableMultiplier + n + m·t < P < m·t + n + N·AcceptableMultiplier\n",
    "            2. It re-scales the uncertainties of the rest of the points\n",
    "                The reasoning behind this rescaling is because if we assume that the decay is a smooth curve, then, clearly, the uncertainties are underestimated\n",
    "            3. Plots the succesful polarization decay\n",
    "                Two plots are saved, one with the original data and another wit the filtered data (both with the corrected uncertainty)    \n",
    "    \"\"\"\n",
    "  \n",
    "    output_folder = Path.cwd() / \"AmorphousPlotResults\"\n",
    "    output_folder.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "\n",
    "    x = df_filtered['DeltaTime'].values\n",
    "    y_hard = df_filtered['PolarizationD3'].values\n",
    "    y_soft = df_filtered['SoftPolarizationD3'].values\n",
    "\n",
    "    #Linear fit\n",
    "    def linear_func(x, m, n):\n",
    "        return m * x + n\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(linear_func, x, y_hard)\n",
    "        m, n = popt\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error fitting data: {e}\")\n",
    "        return df_filtered  # Return original if error\n",
    "\n",
    "    #Find smallest N for 75% within band\n",
    "    num_points = len(x)\n",
    "    sorted_idx = np.argsort(x)\n",
    "    x_sorted = x[sorted_idx]\n",
    "    y_sorted = y_hard[sorted_idx]\n",
    "\n",
    "    N_start = 0.0001\n",
    "    N_step = 0.0001\n",
    "    N_max = 0.4\n",
    "    N = N_start\n",
    "    needed_N = None\n",
    "\n",
    "    while N <= N_max:\n",
    "        y_fit = linear_func(x_sorted, m, n)\n",
    "        upper = y_fit + N\n",
    "        lower = y_fit - N\n",
    "        inside = np.logical_and(y_sorted <= upper, y_sorted >= lower)\n",
    "        percent_inside = np.sum(inside) / num_points * 100\n",
    "\n",
    "        if percent_inside >= 75:\n",
    "            needed_N = N\n",
    "            break\n",
    "        N += N_step\n",
    "\n",
    "    if needed_N is None:\n",
    "        log_message(f\"    [{filename}] No N found to contain 75% within ±{N_max}\")\n",
    "        return df_filtered  # Return original if no good N is found (has never happened but just in case)\n",
    "\n",
    "    # Compute extended band and filter out the points outside the extended band\n",
    "    y_fit_full = linear_func(x, m, n)\n",
    "    upper_band = y_fit_full + needed_N * AcceptableMultiplier\n",
    "    lower_band = y_fit_full - needed_N * AcceptableMultiplier\n",
    "\n",
    "    mask = np.logical_and(y_hard <= upper_band, y_hard >= lower_band)\n",
    "    df_cleaned = df_filtered[mask].copy()\n",
    "    log_message(f\"    [{filename}] Filtering kept {np.sum(mask)} of {len(mask)} rows (±{needed_N * AcceptableMultiplier:.2e})\")\n",
    "\n",
    "    # Rescale uncertainties using reduced chi-squared \n",
    "    sigma = df_filtered['ErrPolarizationD3'].values\n",
    "    popt, pcov = curve_fit(linear_func, x, y_hard, sigma=sigma, absolute_sigma=True)\n",
    "    m_fit, n_fit = popt\n",
    "    m_err, n_err = np.sqrt(np.diag(pcov)) #Covariance matrix has on its diagonal Cov(X_j,X_j) which is the variance so its square root is the uncertainty\n",
    "    \n",
    "    # Recalculate the reduced chi-squared\n",
    "    residuals = (y_hard - linear_func(x, *popt)) / sigma\n",
    "    dof = len(x) - len(popt)\n",
    "    chi_squared_red = np.sum(residuals**2) / dof\n",
    "    correction_factor = np.sqrt(chi_squared_red)\n",
    "    \n",
    "    # Automatically apply correction if needed\n",
    "    if correction_factor > 1:\n",
    "        df_cleaned['ErrPolarizationD3'] *= correction_factor\n",
    "        log_message(f\"    [{filename}] Applied uncertainty correction factor: √(χ²) = {correction_factor:.2f}\")\n",
    "    else:\n",
    "        log_message(f\"    [{filename}] No correction applied: √(χ²) = {correction_factor:.2f}\")\n",
    "    \n",
    "    # Optional: log the fit results\n",
    "    log_message(f\"    [{filename}] Fit results: m = {m_fit:.3e} ± {m_err:.3e}, n = {n_fit:.4f} ± {n_err:.4f}\")\n",
    "\n",
    "\n",
    "    def make_clean_name(filename: str) -> str:\n",
    "        \"\"\"\n",
    "        Turn e.g.\n",
    "          PolarizationD3_CaFeAl_13_7_6_24_2_MillerIndex_(0,0,2)_Filtered.txt\n",
    "        into:\n",
    "          CaFeAl_13_7_6_24_2_(0,0,2)\n",
    "        and handle cases where filename contains '/' or '\\\\' (dates like DD/MM/YY).\n",
    "        \"\"\"\n",
    "        s = str(filename).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")  # prevent path splitting\n",
    "        base = Path(s).stem  \n",
    "        if base.startswith(\"PolarizationD3_\"):\n",
    "            base = base[len(\"PolarizationD3_\"):]\n",
    "        if base.endswith(\"_Filtered\"):\n",
    "            base = base[:-len(\"_Filtered\")]\n",
    "        base = base.replace(\"MillerIndex_\", \"\")\n",
    "        return base\n",
    "    \n",
    "    def extended_area_plot_filename(filename: str) -> str:\n",
    "        \"\"\"EuAgAs_5_31_10_23_0_(3,0,0)_ExtendedArea.png\"\"\"\n",
    "        return f\"{make_clean_name(filename)}_ExtendedArea.png\"\n",
    "    # Extract values\n",
    "    # Data\n",
    "    T = df_filtered['DeltaTime'].values\n",
    "    P_soft = df_filtered['SoftPolarizationD3'].values\n",
    "    P_hard = df_filtered['PolarizationD3'].values\n",
    "    Err = df_filtered['ErrPolarizationD3'].values if 'ErrPolarizationD3' in df_filtered.columns else np.zeros_like(P_soft)\n",
    "    \n",
    "    # Clean title + filename\n",
    "    clean = make_clean_name(filename)\n",
    "    save_name = extended_area_plot_filename(filename)  # ends with _ExtendedArea.png\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Black points with error bars\n",
    "    plt.scatter(T, P_hard, s=30, color=\"black\", label=\"PolarizationD3\", marker='o')\n",
    "    if Err is not None:\n",
    "        plt.errorbar(T, P_hard, yerr=Err, fmt='none', ecolor='black', alpha=0.6, capsize=2)\n",
    "    \n",
    "    # Blue linear fit\n",
    "    fit = linear_func(T, m, n)\n",
    "    plt.plot(T, fit, '-', color='blue', label=\"Linear Fit\")\n",
    "    \n",
    "    # Bands: light blue (\\pm N) and translucent green /\\pm N*AcceptableMultiplier)\n",
    "    if needed_N is not None:\n",
    "        upper_narrow = fit + needed_N\n",
    "        lower_narrow = fit - needed_N\n",
    "        upper_wide   = fit + needed_N * AcceptableMultiplier\n",
    "        lower_wide   = fit - needed_N * AcceptableMultiplier\n",
    "    \n",
    "        plt.fill_between(T, lower_narrow, upper_narrow, color='lightblue', alpha=0.35, label=f'Band ±{needed_N:.2e}')\n",
    "        plt.fill_between(T, lower_wide,   upper_wide,   color='green',     alpha=0.18, label=f'Filter Band ±{(needed_N*AcceptableMultiplier):.2e}')\n",
    "    \n",
    "    # Labels\n",
    "    plt.xlabel(\"DeltaTime\")\n",
    "    plt.ylabel(\"PolarizationD3\")\n",
    "    plt.title(f\"{clean}_ExtendedArea\")  # title matches saved name (without .png)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    vals_min = [np.nanmin(P_hard - Err), np.nanmin(P_hard + Err)]\n",
    "    vals_max = [np.nanmax(P_hard - Err), np.nanmax(P_hard + Err)]\n",
    "    if needed_N is not None:\n",
    "        vals_min += [np.nanmin(lower_narrow), np.nanmin(lower_wide)]\n",
    "        vals_max += [np.nanmax(upper_narrow), np.nanmax(upper_wide)]\n",
    "    ymin = np.nanmin(vals_min)\n",
    "    ymax = np.nanmax(vals_max)\n",
    "    pad = 0.02 * (ymax - ymin if np.isfinite(ymax - ymin) and (ymax - ymin) > 0 else 1.0)\n",
    "    plt.ylim(ymin - pad, ymax + pad)\n",
    "    plot_path_hard = output_folder / save_name\n",
    "    plot_path_hard.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(long_path(plot_path_hard), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def ensure_file(path):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        path (str): A directory path\n",
    "        \n",
    "    Output:\n",
    "        A string with a path that avoids Windows related issues\n",
    "    Note:\n",
    "        It makes sure a file path is Windows-safe, its folders exist\n",
    "        and the file exists—without crashing due to long paths or missing directories.\n",
    "        It works on any environment\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if path.parent != Path('.'):\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not path.exists():\n",
    "        path.touch()\n",
    "\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71b017-9d9c-4b83-a2aa-ff26d9c03c3b",
   "metadata": {},
   "source": [
    "## 4. Clean old files and force certain experiments\n",
    "\n",
    "Some parts of the code might use data from different sessions. It is safer to erase them and create all files from scratch everytime. This is not a big deal because this code file should only be run once unless the data base changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e242d5b2-df25-4aec-ae2f-14f7dababedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_erase = [\n",
    "    \"AmorphousLog_Testing_Creation.txt\",\n",
    "    \"AmorphousPolariserAndAnalyser_IDs.txt\",\n",
    "    \"AmorphousSeparatedFolder\",\n",
    "    \"AmorphousPlotResults\",\n",
    "    \"AmorphousMLDataBase\",\n",
    "    \"AmorphousFailuresTest\",\n",
    "    \"AmorphousDataBase\",\n",
    "    \"AmorphousBadFiles\",\n",
    "    \"AmorphousFailuresFiles\"\n",
    "]\n",
    "for item in to_erase:\n",
    "    path = os.path.abspath(item) \n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                log_message(f\"Deleted file: {path}\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                log_message(f\"Deleted folder: {path}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\" Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"Not found (skipped): {path}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4323be-39c8-4d24-8abf-8fd37ae77300",
   "metadata": {},
   "source": [
    "## 5. ZIP Folder Treatment and .fli data extraction\n",
    "\n",
    "The code will take all zipped folders from the folder _D3Files_ and prepare them to get their .fli files extracted.\n",
    "\n",
    "First, it will check if there are duplicate zip folders. To check it it will compare the folder name and the hash sha256. Duplicate folders will be erased. For more information about hash sha256 check for example:\n",
    ">Wikipedia contributors. (2026, January 2). SHA-2. In Wikipedia, The Free Encyclopedia. Retrieved 10:49, January 17, 2026, from https://en.wikipedia.org/w/index.php?title=SHA-2&oldid=1330753870\n",
    "\n",
    "Second, it will copy the contents of the zipped folders and create a new folder with the name of the experiment inside _D3Files_. No more zipped folders are erased and information gets duplicated.\n",
    "\n",
    "Third, it will try to find all .fli files inside all the unzipped folders whether if they come from a zipped file or not. It will then send them to the newly created folder _AmorphousDataBase_. If, for each experiment proposal there are more than one .fli files, they get a numeric suffix ('_1', '_2',...) to distinguish them. Afterwards, all unzipped folders get erased leaving behind only the non-duplicated zipped folders.\n",
    "\n",
    "Note: All unzipped folders in D3Files will be explored, however they will get erased at the end of the pipeline. If you want them to persist for future runs of the code, they should be zipped first. For ILL users, when navigating the ILL Cloud, the easiest way to prepare the zip files is to download the _processed_ folder for each experiment proposal. The code is \"smart\" enough to only process .fli files with polarization information. Therefore, there is no need to manually prepare anything.\n",
    "To be precise, this cell of code will take all the zip files, extract them nd remove duplicates using the file name And the hash sha256. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83733962-70fc-47fa-b870-047a1abb701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DUPLICATION TREATMENT \"\"\"\n",
    "def file_hash(filepath, algo=\"sha256\", block_size=65536):\n",
    "    \"\"\"Compute hash of a file (default SHA256).\"\"\"\n",
    "    h = hashlib.new(algo)\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for block in iter(lambda: f.read(block_size), b\"\"):\n",
    "            h.update(block)\n",
    "    return h.hexdigest()\n",
    "\n",
    "folder = Path(\"D3Files\")  # Folder where all the raw data folders reside\n",
    "zip_files = [f for f in folder.iterdir() if f.suffix.lower() == \".zip\"]  # List all zip files\n",
    "\n",
    "log_message(f\"Reading ZIP files. Checking for true duplicates by content...\")\n",
    "base_names = set()\n",
    "seen_hashes = {}\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    filehash = file_hash(zip_file)\n",
    "    name = zip_file.stem\n",
    "\n",
    "    if filehash in seen_hashes:\n",
    "        log_message(f\"   Duplicate confirmed by hash! Removing: {zip_file.name} (same as {seen_hashes[filehash].name})\")\n",
    "        zip_file.unlink()   # delete the file\n",
    "    else:\n",
    "        seen_hashes[filehash] = zip_file\n",
    "        base_names.add(name)\n",
    "\n",
    "log_message(f\"\\nAll duplicates (by content) removed. Begin unzipping...\\n\")\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\"\"\" UNZIPPING \"\"\"\n",
    "\n",
    "log_message(f\"Begin unzipping...\\n\")\n",
    "\n",
    "# Refresh zip_files list after removals\n",
    "zip_files = [f for f in folder.iterdir() if f.suffix.lower() == \".zip\"]\n",
    "\n",
    "# Unzip and remove original zip files\n",
    "for zip_file in zip_files:\n",
    "    if zipfile.is_zipfile(zip_file):\n",
    "        folder_name = sanitize(zip_file.stem)\n",
    "        extract_dir = folder / folder_name\n",
    "        log_message(f\"   Unzipping: {zip_file.name} -> {extract_dir}\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_dir)\n",
    "        except Exception as e:\n",
    "            log_message(f\"   WARNING: Error extracting {zip_file.name}: {e}\")\n",
    "    else:\n",
    "        log_message(f\"   WARNING: Skipping invalid zip file: {zip_file.name}\")\n",
    "\n",
    "log_message(f\"\\nFinished Unzipping. Experiments stored in individual folders substituting the zip files\\n\")\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\"\"\" .fli FILE EXTRACTION \"\"\"\n",
    "\n",
    "source_folder = folder  # D3Files\n",
    "database_folder = Path(\"AmorphousDataBase\")\n",
    "database_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Scan all items in source_folder to find .fli files\n",
    "log_message(f\"\\nScanning all folders for .fli files...\\n\")\n",
    "for item_path in source_folder.iterdir():\n",
    "    if item_path.is_dir():\n",
    "        log_message(f\"   Processing folder: {item_path.name}\")\n",
    "\n",
    "        # Find all .fli files inside this folder (including subfolders and subsubfolders, etc)\n",
    "        for root, dirs, files in os.walk(item_path):\n",
    "            root_path = Path(root)\n",
    "            for file in files:\n",
    "                if file.lower().endswith(\".fli\"):\n",
    "                    src_file = root_path / file\n",
    "                    dest_file = database_folder / file\n",
    "\n",
    "                    # Handle duplicate names\n",
    "                    counter = 1\n",
    "                    base_name, ext = file.rsplit('.', 1)\n",
    "                    while dest_file.exists():\n",
    "                        dest_file = database_folder / f\"{base_name}_{counter}.{ext}\"\n",
    "                        counter += 1\n",
    "\n",
    "                    log_message(f\"   Copying: {src_file} -> {dest_file}\")\n",
    "                    shutil.copy2(src_file, dest_file)\n",
    "\n",
    "        # After processing all .fli files, delete the original folder\n",
    "        log_message(f\"   Deleting folder: {item_path}\")\n",
    "        shutil.rmtree(item_path)\n",
    "        log_message(f\"\\nAll .fli files collected, sent from folder {source_folder} to folder {database_folder} and unzipped folders removed.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2711b1-7839-4d46-96bf-d93e02cdeb33",
   "metadata": {},
   "source": [
    "## 6. Separation of .fli files according to experiments\n",
    "\n",
    "Some fli files have the wrong structure which means that they are not polarization measurements and if they are polarization files they may have used more than one polarization cell. Therefore, we need to remove all the non-polarization sets of data and separate the good fli files depending of the type of polarization cell they used.\n",
    "\n",
    "For evey fli file we will read the contents and try to find the header (two strings in two consecutive lines). This symbolizes the installation of a new polarizer cell. If there are numerical values before the first header, that means that the process of saving the file occured before swapping the cell. Those data rows will be skipped. A correct fli file will have the following structure:\n",
    "\n",
    "|  |  |  |  |  |  |  |  |  |  |  |  |\n",
    "|--|--|--|--|--|--|--|--|--|--|--|--|\n",
    "| polariser cell info | ge18004 | pressure/init. polar | 2.30 | 0.79 | initial date/time | 21/11/23 | @ | 12:45 |\n",
    "| analyser cell info  | sic1402 | pressure/init. polar | 2.00 | 0.79 | initial date/time | 21/11/23 | @ | 12:45 |\n",
    "| 40661 | 3.000 | 3.000 | 3.000 | 21/11/23 | 12:50:35 | 0.00 | 0.7890 | 0.0020 | 8.4795 | 0.0897 | 120.00 |\n",
    "| 40662 | 3.000 | 3.000 | 3.000 | 21/11/23 | 12:54:43 | 0.00 | 0.7851 | 0.0020 | 8.3048 | 0.0867 | 120.00 |\n",
    "|  ...  |       |       |       |          |          |      |        |        |        |        |        |\n",
    "\n",
    "Which corresponds to the following information for the first two rows:\n",
    "1. 'polariser cell info'/'analyser cell info' (str): Log of the installation of the first and second polariser cells\n",
    "2. 'PolariserID' (str): A string with the type of cell used\n",
    "3. 'pressure/init. polar' (str): A string to introduce the $^\\mathrm{3}$He gas pressure and the polarization measured at the creation lab.\n",
    "4. 'PolariserPressure' (float): $^\\mathrm{3}$He gas pressure in some units\n",
    "5. 'InitialLabPolarization' (float): Polarization measured at the creation lab\n",
    "6. 'initial date/time' (str): A string that introduces the day, month and year and the hour and minutes.\n",
    "7. 'Date' (str): A string with the information DD/MM/YY\n",
    "8. '@' (str): A string to separate date and time\n",
    "9. 'time' (str): A string with the information HH:MM\n",
    "\n",
    "And for the rest of the rows:\n",
    "1. 'Measurement number' (int): The number index of the measurement.\n",
    "2. 'First_Miller_Index' (float): The first Miller index of the crystal. Polarization is measured using a known Si Bragg crystal. For the source of the origin of the Si crystal see:\n",
    ">Stunault, Anne & Vial, S & Pusztai, Laszlo & Cuello, Gabriel & Temleitner, László. (2016). Structure of hydrogenous liquids: separation of coherent and incoherent cross sections using polarised neutrons. Journal of Physics: Conference Series. 711. 012003. 10.1088/1742-6596/711/1/012003. \n",
    "3. 'Second_Miller_Index' (float)\n",
    "4. 'Third_Miller_Index' (float):\n",
    "5. 'Date' (str): A string with the information DD/MM/YY of that measurement\n",
    "6. 'time' (str): A string with the information HH:MM:SS of that measurement\n",
    "7. Unknown float, maybe Temperature\n",
    "8. 'D3Polarization' (float): A float with the polarization measurement\n",
    "9. 'ErrD3Polarization' (float): A float with the uncertainty of that polarization measurement\n",
    "10. 'FlippingRatio' (float): The flipping ratio. Given either the flipping ratio or the polarization value, the other one is fully determined. Therefore, only one is needed and that is why we don´t work with the flipping ratio\n",
    "11. 'ErrFlippingRatio' (float): The uncertainty of the flipping ratio\n",
    "12. 'Elapsed time' (float): It is the time used to obtain the measurement (integration of the beam over that number of seconds)\n",
    "\n",
    "Temperature did not seem to have an effect on the decay. Therefore, it has been eliminated in this code cell. Here is a summary of what the code does:\n",
    "\n",
    "1. The code will go through the .fli files and find all combinations of consecutive rows with 'polariser cell info' and 'analyser cell info'. It doesn´t care about the order which makes the code more robust. We will consider that a polariser cell has been properly installed whrn both of these rows are present and that the cell has been changed once a new set of polariser and analyser rows are encountered. At the moment it ignores the experiments that use the 'magical box' as we are not sure if they are experiments compatible with the ones studied here\n",
    "2. For evey cell change, a new .fli file is created storing all the information including both polariser and analyser rows and the measured data rows. Also, all cell IDs are recorded\n",
    "3. For all .fli files the code now will:\n",
    "    \n",
    "- Remove unwanted rows\n",
    "- Extract data form the header rows (polariser row and analyser row)\n",
    "- Remove unwanted columns\n",
    "- Set a time reference with the first measurement row. All other time values get referenced with respect to this moment in time and converted into seconds.\n",
    "- Ignore all Miller index combinations that are not integers\n",
    "- Run through all Miller index combinations until one passes all the filters defined in previous code cells\n",
    "- Plot the succesful experiments\n",
    "- Save two files for each experiment. One with the header rows and another one with just the numeric rows (with a new header that explains what each column has)\n",
    "\n",
    "For every succesful experiment we will output:\n",
    "1. Image:  **\"PolarizationD3\\_{folder_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Multiplier={Multiplier}.png\"** in AmorphousPlotResults. Shows the plot with the extended area with the raw data\n",
    "2. Image:  **\"PolarizationD3\\_{folder_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Multiplier={Multiplier}\\_Soft.png\"** in AmorphousPlotResults. Shows the plot with the extended area with the filtered data\n",
    "3. Txt:    **\"PolarizationD3\\_{folder_name}\\_{DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}.txt\"** in AmorphousMLDataBase. It contains the four data columns (DeltaTime, PolarizationD3, SoftPolarizationD3, ErrPolarizationD3)\n",
    "4. Txt:    **\"PolarizationD3\\_{folder_name}\\_ {DD}/{MM}/{YY}\\_{i}\\_MillerIndex\\_{PrettyCombination}\\_Parameters.txt\"** in AmorphousMLDataBase. It contains the parameters (CellID, Pressure, LabPolarization, LabTime)\n",
    "   \n",
    "\n",
    "These plots are not necessary but are saved for the user to know what all the files look like.\n",
    "The files that are wrong or useless when all is done are the folowing:\n",
    "1. Txt:    **\"{folder_name}\\_Arrays\\_{i}.txt\"** in SeparatedFolder/{folder\\_name}. It still has the header and useless columns. It is the fli file of evey chunk, of every recorded experiment (correct or incorrect)\n",
    "2. Folder: **\"BadTest\"** contains all the graphs of the data sets that were considered not worthy but had more points that the ones saved. Check them if your experiment was not properly added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad65e2d-1601-4da7-a1c1-d09fc2ad1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the original folder and the final folder\n",
    "DataBase = Path('AmorphousDataBase')\n",
    "output_base = Path('AmorphousSeparatedFolder')\n",
    "\n",
    "# List all .fli files in that folder, prepare folders\n",
    "FileNameList = [f.name for f in DataBase.glob('*.fli')]\n",
    "polyorder = 2\n",
    "default_window_length = 5\n",
    "SeparatedFolder = Path(\"AmorphousSeparatedFolder\")\n",
    "BadFilesFolder = Path(\"AmorphousBadFiles\")\n",
    "MLDataBaseFolder = Path(\"AmorphousMLDataBase\")\n",
    "BadFilesFolder.mkdir(exist_ok=True)\n",
    "MLDataBaseFolder.mkdir(exist_ok=True)\n",
    "log_message(f\"\\n\\n Files in the data base that will be (tried) to be used\\n {FileNameList}\\n\")\n",
    "\n",
    "for FileName in FileNameList:\n",
    "    \"\"\"READ THE FILE AND SEPARATE IT INTO EACH EXPERIMENT USING THE POLARIZATION CELL\"\"\"\n",
    "    # 1.1- Open file\n",
    "    folder_name = FileName.replace(\".fli\", \"\")\n",
    "    output_folder = output_base / folder_name\n",
    "    file_path = DataBase / FileName\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    with open(long_path(file_path), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 1.2- Locate the header with CellID, Pressure, etc. Chunks are the data rows between 'polariser cell info'\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    started = False\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"polariser cell info\"):\n",
    "            if started and current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = [line]\n",
    "            started = True\n",
    "        else:\n",
    "            if started:\n",
    "                current_chunk.append(line)\n",
    "    if not started:\n",
    "        log_message(f\" File '{FileName}' does NOT contain any 'polariser cell info' header. Skipping.\\n\")\n",
    "        continue\n",
    "    else:\n",
    "        log_message(f\" File '{FileName}' contains at least one 'polariser cell info' header.\")\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    # 1.3- Save .fli files for each chunk\n",
    "    base_name = FileName.replace(\".fli\", \"\")\n",
    "    log_message(f\"\\n\\nCreating all the Array files \\n\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        fli_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "        fli_path = output_folder / fli_filename\n",
    "        with open(long_path(fli_path), \"w\") as f_out:  \n",
    "            f_out.writelines(chunk)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    started = False  # Thi is a flag to know when we found first header   \n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()  \n",
    "        # Detect header section\n",
    "        if line.startswith(\"polariser cell info\") or line.startswith(\"analyser cell info\"):\n",
    "            header_block = []\n",
    "            header_found = {\"polariser\": None, \"analyser\": None}   \n",
    "            # Collect consecutive headers, but keep only the last polariser+analyser combination rows\n",
    "            while i < len(lines) and (\n",
    "                lines[i].strip().startswith(\"polariser cell info\")\n",
    "                or lines[i].strip().startswith(\"analyser cell info\")\n",
    "            ):\n",
    "                current = lines[i].strip()\n",
    "                if current.startswith(\"polariser\"):\n",
    "                    header_found[\"polariser\"] = current\n",
    "                elif current.startswith(\"analyser\"):\n",
    "                    header_found[\"analyser\"] = current\n",
    "                header_block.append(current)\n",
    "                i += 1   \n",
    "            # Build chunk if both headers are present\n",
    "            if header_found[\"polariser\"] and header_found[\"analyser\"]:\n",
    "                new_chunk = []           \n",
    "                # Always keep order: polariser first, analyser second\n",
    "                for header in (header_found[\"polariser\"], header_found[\"analyser\"]):\n",
    "                    if not header.endswith(\"\\n\"):\n",
    "                        header += \"\\n\"\n",
    "                    new_chunk.append(header)           \n",
    "                # Collect data lines until next header\n",
    "                while i < len(lines) and not (\n",
    "                    lines[i].startswith(\"polariser cell info\")\n",
    "                    or lines[i].startswith(\"analyser cell info\")\n",
    "                ):\n",
    "                    line = lines[i]\n",
    "                    if not line.endswith(\"\\n\"):\n",
    "                        line += \"\\n\"  \n",
    "                    new_chunk.append(line)\n",
    "                    i += 1           \n",
    "                chunks.append(new_chunk)\n",
    "            else:\n",
    "                log_message(f\" File '{FileName}' skipped a block: missing polariser or analyser (headers={header_block}).\")  \n",
    "        else:\n",
    "            i += 1\n",
    "    if not chunks:\n",
    "        log_message(f\"   File '{FileName}' does NOT contain any valid polariser+analyser pair. Skipping.\\n\")\n",
    "        continue\n",
    "    else:\n",
    "        log_message(f\" File '{FileName}' contains {len(chunks)} valid experiment blocks.\")\n",
    "\n",
    "\n",
    "    # 2- Save .fli files for every correct chunk\n",
    "    base_name = FileName.replace(\".fli\", \"\")  # remove .fli for clean filenames. Otherwise those .fli extensions appear on the final names\n",
    "    log_message(f\" Creating all the Array files \\n\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        fli_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "        fli_path = output_folder / fli_filename\n",
    "        with open(long_path(fli_path), \"w\") as f_out:\n",
    "            f_out.writelines(chunk)  \n",
    "    \n",
    "    cellid_file = Path(\"AmorphousPolariserAndAnalyser_IDs.txt\")\n",
    "    try:\n",
    "        with open(long_path(cellid_file), 'r') as file:\n",
    "            seen_strings = set(line.strip() for line in file)\n",
    "    except FileNotFoundError:\n",
    "        seen_strings = set()\n",
    "    import os\n",
    "\n",
    "\n",
    "    # 3- Open each Array file and work with it (The Array file still has the header)\n",
    "    with open(ensure_file(cellid_file), 'a') as file:\n",
    "        for i in range(len(chunks)):\n",
    "            FLI_filename = f\"{base_name}_Arrays_{i}.fli\"  # Name of the Array file\n",
    "            FLI_path = output_folder / FLI_filename  \n",
    "            df = pd.read_csv(long_path(FLI_path), sep=r'\\s+', header=None, on_bad_lines='skip')\n",
    "            log_message(f\" Reading {FLI_path}, removing ***WARNING No centering scan found\")\n",
    "            warning_str = \"***WARNING No centering scan found\"\n",
    "\n",
    "\n",
    "\n",
    "            #3.1 Combine first 4 columns as strings, join them with space, and filter rows containing this phrase (it is not important for us)\n",
    "            df = df[~df.iloc[:, :5].astype(str).agg(' '.join, axis=1).str.contains('No centering scan found', regex=False)] \n",
    "            \n",
    "            #3.2 Extract useful information from the header. Hopefully, CellID, Pressure, LabPolarization, Year, Month, Day, time of lab measurement before first experiment measurement (negative time) will be stored locally\n",
    "            log_message(f\" Header Information Extraction...\")\n",
    "            PolariserID =          df.iloc[0].tolist()[3]\n",
    "            AnalyserID =           df.iloc[1].tolist()[3]\n",
    "            PolariserPressure =    df.iloc[0].tolist()[6]\n",
    "            AnalyserPressure =     df.iloc[1].tolist()[6]\n",
    "            LabPolarization = df.iloc[0].tolist()[7]\n",
    "\n",
    "            try:\n",
    "                HM, DD, MM, YY = df.iloc[0].tolist()[14], int(df.iloc[0].tolist()[10]), int(df.iloc[0].tolist()[11]), int(df.iloc[0].tolist()[12])\n",
    "                Day_Ref = f\"{DD:02d}/{MM:02d}/{YY:02d}\"\n",
    "                dt = Time(Day_Ref, HM)\n",
    "            except Exception as e:\n",
    "                log_message(f\"Skipping file {file_path} because of invalid header data: {e}\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            #3.3 All redundant/useless information is removed\n",
    "            log_message(f\" Removing Measurement Index, Unknown column, Flipping Ratio, Uncertainty of Flipping Ratio and Time between measurements,...\")\n",
    "            df = df.iloc[2:].reset_index(drop=True)\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "            df = df.drop(df.columns[5], axis=1)\n",
    "            df = df.drop(df.columns[7], axis=1)\n",
    "            df = df.drop(df.columns[7], axis=1)\n",
    "            df = df.drop(df.columns[7], axis=1)\n",
    "            #log_message(f\"Saving only polarization values for the Spin Directions wanted in both Polarizer Cells, i.e. (+z,+z)\")\n",
    "\n",
    "\n",
    "            #3.4 Convert Miller index columns into integers. From string or object to float and if the float is close to an integer (tolerance is 1e-8) then save as integer. Otherwise remove row\n",
    "            cols_to_convert = [1, 2, 3]\n",
    "            df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce').astype(float)            \n",
    "            mask = np.isclose(df[cols_to_convert], np.round(df[cols_to_convert]), atol=1e-8)\n",
    "            df = df[mask.all(axis=1)].copy()\n",
    "            log_message(f\" All irrational Miller Indices removed. Adding DeltaTime\")\n",
    "            \n",
    "            #3.5 The time columns are converted into difference of time being the referenced time the first +z,+z measurement that has survived at this point\n",
    "            if df.shape[0] < 2:\n",
    "                log_message(f\"   Not enough valid rows after filtering, skipping chunk\")\n",
    "                continue\n",
    "            df['DeltaTime'] = df.apply(\n",
    "                lambda row: deltatime(df[4].iloc[0], df[5].iloc[0], row[4], row[5]), axis=1 )\n",
    "            ref_dt = Time(df[4].iloc[0], df[5].iloc[0])\n",
    "            LabTime = int((dt - ref_dt).total_seconds())\n",
    "\n",
    "            #3.6 Rename the columns PolarizationD3, ErrPolarizationD3 (the polarization column and its uncertainty). The other one with name is DeltaTime. The rest are numbers (will be erased).\n",
    "            #Also we remove the time strings (with DeltaTime they have no new information)\n",
    "            log_message(f\" Renaming PolarizationD3 and ErrPolarizationD3\")\n",
    "            df.rename(columns={\n",
    "                df.columns[5]: 'PolarizationD3',\n",
    "                df.columns[6]: 'ErrPolarizationD3'\n",
    "            }, inplace=True)\n",
    "            df.drop(columns=[df.columns[3], df.columns[4]], inplace=True)\n",
    "            log_message(f\" Dropped Time Strings\")\n",
    "\n",
    "            \n",
    "            #3.7 Begin filtering and softening with previous functions\n",
    "            log_message(f\" Begin removal of Bad files and softening with Savitzky-Golay filter\")\n",
    "            filtered_df, PrettyCombination = filter_best_combination(i,df)\n",
    "            #If nothing survived the filters/purge then use'continue' and go for the next experiment\n",
    "            if filtered_df is None and PrettyCombination is None:\n",
    "                log_message(f\" Chunk {i}: No suitable combination found. Skipping to next chunk or file.\")\n",
    "                log_message(f\"_______________________________________________________________\\n\")\n",
    "                continue  # skip to next chunk\n",
    "            \n",
    "            #3.8 Removal of Miller indices (we have all the information they could give us)\n",
    "            log_message(f\" Removing Miller Indices columns\")\n",
    "            #log_message(filtered_df)\n",
    "            filtered_df = filtered_df.iloc[:, 3:]\n",
    "            desired_order = [\"DeltaTime\", \"PolarizationD3\", \"SoftPolarizationD3\", \"ErrPolarizationD3\"]\n",
    "\n",
    "            \n",
    "            #3.9 Remove the points that won't be useful for the ML algorythm\n",
    "            columns_to_save = [col for col in desired_order if col in filtered_df.columns] # Keep only the columns that actually exist (in case something is missing)\n",
    "            df_SEMIFINAL = filtered_df[columns_to_save].copy()\n",
    "            df_FINAL = filtered_df = RemoveOutcast_FixUncertainty(df_SEMIFINAL, PrettyCombination, filename=f\"PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}\", AcceptableMultiplier=1.3)\n",
    "\n",
    "            #3.10 Plot the succesful experiments\n",
    "            log_message(f\" Plot of Data. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            T = pd.to_numeric(df_FINAL[\"DeltaTime\"], errors='coerce')\n",
    "            P = pd.to_numeric(df_FINAL[\"PolarizationD3\"], errors='coerce')\n",
    "            Err = pd.to_numeric(df_FINAL[\"ErrPolarizationD3\"], errors='coerce')\n",
    "            P_soft = pd.to_numeric(df_FINAL[\"SoftPolarizationD3\"], errors='coerce')\n",
    "            \n",
    "            plt.scatter(T, P, linewidth=1, label='Original') \n",
    "\n",
    "            plt.plot(T, P, linestyle='--', color='blue', alpha=0.7)\n",
    "            plt.scatter(T, P, linewidth=1, label='Original') \n",
    "\n",
    "            plt.plot(T, P, linestyle='--', color='blue', alpha=0.7)\n",
    "            plt.errorbar(T, P, yerr=Err, fmt='none', ecolor='gray', alpha=0.5)\n",
    "\n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"PolarizationD3\")\n",
    "            plot_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}.png\"\n",
    "            plt.title(plot_filename)\n",
    "            plt.ylim(np.min(P - Err), np.max(P + Err))\n",
    "            plt.yticks(np.linspace(np.min(P - Err), np.max(P + Err), 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plot_path = output_folder / plot_filename\n",
    "            plt.savefig(long_path(plot_path), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            log_message(f\" Plot of Filtered Data. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Softened\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "\n",
    "            plt.scatter(T, P_soft, linewidth=1, label='Filtered')\n",
    "            plt.plot(T, P_soft, linestyle='--', color='green', alpha=0.7)\n",
    "            plt.errorbar(T, P_soft, yerr=Err, fmt='none', ecolor='gray', alpha=0.5)\n",
    "\n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"SoftPolarizationD3\")\n",
    "            plot_filename_soft = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Softened.png\"\n",
    "            plt.title(plot_filename_soft)\n",
    "            plt.ylim(np.min(P_soft - Err), np.max(P_soft + Err))\n",
    "            plt.yticks(np.linspace(np.min(P_soft - Err), np.max(P_soft + Err), 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plot_path_soft = output_folder / plot_filename_soft\n",
    "            plt.savefig(long_path(plot_path_soft), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            log_message(f\" Comparison Plot. PolarizationD3_{folder_name}_{DD}/{MM}/{YY}_{i}_MillerIndex_{PrettyCombination}_Comparison\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "\n",
    "            plt.scatter(T, P, linewidth=1, color='blue', alpha=0.6, label='Original')\n",
    "            plt.plot(T, P, linestyle='--', color='blue', alpha=0.5)\n",
    "\n",
    "            plt.scatter(T, P_soft, linewidth=1, color='green', alpha=0.6, label='Filtered')\n",
    "            plt.plot(T, P_soft, linestyle='--', color='green', alpha=0.5)\n",
    "\n",
    "            plt.errorbar(T, P, yerr=Err, fmt='none', ecolor='gray', alpha=0.3)\n",
    "            plt.errorbar(T, P_soft, yerr=Err, fmt='none', ecolor='gray', alpha=0.3)\n",
    "\n",
    "            plt.xlabel(\"DeltaTime\")\n",
    "            plt.ylabel(\"Polarization\")\n",
    "            plot_filename_combined = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Combined.png\"\n",
    "            plt.title(plot_filename_combined)\n",
    "            min_y = min(np.min(P - Err), np.min(P_soft - Err))\n",
    "            max_y = max(np.max(P + Err), np.max(P_soft + Err))\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(min_y, max_y)\n",
    "            plt.yticks(np.linspace(min_y, max_y, 10))\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plot_path_combined = output_folder / plot_filename_combined\n",
    "            plt.savefig(long_path(plot_path_combined), dpi=300, bbox_inches='tight')\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "            # Save CSV and parameter files\n",
    "            log_message(f\" Saving chunk data and parameters\")\n",
    "            csv_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}.txt\"\n",
    "            parameter_filename = f\"PolarizationD3_{folder_name}_{DD}_{MM}_{YY}_{i}_MillerIndex_{PrettyCombination}_Parameters.txt\"\n",
    "            csv_path = output_folder / csv_filename\n",
    "            ml_csv_path = MLDataBaseFolder / csv_filename\n",
    "            ml_parameter_path = MLDataBaseFolder / parameter_filename\n",
    "            \n",
    "            df_FINAL['DeltaTime'] = df_FINAL['DeltaTime'] - df_FINAL['DeltaTime'].iloc[0]\n",
    "            df_FINAL.to_csv(csv_path, index=False, sep=',')\n",
    "            df_FINAL.to_csv(ml_csv_path, index=False, sep=',')\n",
    "            log_message(f\" Saved CSV: {csv_filename}\")\n",
    "            \n",
    "            # Save parameter file\n",
    "            with ml_parameter_path.open('w', encoding='utf-8', errors='replace') as f:\n",
    "                f.write(\"CellID,AnalyserID,PolariserPressure,AnalyserPressure,LabPolarization,LabTime\\n\")\n",
    "                f.write(f\"{PolariserID},{AnalyserID},{PolariserPressure},{AnalyserPressure},{LabPolarization},{LabTime}\")\n",
    "            log_message(f\" Saved Parameters: {parameter_filename}\")\n",
    "            log_message(f\" Parameter and Array files saved to ML database: {MLDataBaseFolder}\\n{'_'*65}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0344d-635e-4d44-be9b-1da30a3127f5",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "\n",
    "It erases all intermediate files and prepares the remaining ones for the ML pipeline\n",
    "\n",
    "1. Removes all .fli files that have been created.\n",
    "2. Removes empty folders\n",
    "3. Collects all unique polariser–analyser ID pairs\n",
    " \n",
    "As a result, the only useful files are _AmorphousPolariserAndAnalyser_IDs.txt_ and the folder _AmorphousMLDataBase_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd9b46-735b-4506-9f31-e0ed1acdedec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove unwanted folders and files\n",
    "for i in range(len(chunks)):\n",
    "    temp_filename = f\"{base_name}_Arrays_{i}.fli\"\n",
    "    temp_path = output_folder / temp_filename\n",
    "    try:\n",
    "        temp_path.unlink()  # delete the file\n",
    "    except FileNotFoundError:\n",
    "        pass       \n",
    "\n",
    "log_message(f\"Created and saved {len(chunks)} CSV files from file called {FileName}.\")\n",
    "\n",
    "# Remove folder if empty\n",
    "if output_folder.exists() and not any(output_folder.iterdir()):\n",
    "    output_folder.rmdir()\n",
    "    log_message(f\"Removed empty folder: {output_folder}\")\n",
    "log_message('\\n\\n')\n",
    "\n",
    "# Collect unique polariser/analyser ID pairs because they are need for the ML .ipynb file\n",
    "ml_database_folder = Path(\"AmorphousMLDataBase\")\n",
    "parameter_files = list(ml_database_folder.glob(\"*Parameters.txt\"))\n",
    "log_message(f\"Found {len(parameter_files)} parameter files.\")\n",
    "\n",
    "unique_id_pairs = set()\n",
    "for filepath in parameter_files:\n",
    "    try:\n",
    "        with filepath.open('r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 2:\n",
    "                second_row = lines[1].strip()\n",
    "                parts = second_row.split(',')\n",
    "                if len(parts) >= 2:\n",
    "                    polariser_id, analyser_id = parts[0], parts[1]\n",
    "                    unique_id_pairs.add((polariser_id, analyser_id))\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to read {filepath}: {e}\")\n",
    "\n",
    "unique_ids_path = Path(\"AmorphousPolariserAndAnalyser_IDs.txt\")\n",
    "with unique_ids_path.open('w', encoding='utf-8') as f:\n",
    "    for polariser_id, analyser_id in sorted(unique_id_pairs):\n",
    "        f.write(f\"{polariser_id},{analyser_id}\\n\")\n",
    "log_message(f\"Saved {len(unique_id_pairs)} unique polariser/analyser ID pairs to {unique_ids_path}.\")\n",
    "\n",
    "# Delete intermediate folders \n",
    "for folder_name in [\"AmorphousSeparatedFolder\", \"AmorphousDataBase\"]:\n",
    "    folder_path = Path(folder_name)\n",
    "    if folder_path.exists():\n",
    "        shutil.rmtree(folder_path)\n",
    "        log_message(f\"Folder '{folder_path}' has been deleted.\")\n",
    "    else:\n",
    "        log_message(f\"Folder '{folder_path}' does not exist.\")\n",
    "\n",
    "#Remove duplicate ML database files \n",
    "hash_map = defaultdict(list)\n",
    "\n",
    "def file_sha256(filepath: Path, block_size=65536) -> str:\n",
    "    \"\"\"Compute SHA256 hash of a file (safe for large files).\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with filepath.open(\"rb\") as f:\n",
    "        while chunk := f.read(block_size):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "# Scan all base .txt files (skip '_Parameters' files)\n",
    "for txt_file in ml_database_folder.glob(\"*.txt\"):\n",
    "    if \"_parameters\" not in txt_file.stem.lower():\n",
    "        file_hash = file_sha256(txt_file)\n",
    "        hash_map[file_hash].append(txt_file)\n",
    "\n",
    "# Report & delete duplicates\n",
    "duplicates_found = False\n",
    "for file_hash, paths in hash_map.items():\n",
    "    if len(paths) > 1:\n",
    "        duplicates_found = True\n",
    "        log_message(f\"\\nDuplicate group (hash={file_hash}):\")\n",
    "        log_message(f\"   Keeping: {paths[0]}\")\n",
    "\n",
    "        # Delete all but the first file\n",
    "        for p in paths[1:]:\n",
    "            param_file = p.with_name(f\"{p.stem}_Parameters{p.suffix}\")\n",
    "            try:\n",
    "                p.unlink()\n",
    "                log_message(f\"   Deleted duplicate base file: {p}\")\n",
    "            except Exception as e:\n",
    "                log_message(f\"   Could not delete base file {p}: {e}\")\n",
    "            if param_file.exists():\n",
    "                try:\n",
    "                    param_file.unlink()\n",
    "                    log_message(f\"   Deleted parameter file: {param_file}\")\n",
    "                except Exception as e:\n",
    "                    log_message(f\"   Could not delete parameter file {param_file}: {e}\")\n",
    "\n",
    "if not duplicates_found:\n",
    "    log_message(\"No duplicates found in MLDataBase!\")\n",
    "else:\n",
    "    log_message(\"\\nDuplicate cleanup complete!\")\n",
    "\n",
    "\n",
    "# Remove AmorphousCell_ID.txt if exists\n",
    "file_path = Path(\"AmorphousCell_ID.txt\")\n",
    "if file_path.exists():\n",
    "    file_path.unlink()\n",
    "    log_message(f\"{long_path(file_path)} has been deleted.\")\n",
    "else:\n",
    "    log_message(f\"{long_path(file_path)} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ee3c9-8cbd-4864-93fa-d583f9cd83d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
