{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4883b95c-2915-473b-a8ab-4c7c474ba96b",
   "metadata": {},
   "source": [
    "<h1> LTestChiSquaredTests.ipynb </h1>\n",
    "\n",
    "This code is prepared so that it condenses all the information from the code files inside TestAModelFolder/ML. \n",
    "\n",
    "First copy and paste the folders that CrystallineMLAlone.ipynb and AmorphousMLAlone.ipynb have created inside LTestVisualCheck (at the same level as the ipynb files).\n",
    "Then run the first cell of LTestChiSquaredTests.ipynb if you want to obtain information using the chi squared metric. This cell will create a subfolder called ChiSquared where only three files are important:\n",
    " - Chi2_rank_vertical.png For every experiment, the chi squared value of the preditions of each model is obtained. Then all chi squared values of the different models in each experiment is compared. The one that has the smaller chi squared value obtains a +1, the second smallest a +2 and so on. This process is repeated for each experiment and these integer values are summed. At the end we can say that the model with the smallest score has performed better overall\n",
    " - Chi2_AccumulativeScores.txt has the same information than the graph but on a txt file\n",
    " - Chi2_NumberOfExperimentsWhereEachModelOutperforms.txt as its name indicates counts only the number of experiment where each model obtains the smallest chi squared value. Sometimes, models that overfit perform better on this test (as they pass through all the points)\n",
    "\n",
    "Run the second folder if you want the same information using a metric similar to the Mean Average Error (MAE). Instead of summing over the differences squared and divided by the uncertainty of the prediction squared, here we are summing over the absolute value of the differences and divided by the uncertainty (not squared). By running this cell you create the subfolder Lvalues that contains the same type of information (Lvalues_rank_vertical.png, Lvalues_TotalScores.txt with the information of the graph and Lvalues_WinnerScores which is the equivalent version of Chi2_NumberOfExperimentsWhereEachModelOutperforms.txt but with The L metric\n",
    "\n",
    "If you run the third one it will automatically combine the predictions for twelve models for each experiment in a single image. This way it is easier to spot the differences between models in a quick look. The code also orders them in order of complexity (each consecutive row has a bigger complexity) and number of augmentations (each consecutive column ahs more augmentation). This way the models are organized from simpler (top left) to more complex (bottom right)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008a2a8-b8df-4b2a-8d3d-61a2fc3f7e25",
   "metadata": {},
   "source": [
    "<H1> CHI SQUARED TEST </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957eaa3-a94f-4c5f-8ad6-000aa7b19d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def long_path(path):\n",
    "    \"\"\"Return Windows long path format if on Windows.\"\"\"\n",
    "    if os.name == 'nt':\n",
    "        path = os.path.abspath(path)\n",
    "        if not path.startswith('\\\\\\\\?\\\\'):\n",
    "            return '\\\\\\\\?\\\\' + path\n",
    "    return path\n",
    "def compute_chi2_for_experiment(df_real, df_pred):\n",
    "    \"\"\"\n",
    "    Compute the reduced chi-squared value for one experiment:\n",
    "    \n",
    "        χ² = (1/N) * Σ [ (pred - real)² / (err_real)² ]\n",
    "    \"\"\"\n",
    "    if len(df_real) != len(df_pred):\n",
    "        raise ValueError(\"Real and predicted files must have the same length.\")\n",
    "    \n",
    "    N = len(df_real)\n",
    "    diff = df_pred[\"PredictedPolarizationD3\"].values - df_real[\"RealPolarizationD3\"].values\n",
    "    err = df_real[\"ErrRealPolarizationD3\"].values\n",
    "    err[err == 0] = 1e-10  # avoid division by zero\n",
    "    \n",
    "    chi2 = np.sum((diff**2) / (err**2)) / N\n",
    "    return chi2\n",
    "\n",
    "\n",
    "def load_data(experiment_folder):\n",
    "    \"\"\"\n",
    "    Load the txt files containing real and predicted data\n",
    "    for a MissingPolarizationD3_* experiment.\n",
    "    \"\"\"\n",
    "    experiment_folder = long_path(experiment_folder)\n",
    "    files = [long_path(os.path.join(experiment_folder, f)) for f in os.listdir(experiment_folder)]\n",
    "\n",
    "    real_file = next(f for f in files if os.path.basename(f).startswith(\"RawData_PolarizationD3_MissingPolarizationD3_\"))\n",
    "    pred_file = next(f for f in files if os.path.basename(f).startswith(\"PredictedPoints_PolarizationD3_MissingPolarizationD3_\"))\n",
    "\n",
    "    df_real = pd.read_csv(real_file, sep=\"\\t\")\n",
    "    df_pred = pd.read_csv(pred_file, sep=\"\\t\")\n",
    "    return df_real, df_pred\n",
    "\n",
    "\n",
    "def count_missing_folders(model_path):\n",
    "    \"\"\"\n",
    "    Return the list of MissingPolarizationD3_* subfolders for a given model folder.\n",
    "    \"\"\"\n",
    "    model_path = long_path(model_path)\n",
    "    return sorted([\n",
    "        f for f in os.listdir(model_path)\n",
    "        if f.startswith(\"MissingPolarizationD3_\") and os.path.isdir(long_path(os.path.join(model_path, f)))\n",
    "    ])\n",
    "\n",
    "\n",
    "def save_chi2_values_to_txt(model_folder_name, missing_folders, chi2_values, output_dir):\n",
    "    \"\"\"\n",
    "    Save the chi-squared values to a structured text file\n",
    "    inside the 'ChiSquared' folder at the root level.\n",
    "    \"\"\"\n",
    "    chi2_dir = long_path(os.path.join(output_dir, \"ChiSquared\"))\n",
    "    os.makedirs(chi2_dir, exist_ok=True)  # ensure folder exists\n",
    "\n",
    "    filename = f\"Chi2_values_{model_folder_name}.txt\"\n",
    "    filepath = long_path(os.path.join(chi2_dir, filename))\n",
    "\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for folder_name, chi2 in zip(missing_folders, chi2_values):\n",
    "            f.write(f\"{folder_name}: {chi2:.6f}\\n\")\n",
    "\n",
    "    print(f\"File saved: {filepath}\")\n",
    "\n",
    "\n",
    "def main_all_models(root_folder):\n",
    "    \"\"\"\n",
    "    Main loop over all model folders in the given root_folder.\n",
    "    Each folder should contain MissingPolarizationD3_* subfolders.\n",
    "    \"\"\"\n",
    "    root_folder = long_path(root_folder)\n",
    "    all_model_folders = [\n",
    "        d for d in os.listdir(root_folder)\n",
    "        if os.path.isdir(long_path(os.path.join(root_folder, d)))\n",
    "    ]\n",
    "\n",
    "    if not all_model_folders:\n",
    "        print(\"No model folders found.\")\n",
    "        return\n",
    "\n",
    "    for model_folder in all_model_folders:\n",
    "        model_path = long_path(os.path.join(root_folder, model_folder))\n",
    "        missing_folders = count_missing_folders(model_path)\n",
    "\n",
    "        if not missing_folders:\n",
    "            print(f\" No MissingPolarizationD3_ folders in {model_folder}\")\n",
    "            continue\n",
    "\n",
    "        chi2_values = []\n",
    "        for missing_folder in missing_folders:\n",
    "            experiment_path = long_path(os.path.join(model_path, missing_folder))\n",
    "            try:\n",
    "                df_real, df_pred = load_data(experiment_path)\n",
    "                chi2 = compute_chi2_for_experiment(df_real, df_pred)\n",
    "                chi2_values.append(chi2)\n",
    "            except Exception as e:\n",
    "                print(f\"   Error in {missing_folder}: {e}\")\n",
    "\n",
    "        if chi2_values:\n",
    "            save_chi2_values_to_txt(model_folder, missing_folders, chi2_values, root_folder)\n",
    "            chi2_mean = np.mean(chi2_values)\n",
    "        else:\n",
    "            print(f\"No valid χ² values computed for {model_folder}.\")\n",
    "\n",
    "\n",
    "# === Example call ===\n",
    "main_all_models(r\".\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Folder that contains the saved chi-squared text files\n",
    "folder_path = long_path(\"ChiSquared\")\n",
    "\n",
    "# Quick diagnostic\n",
    "if not os.path.isdir(folder_path):\n",
    "    print(f\"Folder not found: {folder_path}\")\n",
    "    print(\"   Tip: run the computation cell first so it creates ChiSquared/ and the files.\")\n",
    "    \n",
    "# Helper to recognize result files from either naming style\n",
    "def is_result_file(name: str) -> bool:\n",
    "    return (\n",
    "        name.endswith(\".txt\") and\n",
    "        (name.startswith(\"Chi2_values_\") or name.startswith(\"ChiSquared_values_\"))\n",
    "    )\n",
    "\n",
    "# Initialize dictionaries\n",
    "Chi2Values = {}             \n",
    "ExperimentName = OrderedDict() \n",
    "\n",
    "# Collect files\n",
    "result_files = []\n",
    "if os.path.isdir(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        filename = long_path(os.path.join(folder_path, filename))  # wrap early\n",
    "        if is_result_file(os.path.basename(filename)):\n",
    "            result_files.append(filename)\n",
    "\n",
    "# Sort for stable order\n",
    "result_files.sort()\n",
    "\n",
    "if not result_files:\n",
    "    print(\"No results found in ChiSquared/ matching 'Chi2_values_*.txt' or 'ChiSquared_values_*.txt'.\")\n",
    "else:\n",
    "    for full_path in result_files:\n",
    "        with open(full_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        Chi2Values[os.path.basename(full_path)] = {}\n",
    "        for line in lines:\n",
    "            if \":\" in line:\n",
    "                try:\n",
    "                    experiment_name, value = line.split(\":\", 1)\n",
    "                    experiment_name = experiment_name.strip()\n",
    "                    value = float(value.strip())\n",
    "                    Chi2Values[os.path.basename(full_path)][experiment_name] = value\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping malformed line in {full_path}: {line.strip()}\")\n",
    "\n",
    "        # Fill ExperimentName only from the first processed file\n",
    "        if not ExperimentName and Chi2Values[os.path.basename(full_path)]:\n",
    "            for i, experiment_name in enumerate(Chi2Values[os.path.basename(full_path)].keys(), 1):\n",
    "                ExperimentName[experiment_name] = f\"exp{i}\"\n",
    "\n",
    "# === Plotting ===\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "exp_ids = list(ExperimentName.values())\n",
    "assert len(Chi2Values) >= 2, \"You need at least two Chi² values files\"\n",
    "files = list(Chi2Values.keys())\n",
    "\n",
    "try:\n",
    "    cmap = plt.colormaps.get_cmap('tab20')  # new API\n",
    "except AttributeError:\n",
    "    cmap = plt.get_cmap('tab20')  # old API\n",
    "\n",
    "colors = cmap.colors\n",
    "line_styles = ['-', '--', ':', '-.', (0, (5, 1)), (0, (3, 1, 1, 1))]\n",
    "line_widths = [2] * len(files)\n",
    "\n",
    "for j, filename in enumerate(files):\n",
    "    chi2_values = [\n",
    "        Chi2Values[filename].get(exp_name, np.nan)  # Use NaN if missing\n",
    "        for exp_name in ExperimentName.keys()\n",
    "    ]\n",
    "    \n",
    "    plt.plot(\n",
    "        range(len(exp_ids)),\n",
    "        chi2_values,\n",
    "        color=colors[j % len(colors)],\n",
    "        linestyle=line_styles[j % len(line_styles)],\n",
    "        linewidth=line_widths[j],\n",
    "        label=filename\n",
    "    )\n",
    "\n",
    "xticks_pos = [i for i in range(len(exp_ids)) if (i + 1) % 10 == 0]\n",
    "xticks_labels = [exp_ids[i] for i in xticks_pos]\n",
    "plt.xticks(xticks_pos, xticks_labels, rotation=45, ha='right')\n",
    "\n",
    "plt.xlabel(\"Experiment\", fontsize=12)\n",
    "plt.ylabel(r\"$\\chi^2$\", fontsize=12)\n",
    "plt.title(r\"Chi-squared values per experiment (model comparison)\", fontsize=14)\n",
    "plt.grid(alpha=0.2, linestyle='--')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Normalize keys (replace \"-\" by \"_\")\n",
    "def normalize_key(k):\n",
    "    return k.replace(\"-\", \"_\")\n",
    "\n",
    "Chi2Values_norm = {\n",
    "    model: {normalize_key(exp): val for exp, val in Chi2Values[model].items()}\n",
    "    for model in Chi2Values\n",
    "}\n",
    "\n",
    "\n",
    "# --- Normalize experiment names mapping ---\n",
    "ExperimentName_norm = {normalize_key(exp): exp for exp in ExperimentName.keys()}\n",
    "\n",
    "# Build 2D array: rows = models, cols = experiments\n",
    "all_chi2_values = np.array([\n",
    "    [Chi2Values_norm[f].get(exp, np.nan) for exp in ExperimentName_norm.keys()]\n",
    "    for f in Chi2Values_norm.keys()\n",
    "])\n",
    "\n",
    "# Preserve the original experiment order (exp1, exp2, ...)\n",
    "exp_ids = [ExperimentName_norm[exp] for exp in ExperimentName_norm.keys()]\n",
    "model_ids = list(Chi2Values_norm.keys())  # rows of all_chi2_values\n",
    "\n",
    "\n",
    "# --- Build Chi² matrix ---\n",
    "files = list(Chi2Values.keys())   # Model names (files)\n",
    "nb_models = len(files)\n",
    "exp_ids = list(ExperimentName.keys())  # Experiments in original order\n",
    "\n",
    "all_chi2_values = []\n",
    "for f in files:\n",
    "    model_vals = []\n",
    "    for exp in exp_ids:\n",
    "        if exp not in Chi2Values[f]:\n",
    "            print(f\"Warning: experiment '{exp}' missing for model '{f}'\")\n",
    "            model_vals.append(np.nan)  # fill missing with NaN\n",
    "        else:\n",
    "            model_vals.append(Chi2Values[f][exp])\n",
    "    all_chi2_values.append(model_vals)\n",
    "all_chi2_values = np.array(all_chi2_values)\n",
    "\n",
    "# --- Compute average Chi² across models for each experiment ---\n",
    "avg_chi2 = np.nanmean(all_chi2_values, axis=0)  # ignore NaN when averaging\n",
    "\n",
    "# --- Colors and line styles ---\n",
    "try:\n",
    "    cmap = plt.colormaps['tab20']   # new API (>=3.5)\n",
    "except (AttributeError, TypeError):\n",
    "    cmap = plt.get_cmap('tab20')    # old API\n",
    "\n",
    "colors = cmap(range(nb_models))\n",
    "\n",
    "# --- Plot deviation from the average ---\n",
    "plt.figure(figsize=(18, 12))\n",
    "for j in range(nb_models):\n",
    "    diff_from_avg = all_chi2_values[j] - avg_chi2\n",
    "    plt.plot(\n",
    "        range(1, len(exp_ids) + 1),  # experiments indexed starting at 1\n",
    "        diff_from_avg,\n",
    "        linewidth=2,\n",
    "        color=colors[j % len(colors)],\n",
    "        label=files[j]\n",
    "    )\n",
    "\n",
    "xticks_pos = [k + 1 for k in range(len(exp_ids)) if (k + 1) % 10 == 0]\n",
    "plt.xticks(xticks_pos, rotation=0)\n",
    "plt.xlabel(\"Experiment Index\", fontsize=12)\n",
    "plt.ylabel(\"ΔChi² (model - average)\", fontsize=12)\n",
    "plt.title(\"Deviation of Chi² values from experiment-wise average\", fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Save number of wins (lowest Chi² per experiment) ---\n",
    "output_folder = long_path(\"ChiSquared\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "model_scores = {f: 0 for f in files}\n",
    "for chi2_vals in all_chi2_values.T:\n",
    "    if np.all(np.isnan(chi2_vals)):\n",
    "        continue\n",
    "    min_index = np.nanargmin(chi2_vals)\n",
    "    winning_model = files[min_index]\n",
    "    model_scores[winning_model] += 1\n",
    "\n",
    "sorted_scores = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "output_file = long_path(os.path.join(output_folder, \"Chi2_NumberOfExperimentsWhereEachModelOutperforms.txt\"))\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Approval Rate (Number of wins by minimal Chi². The higher the better):\\n\\n\")\n",
    "    for model, score in sorted_scores:\n",
    "        f.write(f\"{model}: {score} points\\n\")\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "\n",
    "# --- Save rank-based accumulative scores ---\n",
    "all_chi2_values = []\n",
    "for f in files:\n",
    "    model_chi2 = []\n",
    "    for exp in exp_ids:\n",
    "        model_chi2.append(Chi2Values[f].get(exp, np.nan))\n",
    "    all_chi2_values.append(model_chi2)\n",
    "all_chi2_values = np.array(all_chi2_values)\n",
    "\n",
    "model_scores = {f: 0 for f in files}\n",
    "for chi_values in all_chi2_values.T:\n",
    "    sorted_indices = np.argsort(chi_values)  # best to worst\n",
    "    for rank, model_idx in enumerate(sorted_indices, start=1):\n",
    "        model_name = files[model_idx]\n",
    "        model_scores[model_name] += rank\n",
    "\n",
    "sorted_scores = sorted(model_scores.items(), key=lambda x: x[1])\n",
    "\n",
    "output_file = long_path(os.path.join(output_folder, \"Chi2_AccumulativeScores.txt\"))\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Approval Rate (Score by rank. Lower is better):\\n\\n\")\n",
    "    for model, score in sorted_scores:\n",
    "        f.write(f\"{model}: {score} points\\n\")\n",
    "print(f\"Scores saved to {output_file}\")\n",
    "\n",
    "\n",
    "# --- Ensure output folder exists for plots ---\n",
    "output_folder = long_path(\"ChiSquared\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "model_scores = {f: 0.0 for f in files}\n",
    "\n",
    "# For each experiment (iterate over columns of the chi² matrix)\n",
    "for chi2_vals in all_chi2_values.T:\n",
    "    if np.all(np.isnan(chi2_vals)):\n",
    "        continue\n",
    "    min_chi2 = np.nanmin(chi2_vals)\n",
    "    for idx, val in enumerate(chi2_vals):\n",
    "        if np.isnan(val):\n",
    "            score = 0.0\n",
    "        else:\n",
    "            score = min_chi2 / val  # relative to best\n",
    "        model_name = files[idx]\n",
    "        model_scores[model_name] += score\n",
    "\n",
    "# Normalize by number of experiments\n",
    "for model in model_scores:\n",
    "    model_scores[model] /= len(exp_ids)\n",
    "\n",
    "# Sort models (higher = better)\n",
    "sorted_scores = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# --- Save results ---\n",
    "output_file = long_path(os.path.join(output_folder, \"Chi2_RelativeNormalizedScores.txt\"))\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Relative Score (Normalized Chi² per experiment. Higher is better):\\n\\n\")\n",
    "    for model, score in sorted_scores:\n",
    "        f.write(f\"{model}: {score:.4f}\\n\")\n",
    "print(f\"Normalized relative scores saved to {output_file}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Vertical bar chart: Rank-based scoring (lower = better)\n",
    "# -------------------------------\n",
    "sorted_scores_rank = sorted(model_scores.items(), key=lambda x: x[1])\n",
    "models = [m for m, _ in sorted_scores_rank]\n",
    "scores = [s for _, s in sorted_scores_rank]\n",
    "\n",
    "norm = plt.Normalize(min(scores), max(scores))\n",
    "try:\n",
    "    cmap = plt.colormaps['RdYlGn_r']\n",
    "except (AttributeError, TypeError):\n",
    "    cmap = plt.get_cmap('RdYlGn_r')\n",
    "\n",
    "colors = cmap(norm(scores))\n",
    "short_models = [m.replace(\"Chi2_values_AllTestsFolder_\", \"\").replace(\".txt\", \"\") for m in models]\n",
    "short_models = [\"_\".join(m.split(\"_\")[-2:]) for m in short_models]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(short_models, scores, color=colors)\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        score + score * 0.02,\n",
    "        f\"{score:.3f}\".replace(',', '.'),\n",
    "        ha='center', va='bottom',\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array(scores)\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Rank Score (Lower = Better)\")\n",
    "\n",
    "ax.set_ylabel(\"Total Rank Score\", fontsize=12)\n",
    "ax.set_xlabel(\"Model\", fontsize=12)\n",
    "ax.set_title(\"Approval Rate: Rank-Based Scoring (Chi², Green=Better, Red=Worse)\", fontsize=14)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(long_path(os.path.join(output_folder, \"Chi2_rank_vertical.png\")), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Vertical bar chart: Relative normalized scoring (higher = better)\n",
    "# -------------------------------\n",
    "sorted_scores_rel = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "models = [m for m, _ in sorted_scores_rel]\n",
    "scores = [s for _, s in sorted_scores_rel]\n",
    "\n",
    "short_models = [m.replace(\"Chi2_values_AllTestsFolder_\", \"\").replace(\".txt\", \"\") for m in models]\n",
    "short_models = [\"_\".join(m.split(\"_\")[-2:]) for m in short_models]\n",
    "\n",
    "norm = plt.Normalize(min(scores), max(scores))\n",
    "try:\n",
    "    cmap = plt.colormaps['RdYlGn']\n",
    "except (AttributeError, TypeError):\n",
    "    cmap = plt.get_cmap('RdYlGn')\n",
    "\n",
    "colors = cmap(norm(scores))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars = ax.bar(short_models, scores, color=colors, edgecolor='black')\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        score + score * 0.02,\n",
    "        f\"{score:.3f}\".replace(',', '.'),\n",
    "        ha='center', va='bottom',\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array(scores)\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Relative Score (Higher = Better)\")\n",
    "\n",
    "ax.set_ylabel(\"Total Relative Score (averaged over experiments)\", fontsize=12)\n",
    "ax.set_xlabel(\"Model\", fontsize=12)\n",
    "ax.set_title(\"Approval Rate: Normalized Relative Scoring (Chi², Green = Best, Red = Worst)\", fontsize=14)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "ax.set_ylim(0, max(scores) * 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(long_path(os.path.join(output_folder, \"Chi2_relative_vertical.png\")), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23293ae4-8bea-4699-af93-83eb1128a480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pair models with scores\n",
    "model_scores = list(zip(short_models, scores))\n",
    "\n",
    "# Sort by score (ascending)\n",
    "model_scores_sorted = sorted(model_scores, key=lambda x: x[1])\n",
    "\n",
    "# Print sorted names with scores\n",
    "print(\"Models sorted by score (lowest → highest):\")\n",
    "i=0\n",
    "for model, score in model_scores_sorted:\n",
    "    i=i+1\n",
    "    print(f\"{model}: {score:.3f}\")\n",
    "# Unzip sorted models and scores\n",
    "print(i)\n",
    "sorted_models, sorted_scores = zip(*model_scores_sorted)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Pair models with scores\n",
    "model_scores = list(zip(short_models, scores))\n",
    "\n",
    "# Sort by score (ascending → worst at top, best at bottom)\n",
    "model_scores_sorted = sorted(model_scores, key=lambda x: x[1])\n",
    "\n",
    "# Unzip\n",
    "sorted_models, sorted_scores = zip(*model_scores_sorted)\n",
    "\n",
    "# Define colormap (red → yellow → green)\n",
    "cmap = cm.get_cmap(\"RdYlGn\")\n",
    "norm = mcolors.Normalize(vmin=min(sorted_scores), vmax=max(sorted_scores))\n",
    "\n",
    "# Assign colors based on scores\n",
    "colors = [cmap(norm(s)) for s in sorted_scores]\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, len(sorted_models) * 0.15))\n",
    "\n",
    "# Plot horizontal bars with mapped colors\n",
    "bars = ax.barh(sorted_models, sorted_scores, color=colors, edgecolor=\"black\")\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"Total Relative Score\", fontsize=12)\n",
    "ax.set_ylabel(\"Model\", fontsize=12)\n",
    "ax.set_title(\"Approval Rate: Sorted Scores (Red = Worst, Green = Best)\", fontsize=14)\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Make tick labels smaller\n",
    "ax.tick_params(axis=\"x\", labelsize=9)\n",
    "ax.tick_params(axis=\"y\", labelsize=7)\n",
    "\n",
    "# Add score labels at the end of each bar\n",
    "for bar, score in zip(bars, sorted_scores):\n",
    "    ax.text(\n",
    "        bar.get_width() + max(sorted_scores) * 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{score:.3f}\".replace(',', '.'),\n",
    "        va=\"center\", ha=\"left\", fontsize=8\n",
    "    )\n",
    "\n",
    "# Remove top/bottom empty space\n",
    "ax.set_ylim(-0.5, len(sorted_models) - 0.5)\n",
    "\n",
    "# Extend X-axis so labels fit\n",
    "ax.set_xlim(0, max(sorted_scores) * 1.1)\n",
    "\"\"\"\n",
    "# Add colorbar legend\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Relative Score (Higher = Better)\")\n",
    "\"\"\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"FullVerticalChiCrystalline.png\", dpi=900, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b42e25-3a48-4e50-b529-60f45d038d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Helper: extract numeric suffix\n",
    "def extract_number(model_name):\n",
    "    match = re.search(r'_(\\d+)$', model_name)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "# Pair models with scores\n",
    "model_scores = list(zip(short_models, scores))\n",
    "\n",
    "# Sort by numeric suffix (ascending: 1 → N)\n",
    "model_scores_sorted = sorted(model_scores, key=lambda x: extract_number(x[0]))\n",
    "\n",
    "# Unzip into separate lists\n",
    "sorted_models, sorted_scores = zip(*model_scores_sorted)\n",
    "\n",
    "# Define colormap (red → yellow → green)\n",
    "cmap = cm.get_cmap(\"RdYlGn\")\n",
    "norm = mcolors.Normalize(vmin=min(sorted_scores), vmax=max(sorted_scores))\n",
    "\n",
    "# Assign colors based on scores\n",
    "colors = [cmap(norm(s)) for s in sorted_scores]\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, len(sorted_models) * 0.5))\n",
    "\n",
    "# Plot horizontal bars with mapped colors\n",
    "bars = ax.barh(sorted_models, sorted_scores, color=colors, edgecolor=\"black\")\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"Total Relative Score\", fontsize=12)\n",
    "ax.set_ylabel(\"Model\", fontsize=12)\n",
    "ax.set_title(\"Approval Rate: Ordered by Model Number (Red = Worst, Green = Best)\", fontsize=14)\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Make tick labels smaller\n",
    "ax.tick_params(axis=\"x\", labelsize=9)\n",
    "ax.tick_params(axis=\"y\", labelsize=7)\n",
    "\n",
    "# Add score labels at the end of each bar\n",
    "for bar, score in zip(bars, sorted_scores):\n",
    "    ax.text(\n",
    "        bar.get_width() + max(sorted_scores) * 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{score:.3f}\".replace(',', '.'),\n",
    "        va=\"center\", ha=\"left\", fontsize=8\n",
    "    )\n",
    "\n",
    "# Remove top/bottom empty space\n",
    "ax.set_ylim(-0.5, len(sorted_models) - 0.5)\n",
    "\n",
    "# Extend X-axis so labels fit\n",
    "ax.set_xlim(0, max(sorted_scores) * 1.1)\n",
    "\n",
    "# If you want a colorbar legend, uncomment this block:\n",
    "\"\"\"\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Relative Score (Higher = Better)\")\n",
    "\"\"\"\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"FullVerticalChiCrystalline.png\", dpi=900, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133d701-d3dc-4208-bc6a-d9a14dcd1f5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1> L TEST </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc3251-752b-42d7-9482-afe273f9d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def long_path(path):\n",
    "    \"\"\"Return Windows long path format if on Windows.\"\"\"\n",
    "    if os.name == 'nt':\n",
    "        path = os.path.abspath(path)\n",
    "        if not path.startswith('\\\\\\\\?\\\\'):\n",
    "            return '\\\\\\\\?\\\\' + path\n",
    "    return path\n",
    "\n",
    "def compute_L_for_experiment(df_real, df_pred):\n",
    "    \"\"\"\n",
    "    Compute the reduced L-squared value for one experiment:\n",
    "    \n",
    "        L = (1/N) * Σ abs[(pred - real) / (err_real)]\n",
    "    \"\"\"\n",
    "    if len(df_real) != len(df_pred):\n",
    "        raise ValueError(\"Real and predicted files must have the same length.\")\n",
    "    \n",
    "    N = len(df_real)\n",
    "    diff = df_pred[\"PredictedPolarizationD3\"].values - df_real[\"RealPolarizationD3\"].values\n",
    "    err = df_real[\"ErrRealPolarizationD3\"].values\n",
    "    err[err == 0] = 1e-10  # avoid division by zero\n",
    "    \n",
    "    L = np.sum(np.abs(diff) / (err)) / N\n",
    "    return L\n",
    "\n",
    "def load_data(experiment_folder):\n",
    "    \"\"\"\n",
    "    Load the txt files containing real and predicted data\n",
    "    for a MissingPolarizationD3_* experiment.\n",
    "    \"\"\"\n",
    "    experiment_folder = long_path(experiment_folder)\n",
    "    files = os.listdir(experiment_folder)\n",
    "    real_file = next(f for f in files if f.startswith(\"RawData_PolarizationD3_MissingPolarizationD3_\"))\n",
    "    pred_file = next(f for f in files if f.startswith(\"PredictedPoints_PolarizationD3_MissingPolarizationD3_\"))\n",
    "\n",
    "    df_real = pd.read_csv(long_path(os.path.join(experiment_folder, real_file)), sep=\"\\t\")\n",
    "    df_pred = pd.read_csv(long_path(os.path.join(experiment_folder, pred_file)), sep=\"\\t\")\n",
    "    return df_real, df_pred\n",
    "\n",
    "def count_missing_folders(model_path):\n",
    "    \"\"\"\n",
    "    Return the list of MissingPolarizationD3_* subfolders for a given model folder.\n",
    "    \"\"\"\n",
    "    model_path = long_path(model_path)\n",
    "    return sorted([\n",
    "        f for f in os.listdir(model_path)\n",
    "        if f.startswith(\"MissingPolarizationD3_\") and os.path.isdir(long_path(os.path.join(model_path, f)))\n",
    "    ])\n",
    "\n",
    "def save_L_values_to_txt(model_folder_name, missing_folders, L_values, output_dir):\n",
    "    \"\"\"\n",
    "    Save the L-squared values to a structured text file\n",
    "    inside the 'L' folder at the root level.\n",
    "    \"\"\"\n",
    "    L_dir = long_path(os.path.join(output_dir, \"L\"))\n",
    "    os.makedirs(L_dir, exist_ok=True)  # ensure folder exists\n",
    "\n",
    "    filename = f\"L_values_{model_folder_name}.txt\"\n",
    "    filepath = long_path(os.path.join(L_dir, filename))\n",
    "\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for folder_name, L in zip(missing_folders, L_values):\n",
    "            f.write(f\"{folder_name}: {L:.6f}\\n\")\n",
    "\n",
    "    print(f\"File saved: {filepath}\")\n",
    "\n",
    "def main_all_models(root_folder):\n",
    "    \"\"\"\n",
    "    Main loop over all model folders in the given root_folder.\n",
    "    Each folder should contain MissingPolarizationD3_* subfolders.\n",
    "    \"\"\"\n",
    "    root_folder = long_path(root_folder)\n",
    "    all_model_folders = [\n",
    "        d for d in os.listdir(root_folder)\n",
    "        if os.path.isdir(long_path(os.path.join(root_folder, d)))\n",
    "    ]\n",
    "\n",
    "    if not all_model_folders:\n",
    "        print(\"No model folders found.\")\n",
    "        return\n",
    "\n",
    "    for model_folder in all_model_folders:\n",
    "        #print(f\"\\n Processing model: {model_folder}\")\n",
    "        model_path = long_path(os.path.join(root_folder, model_folder))\n",
    "        missing_folders = count_missing_folders(model_path)\n",
    "\n",
    "        if not missing_folders:\n",
    "            print(f\" No MissingPolarizationD3_ folders in {model_folder}\")\n",
    "            continue\n",
    "\n",
    "        L_values = []\n",
    "        for missing_folder in missing_folders:\n",
    "            experiment_path = long_path(os.path.join(model_path, missing_folder))\n",
    "            try:\n",
    "                df_real, df_pred = load_data(experiment_path)\n",
    "                L = compute_L_for_experiment(df_real, df_pred)\n",
    "                L_values.append(L)\n",
    "                #print(f\"  {missing_folder}: L = {L:.6f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Error in {missing_folder}: {e}\")\n",
    "\n",
    "        if L_values:\n",
    "            save_L_values_to_txt(model_folder, missing_folders, L_values, root_folder)\n",
    "            L_mean = np.mean(L_values)\n",
    "            #print(f\"Average L for {model_folder}: {L_mean:.6f}\")\n",
    "        else:\n",
    "            print(f\"No valid L values computed for {model_folder}.\")\n",
    "\n",
    "# === Example call ===\n",
    "main_all_models(r\".\")\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "def long_path(path):\n",
    "    \"\"\"Return Windows long path format if on Windows.\"\"\"\n",
    "    if os.name == 'nt':\n",
    "        path = os.path.abspath(path)\n",
    "        if not path.startswith('\\\\\\\\?\\\\'):\n",
    "            return '\\\\\\\\?\\\\' + path\n",
    "    return path\n",
    "\n",
    "# Folder that contains the saved L-squared text files\n",
    "folder_path = long_path(\"L\")\n",
    "\n",
    "# Quick diagnostic\n",
    "if not os.path.isdir(folder_path):\n",
    "    print(f\"Folder not found: {folder_path}\")\n",
    "    print(\"   Tip: run the computation cell first so it creates L/ and the files.\")\n",
    "    \n",
    "# Helper to recognize result files from either naming style\n",
    "def is_result_file(name: str) -> bool:\n",
    "    return (\n",
    "        name.endswith(\".txt\") and\n",
    "        (name.startswith(\"L_values_\") or name.startswith(\"L_values_\"))\n",
    "    )\n",
    "\n",
    "# Initialize dictionaries\n",
    "LValues = {}             \n",
    "ExperimentName = OrderedDict() \n",
    "\n",
    "# Collect files\n",
    "result_files = []\n",
    "if os.path.isdir(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if is_result_file(filename):\n",
    "            result_files.append(filename)\n",
    "\n",
    "# Sort for stable order\n",
    "result_files.sort()\n",
    "\n",
    "if not result_files:\n",
    "    print(\"No results found in L/ matLng 'L_values_*.txt' or 'L_values_*.txt'.\")\n",
    "else:\n",
    "    for filename in result_files:\n",
    "        full_path = long_path(os.path.join(folder_path, filename))\n",
    "        with open(full_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        LValues[filename] = {}\n",
    "        for line in lines:\n",
    "            if \":\" in line:\n",
    "                try:\n",
    "                    experiment_name, value = line.split(\":\", 1)\n",
    "                    experiment_name = experiment_name.strip()\n",
    "                    value = float(value.strip())\n",
    "                    LValues[filename][experiment_name] = value\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping malformed line in {filename}: {line.strip()}\")\n",
    "\n",
    "        # Fill ExperimentName only from the first processed file\n",
    "        if not ExperimentName and LValues[filename]:\n",
    "            for i, experiment_name in enumerate(LValues[filename].keys(), 1):\n",
    "                ExperimentName[experiment_name] = f\"exp{i}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# List of experiment identifiers (exp1, exp2, ...)\n",
    "exp_ids = list(ExperimentName.values())\n",
    "\n",
    "# Ensure we have at least 2 models\n",
    "assert len(LValues) >= 2, \"You need at least two L values files\"\n",
    "\n",
    "# List of files to plot (model names)\n",
    "files = list(LValues.keys())\n",
    "\n",
    "# Use modern colormap handling\n",
    "try:\n",
    "    cmap = plt.colormaps.get_cmap('tab20')  # new API\n",
    "except AttributeError:\n",
    "    cmap = plt.get_cmap('tab20')  # old API\n",
    "\n",
    "colors = cmap.colors\n",
    "line_styles = ['-', '--', ':', '-.', (0, (5, 1)), (0, (3, 1, 1, 1))]\n",
    "line_widths = [2] * len(files)\n",
    "\n",
    "# Plot the curves\n",
    "for j, filename in enumerate(files):\n",
    "    L_values = [\n",
    "        LValues[filename].get(exp_name, np.nan)  # Use NaN if missing\n",
    "        for exp_name in ExperimentName.keys()\n",
    "    ]\n",
    "    \n",
    "    plt.plot(\n",
    "        range(len(exp_ids)),\n",
    "        L_values,\n",
    "        color=colors[j % len(colors)],\n",
    "        linestyle=line_styles[j % len(line_styles)],\n",
    "        linewidth=line_widths[j],\n",
    "        label=filename\n",
    "    )\n",
    "\n",
    "# Show xticks every 10 experiments\n",
    "xticks_pos = [i for i in range(len(exp_ids)) if (i + 1) % 10 == 0]\n",
    "xticks_labels = [exp_ids[i] for i in xticks_pos]\n",
    "plt.xticks(xticks_pos, xticks_labels, rotation=45, ha='right')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Experiment\", fontsize=12)\n",
    "plt.ylabel(r\"$L$\", fontsize=12)\n",
    "plt.title(r\"L values per experiment (model comparison)\", fontsize=14)\n",
    "plt.grid(alpha=0.2, linestyle='--')\n",
    "\n",
    "# Legend outside the plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "import numpy as np\n",
    "\n",
    "# Normalize function (replace \"-\" by \"_\")\n",
    "def normalize_key(k):\n",
    "    return k.replace(\"-\", \"_\")\n",
    "\n",
    "# Normalize experiment names inside LValues\n",
    "LValues_norm = {\n",
    "    model: {normalize_key(exp): val for exp, val in LValues[model].items()}\n",
    "    for model in LValues\n",
    "}\n",
    "\n",
    "# Normalize experiment names mapping\n",
    "ExperimentName_norm = {normalize_key(exp): exp for exp in ExperimentName.keys()}\n",
    "\n",
    "# Build 2D array: rows = models, cols = experiments\n",
    "all_L_values = np.array([\n",
    "    [LValues_norm[f].get(exp, np.nan) for exp in ExperimentName_norm.keys()]\n",
    "    for f in LValues_norm.keys()\n",
    "])\n",
    "\n",
    "# Preserve the original experiment order (exp1, exp2, ...)\n",
    "exp_ids = [ExperimentName_norm[exp] for exp in ExperimentName_norm.keys()]\n",
    "\n",
    "# Keep track of model names (rows of all_L_values)\n",
    "model_ids = list(LValues_norm.keys())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Build L matrix ---\n",
    "files = list(LValues.keys())   # Model names (files)\n",
    "nb_models = len(files)\n",
    "\n",
    "exp_ids = list(ExperimentName.keys())  # Experiments in original order\n",
    "\n",
    "all_L_values = []\n",
    "for f in files:\n",
    "    model_vals = []\n",
    "    for exp in exp_ids:\n",
    "        if exp not in LValues[f]:\n",
    "            print(f\"Warning: experiment '{exp}' missing for model '{f}'\")\n",
    "            model_vals.append(np.nan)  # fill missing with NaN\n",
    "        else:\n",
    "            model_vals.append(LValues[f][exp])\n",
    "    all_L_values.append(model_vals)\n",
    "\n",
    "all_L_values = np.array(all_L_values)\n",
    "\n",
    "# --- Compute average L across models for each experiment ---\n",
    "avg_L = np.nanmean(all_L_values, axis=0)  # ignore NaN when averaging\n",
    "\n",
    "# --- Colors and line styles ---\n",
    "try:\n",
    "    cmap = plt.colormaps['tab20']   # new API (>=3.5)\n",
    "except (AttributeError, TypeError):\n",
    "    cmap = plt.get_cmap('tab20')    # old API\n",
    "\n",
    "colors = cmap(range(nb_models))\n",
    "#line_styles = ['--', ':', '-.', (0, (3, 1, 1, 1)), (0, (5, 2))]\n",
    "\n",
    "# --- Plot deviation from the average ---\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "for j in range(nb_models):\n",
    "    diff_from_avg = all_L_values[j] - avg_L\n",
    "    plt.plot(\n",
    "        range(1, len(exp_ids) + 1),  # experiments indexed starting at 1\n",
    "        diff_from_avg,\n",
    "        #linestyle=line_styles[j % len(line_styles)],\n",
    "        linewidth=2,\n",
    "        color=colors[j % len(colors)],\n",
    "        label=files[j]\n",
    "    )\n",
    "\n",
    "# X-ticks every 10 experiments\n",
    "xticks_pos = [k + 1 for k in range(len(exp_ids)) if (k + 1) % 10 == 0]\n",
    "plt.xticks(xticks_pos, rotation=0)\n",
    "\n",
    "plt.xlabel(\"Experiment Index\", fontsize=12)\n",
    "plt.ylabel(\"ΔL (model - average)\", fontsize=12)\n",
    "plt.title(\"Deviation of L values from experiment-wise average\", fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "import os\n",
    "\n",
    "# Folder to save results\n",
    "output_folder = long_path(\"L\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Initialize a dictionary to store scores\n",
    "model_scores = {f: 0 for f in files}\n",
    "\n",
    "# For each experiment (column of all_L_values)\n",
    "for L_vals in all_L_values.T:\n",
    "    # Ignore NaNs when determining the minimum\n",
    "    if np.all(np.isnan(L_vals)):\n",
    "        continue  # skip if all models missing for this experiment\n",
    "    \n",
    "    min_index = np.nanargmin(L_vals)  # index of the winning model\n",
    "    winning_model = files[min_index]\n",
    "    model_scores[winning_model] += 1\n",
    "\n",
    "# Sort by descending score\n",
    "sorted_scores = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Save to txt\n",
    "output_file = os.path.join(output_folder, \"L_NumberOfExperimentsWhereEachModelOutperforms.txt\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Approval Rate (Number of wins by minimal L. The higher the better):\\n\\n\")\n",
    "    for model, score in sorted_scores:\n",
    "        f.write(f\"{model}: {score} points\\n\")\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the L folder exists\n",
    "output_folder = long_path(\"L\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Rebuild all_l_values from LValues\n",
    "files = list(LValues.keys())  # model names\n",
    "exp_ids = list(ExperimentName.keys())  # experiment names in original order\n",
    "\n",
    "# Build the L-squared matrix with NaN for missing experiments\n",
    "all_L_values = []\n",
    "for f in files:\n",
    "    model_L = []\n",
    "    for exp in exp_ids:\n",
    "        model_L.append(LValues[f].get(exp, np.nan))\n",
    "    all_L_values.append(model_L)\n",
    "all_L_values = np.array(all_L_values)\n",
    "\n",
    "# Initialize a dictionary to store rank-based scores\n",
    "model_scores = {f: 0 for f in files}\n",
    "\n",
    "# For each experiment (column)\n",
    "for L_values in all_L_values.T:\n",
    "    sorted_indices = np.argsort(L_values)  # best to worst\n",
    "    for rank, model_idx in enumerate(sorted_indices, start=1):\n",
    "        model_name = files[model_idx]\n",
    "        model_scores[model_name] += rank\n",
    "\n",
    "# Sort results by ascending score (lower is better)\n",
    "sorted_scores = sorted(model_scores.items(), key=lambda x: x[1])\n",
    "\n",
    "# Save to text file\n",
    "output_file = long_path(os.path.join(output_folder, \"L_AccumulativeScores.txt\"))\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Approval Rate (Score by rank. Lower is better):\\n\\n\")\n",
    "    for model, score in sorted_scores:\n",
    "        f.write(f\"{model}: {score} points\\n\")\n",
    "\n",
    "print(f\"Scores saved to: {output_file}\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ensure output folder exists\n",
    "output_folder = long_path(\"L\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_scores = {f: 0.0 for f in files}\n",
    "\n",
    "# For each experiment (iterate over columns of the L matrix)\n",
    "for L_vals in all_L_values.T:\n",
    "    # Skip experiments where all models are missing\n",
    "    if np.all(np.isnan(L_vals)):\n",
    "        continue\n",
    "\n",
    "    # Find the minimum L for this experiment\n",
    "    min_L = np.nanmin(L_vals)\n",
    "\n",
    "    # Compute normalized score for each model: best model = 1, others < 1\n",
    "    for idx, val in enumerate(L_vals):\n",
    "        if np.isnan(val):\n",
    "            score = 0.0  # missing data → 0\n",
    "        else:\n",
    "            score = min_L / val  # relative to best\n",
    "        model_name = files[idx]\n",
    "        model_scores[model_name] += score  # sum across experiments\n",
    "\n",
    "# Compute average relative score per experiment\n",
    "for model in model_scores:\n",
    "    model_scores[model] /= len(exp_ids)  # optional: normalize by number of experiments\n",
    "\n",
    "# Sort models by descending relative score (higher is better)\n",
    "sorted_scores = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Save results to a text file\n",
    "output_file = os.path.join(output_folder, \"L_RelativeNormalizedScores.txt\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Relative Score (Normalized L per experiment. Higher is better):\\n\\n\")\n",
    "    for model, score in sorted_scores:\n",
    "        f.write(f\"{model}: {score:.4f}\\n\")\n",
    "\n",
    "print(f\"Normalized relative scores saved to {output_file}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Vertical bar chart: Rank-based scoring (lower=better)\n",
    "# -------------------------------\n",
    "sorted_scores_rank = sorted(model_scores.items(), key=lambda x: x[1])  # rank-based\n",
    "models = [m for m, _ in sorted_scores_rank]\n",
    "scores = [s for _, s in sorted_scores_rank]\n",
    "\n",
    "norm = plt.Normalize(min(scores), max(scores))\n",
    "try:\n",
    "    cmap = plt.colormaps['RdYlGn_r']   # new API (>=3.5)\n",
    "except (AttributeError, TypeError):\n",
    "    cmap = plt.get_cmap('RdYlGn_r')    # old API\n",
    "\n",
    "colors = cmap(norm(scores))\n",
    "# Extract short names for X-axis\n",
    "short_models = [m.replace(\"L_values_AllTestsFolder_\", \"\").replace(\".txt\", \"\") for m in models]\n",
    "short_models = [\"_\".join(m.split(\"_\")[-2:]) for m in short_models]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(short_models, scores, color=colors)\n",
    "\n",
    "# Add score labels above bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,     # center above bar\n",
    "        score + score * 0.02,                  # offset above bar\n",
    "        f\"{score:.3f}\".replace('.', ','),      # 3 decimals, comma style\n",
    "        ha='center', va='bottom',\n",
    "        fontsize=8                             # smaller font\n",
    "    )\n",
    "ymax = max(scores)\n",
    "ax.set_ylim(0, ymax * 1.1)\n",
    "# Colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array(scores)\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Rank Score (Lower = Better)\")\n",
    "\n",
    "ax.set_ylabel(\"Total Rank Score\", fontsize=12)\n",
    "ax.set_xlabel(\"Model\", fontsize=12)\n",
    "ax.set_title(\"Approval Rate: Rank-Based Scoring (L, Green=Better, Red=Worse)\", fontsize=14)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(long_path(os.path.join(output_folder, \"L_rank_vertical.png\")), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Prepare data for the relative scoring graph ---\n",
    "# model_scores here is the normalized/relative scoring dictionary\n",
    "sorted_scores_rel = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)  # higher = better\n",
    "\n",
    "models = [m for m, _ in sorted_scores_rel]\n",
    "scores = [s for _, s in sorted_scores_rel]\n",
    "\n",
    "# Shorten names if needed\n",
    "short_models = [m.replace(\"L_values_AllTestsFolder_\", \"\").replace(\".txt\", \"\") for m in models]\n",
    "\n",
    "# Normalize colors\n",
    "norm = plt.Normalize(min(scores), max(scores))\n",
    "try:\n",
    "    cmap = plt.colormaps['RdYlGn']   # new API (>=3.5)\n",
    "except (AttributeError, TypeError):\n",
    "    cmap = plt.get_cmap('RdYlGn')\n",
    "\n",
    "colors = cmap(norm(scores))\n",
    "\n",
    "# Extract short names (only last two parts: Name_number)\n",
    "short_models = [\"_\".join(m.split(\"_\")[-2:]) for m in short_models]\n",
    "\n",
    "# --- Create the figure ---\n",
    "fig, ax = plt.subplots(figsize=(14, 6))  # wider to accommodate long names\n",
    "bars = ax.bar(short_models, scores, color=colors, edgecolor='black')\n",
    "\n",
    "# Add value labels above bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,     # center above bar\n",
    "        score + score * 0.02,                  # offset above bar\n",
    "        f\"{score:.3f}\".replace('.', ','),      # 3 decimals, comma style\n",
    "        ha='center', va='bottom',\n",
    "        fontsize=8                             # smaller font\n",
    "    )\n",
    "\n",
    "# Colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array(scores)\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Relative Score (Higher = Better)\")\n",
    "\n",
    "# Labels and title\n",
    "ax.set_ylabel(\"Total Relative Score (averaged over experiments)\", fontsize=12)\n",
    "ax.set_xlabel(\"Model\", fontsize=12)\n",
    "ax.set_title(\"Approval Rate: Normalized Relative Scoring (L, Green = Best, Red = Worst)\", fontsize=14)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Rotate X-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Make the y-axis a bit taller for small differences\n",
    "ax.set_ylim(0, max(scores) * 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(long_path(os.path.join(output_folder, \"L_relative_vertical.png\")), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e030cce-7f5e-416c-aad3-32072f3dbaa6",
   "metadata": {},
   "source": [
    "<h1> PLOTS COMPARING THE MODELS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ebdc87-2fd9-4822-bd88-8df144cbfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "search_root = Path.cwd()\n",
    "output_dir = search_root / \"ExperimentComparison\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Look for any Missing*.jpg in all subfolders\n",
    "all_images = glob.glob(str(search_root / \"**\" / \"Missing*.jpg\"), recursive=True)\n",
    "print(f\"Found {len(all_images)} images total.\")\n",
    "def best_subplot_shape(n):\n",
    "    \"\"\"\n",
    "    Given n images, return (rows, cols) for a balanced subplot grid.\n",
    "    \"\"\"\n",
    "    # Start with square root\n",
    "    cols = math.ceil(math.sqrt(n))\n",
    "    rows = math.ceil(n / cols)\n",
    "    return rows, cols\n",
    "#\"Naif834\", \"Naif838\", \"Naif8316\", \"Naif8332\", \"Naif854\", \"Naif858\", \"Naif8516\", \"Naif8532\", \"Naif8104\", \"Naif8108\", \"Naif81016\", \"Naif81032\",\n",
    "# , \"Naif2434\", \"Naif2438\", \"Naif24316\", \"Naif24332\", \"Naif2454\", \"Naif2458\", \"Naif24516\", \"Naif24532\", \"Naif24104\", \"Naif24108\", \"Naif241016\", \"Naif241032\"\n",
    "\n",
    "desired_order = [\"NaifTwice1D483316\", \"NaifTwice1D483332\", \"NaifTwice1D483516\", \"NaifTwice1D483532\", \"NaifTwice1D485316\", \"NaifTwice1D485332\", \"NaifTwice1D485516\", \"NaifTwice1D485532\", \"NaifTwice1D443316\", \"NaifTwice1D443332\", \"NaifTwice1D443516\", \"NaifTwice1D443532\", \"NaifTwice1D445316\", \"NaifTwice1D445332\", \"NaifTwice1D445516\", \"NaifTwice1D445532\", \"NaifTwice1D883316\", \"NaifTwice1D883332\", \"NaifTwice1D883516\", \"NaifTwice1D883532\", \"NaifTwice1D885316\", \"NaifTwice1D885332\", \"NaifTwice1D885516\", \"NaifTwice1D885532\", \"NaifTwice1D843316\", \"NaifTwice1D843332\", \"NaifTwice1D843516\", \"NaifTwice1D843532\", \"NaifTwice1D845316\", \"NaifTwice1D845332\", \"NaifTwice1D845516\", \"NaifTwice1D845532\"]\n",
    "\n",
    "#[\"NaifTwice1D883316\", \"NaifTwice1D883332\", \"NaifTwice1D883516\", \"NaifTwice1D883532\", \"NaifTwice1D885316\", \"NaifTwice1D885332\", \"NaifTwice1D885516\", \"NaifTwice1D885532\", \"NaifTwice1D843316\", \"NaifTwice1D843332\", \"NaifTwice1D843516\", \"NaifTwice1D843532\", \"NaifTwice1D845316\", \"NaifTwice1D845332\", \"NaifTwice1D845516\", \"NaifTwice1D845532\"]\n",
    "\n",
    "#[\"Naif1634\", \"Naif1638\", \"Naif16316\", \"Naif16332\", \"Naif1654\", \"Naif1658\", \"Naif16516\", \"Naif16532\", \"Naif16104\", \"Naif16108\", \"Naif161016\", \"Naif161032\"]\n",
    "\n",
    "order_index = {name: i for i, name in enumerate(desired_order)}\n",
    "\n",
    "def extract_naif_name(folder_name: str) -> str:\n",
    "    \"\"\"Extract the NaifXXX part from a folder name like 'CrystallineAllTestsFolder_Naif81016_3'.\"\"\"\n",
    "    import re\n",
    "    match = re.search(r\"Naif\\d+\", folder_name)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def shorten_exp_id(exp_id: str) -> str:\n",
    "    parts = exp_id.split(\"_\")\n",
    "    try:\n",
    "        miller_idx = parts.index(\"MillerIndex\")\n",
    "        core_parts = parts[1:miller_idx]  # skip first part\n",
    "    except ValueError:\n",
    "        core_parts = parts[1:]\n",
    "    return \"_\".join(core_parts)\n",
    "\n",
    "# --- Group images ---\n",
    "groups = {}\n",
    "for p in all_images:\n",
    "    p = Path(p)\n",
    "    if p.parent.name.startswith(\"Missing\"):\n",
    "        exp_id = p.parent.name\n",
    "    else:\n",
    "        exp_id = p.stem\n",
    "    groups.setdefault(exp_id, []).append(p)\n",
    "\n",
    "print(f\"Found {len(groups)} different experiments.\")\n",
    "\n",
    "\n",
    "# Get the directory where the notebook is located\n",
    "ssearch_root = Path.cwd()  # assuming you run this from the notebook’s folder\n",
    "\n",
    "# List only folders in this directory\n",
    "sall_folders = [f for f in ssearch_root.iterdir() if f.is_dir()]\n",
    "\n",
    "# Count folders containing \"AllTestsFolder\" in their name\n",
    "DIM = sum(\"AllTestsFolder\" in f.name for f in sall_folders)\n",
    "print(f\"Found {DIM} different models.\")\n",
    "\n",
    "# --- Build collages ---\n",
    "for idx, (exp_id, paths) in enumerate(groups.items(), start=1):\n",
    "    # Debug: print original folder names\n",
    "    #print(f\"\\n--- Group: {exp_id} ---\")\n",
    "    #print(\"Original folder names:\")\n",
    "    #for p in paths:\n",
    "        #print(\"  \", p.parents[1].name)\n",
    "\n",
    "    # Reorder paths strictly following desired_order\n",
    "    paths_sorted = []\n",
    "    for desired_name in desired_order:\n",
    "        for p in paths:\n",
    "            naif_name = extract_naif_name(p.parents[1].name)\n",
    "            if naif_name == desired_name:\n",
    "                paths_sorted.append(p)\n",
    "\n",
    "    # Append any leftover images not in desired_order at the end\n",
    "    for p in paths:\n",
    "        if p not in paths_sorted:\n",
    "            paths_sorted.append(p)\n",
    "\n",
    "    #print(\"After desired_order sorting:\")\n",
    "    #for p in paths_sorted:\n",
    "        #print(\"  \", p.parents[1].name)\n",
    "\n",
    "    # Now use paths_sorted to fill the figure\n",
    "    count = DIM\n",
    "    rows, cols = best_subplot_shape(count)\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols, rows))    \n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, img_path in zip(axes, paths_sorted):\n",
    "        lp = r\"\\\\?\\{}\".format(os.path.abspath(str(img_path)))\n",
    "        if not os.path.exists(lp):\n",
    "            print(f\"WARNING: File not found: {lp}\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        img = Image.open(lp)\n",
    "        ax.imshow(img)\n",
    "        #ax.set_title(img_path.parents[1].name, fontsize=8)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for ax in axes[len(paths_sorted):]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    short_id = shorten_exp_id(exp_id)\n",
    "    plt.tight_layout()\n",
    "    save_name = output_dir / f\"{short_id}.png\"\n",
    "    save_path = r\"\\\\?\\{}\".format(os.path.abspath(str(save_name)))\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=1200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved {save_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79ab69-a5d9-4ad7-afad-074389647460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
